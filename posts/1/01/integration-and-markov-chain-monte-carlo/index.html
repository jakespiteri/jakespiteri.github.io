<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="ie=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="author" content="Jake Spiteri">
<meta name="description" content="Numerical Integration Quadrature “Quadrature rules” are integral approximations which use a finite number of evaluations of the function. All of the quadrature rules below approximate a function using interpolating polynomials.
We will first look at some key ideas in numerical integration by approximating definite integrals over a finite interval. We will then extend these ideas to semi-infinite and infinite intervals. We will also consider multiple integrals.
In practice we can use Rs integrate function for one-dimensional integrals, and the cubature package for multiple integrals." />
<meta name="keywords" content="statistics, mathematics, data science, machine learning, deep learning" />
<meta name="robots" content="noodp" />
<meta name="theme-color" content="#252627" />
<link rel="canonical" href="https://jakespiteri.co.uk/posts/1/01/integration-and-markov-chain-monte-carlo/" />


    <title>
        
            Integration and Markov Chain Monte Carlo :: Jake Spiteri  — Jake Spiteri
        
    </title>



<link href="https://cdnjs.cloudflare.com/ajax/libs/flag-icon-css/3.2.1/css/flag-icon.min.css" rel="stylesheet"
    type="text/css">



<link rel="stylesheet" href="https://jakespiteri.co.uk/main.min.7bfbbe12786fa0ded4b4c0d792cbb36a5bd0bdb0b856dde57aa7b1f6fe0f2b87.css">


    
        <link rel="stylesheet" type="text/css" href="static/style.css">
    



    <link rel="apple-touch-icon" sizes="180x180" href="https://jakespiteri.co.uk/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="https://jakespiteri.co.uk/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="https://jakespiteri.co.uk/favicon-16x16.png">
    <link rel="manifest" href="https://jakespiteri.co.uk/site.webmanifest">
    <link rel="mask-icon" href="https://jakespiteri.co.uk/safari-pinned-tab.svg" color="#252627">
    <link rel="shortcut icon" href="https://jakespiteri.co.uk/favicon.ico">
    <meta name="msapplication-TileColor" content="#252627">
    <meta name="theme-color" content="#252627">

<meta itemprop="name" content="Integration and Markov Chain Monte Carlo">
<meta itemprop="description" content="Numerical Integration Quadrature “Quadrature rules” are integral approximations which use a finite number of evaluations of the function. All of the quadrature rules below approximate a function using interpolating polynomials.
We will first look at some key ideas in numerical integration by approximating definite integrals over a finite interval. We will then extend these ideas to semi-infinite and infinite intervals. We will also consider multiple integrals.
In practice we can use Rs integrate function for one-dimensional integrals, and the cubature package for multiple integrals.">

<meta itemprop="wordCount" content="4214">
<meta itemprop="image" content="https://jakespiteri.co.uk"/>



<meta itemprop="keywords" content="" /><meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://jakespiteri.co.uk"/>

<meta name="twitter:title" content="Integration and Markov Chain Monte Carlo"/>
<meta name="twitter:description" content="Numerical Integration Quadrature “Quadrature rules” are integral approximations which use a finite number of evaluations of the function. All of the quadrature rules below approximate a function using interpolating polynomials.
We will first look at some key ideas in numerical integration by approximating definite integrals over a finite interval. We will then extend these ideas to semi-infinite and infinite intervals. We will also consider multiple integrals.
In practice we can use Rs integrate function for one-dimensional integrals, and the cubature package for multiple integrals."/>















<link href='//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/dracula.min.css' rel='stylesheet' type='text/css' />



    </head>

    <body class="dark-theme">
        <div class="container">
            <header class="header">
    <span class="header__inner">
        <a href="https://jakespiteri.co.uk/" style="text-decoration: none;">
    <div class="logo">
        
            <span class="logo__mark">></span>
            <span class="logo__text">$ cd /home/</span>
            <span class="logo__cursor" style=
                  "
                   
                   ">
            </span>
        
    </div>
</a>


        <span class="header__right">
            
                <nav class="menu">
    <ul class="menu__inner"><li><a href="https://jakespiteri.co.uk/about/">About</a></li><li><a href="https://jakespiteri.co.uk/posts/">Posts</a></li>
    </ul>
</nav>

                <span class="menu-trigger">
                    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
                        <path d="M0 0h24v24H0z" fill="none"/>
                        <path d="M3 18h18v-2H3v2zm0-5h18v-2H3v2zm0-7v2h18V6H3z"/>
                    </svg>
                </span>
            

            <span class="theme-toggle unselectable"><svg class="theme-toggler" width="24" height="24" viewBox="0 0 48 48" fill="none" xmlns="http://www.w3.org/2000/svg">
  <path d="M22 41C32.4934 41 41 32.4934 41 22C41 11.5066 32.4934 3 22
  3C11.5066 3 3 11.5066 3 22C3 32.4934 11.5066 41 22 41ZM7 22C7
  13.7157 13.7157 7 22 7V37C13.7157 37 7 30.2843 7 22Z"/>
</svg>
</span>
        </span>
    </span>
</header>


            <div class="content">
                
    <main class="post">

        <div class="post-info">
            <p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-clock"><circle cx="12" cy="12" r="10"></circle><polyline points="12 6 12 12 16 14"></polyline></svg>20 minutes

            

            </p>
        </div>

        <article>
            <h1 class="post-title">
                <a href="https://jakespiteri.co.uk/posts/1/01/integration-and-markov-chain-monte-carlo/">Integration and Markov Chain Monte Carlo</a>
            </h1>

            

            <div class="post-content">
                


<div id="numerical-integration" class="section level1">
<h1>Numerical Integration</h1>
<div id="quadrature" class="section level2">
<h2>Quadrature</h2>
<p>“Quadrature rules” are integral approximations which use a finite number of evaluations of the function. All of the quadrature rules below approximate a function using interpolating polynomials.</p>
<p>We will first look at some key ideas in numerical integration by approximating definite integrals over a finite interval. We will then extend these ideas to semi-infinite and infinite intervals. We will also consider multiple integrals.</p>
<p>In practice we can use <code>R</code>s <code>integrate</code> function for one-dimensional integrals, and the <code>cubature</code> package for multiple integrals.</p>
<div id="polynomial-interpolation" class="section level3">
<h3>Polynomial Interpolation</h3>
<p>We consider approximating a continuous function <span class="math inline">\(f\)</span> on <span class="math inline">\([a,b]\)</span> (this can be written <span class="math inline">\(f \in C^0([a,b])\)</span>), using a polynomial function <span class="math inline">\(p\)</span>. We know how to compute integrals of polynomials exactly, and so if <span class="math inline">\(p\)</span> is a good approximation of our integrand then we can compute an accurate approximation to the integral. The Weierstrass Approximation Theorem provides a high-level motivation.</p>
<p><strong>Weierstrass Approximation Theorem.</strong> Let <span class="math inline">\(f \in C^0([a,b])\)</span>. There exists a sequence of polynomials <span class="math inline">\((p_n)\)</span> that converges uniformly to <span class="math inline">\(f\)</span> on <span class="math inline">\([a,b]\)</span>. That is,
<span class="math display">\[ \|f - p_n\|_\infty = \max_{x \in [a,b]} |f(x) - p_n(x)| \rightarrow 0.\]</span></p>
<p>The above theorem suggests that for any given tolerance, we can find a polynomial which approximates a function <span class="math inline">\(f \in C^0([a,b])\)</span> up to said tolerance. The Weierstrass theorem tells us that such polynomials exist, but it does not tell us how to find them. The real difficulty lies in constructing such polynomials in a computationally-efficient manner without a strong knowledge of the function <span class="math inline">\(f\)</span>.</p>
<div id="lagrange-polynomials" class="section level4">
<h4>Lagrange polynomials</h4>
<p>We can approximate a function <span class="math inline">\(f\)</span> by using an interpolating polynomial with <span class="math inline">\(k\)</span> points <span class="math inline">\(\{x_i, f(x_i)\}_{i=1}^n\)</span>. The interpolating polynomial produced is unique, has at most <span class="math inline">\(k-1\)</span> degree, and can be written as a Lagrange polynomial:
<span class="math display">\[p_{k-1}(x) := \sum_{i=1}^k \ell_i(x) f(x_i),\]</span>
where the Lagrange basis polynomials are
<span class="math display">\[\ell_i(x) = \prod_{j=1, j\neq i}^p \frac{x - x_j}{x_i - x_j}, \quad i \in \{1, \dots, k\}.\]</span></p>
<p>Below is code implementing interpolating polynomials. For a polynomial with degree <span class="math inline">\(3\)</span> we can produce the polynomial approximations for <span class="math inline">\(k \in \{2,3,4\}\)</span> and for specific choices of <span class="math inline">\(x_1, \dots, x_4\)</span>.</p>
<pre class="r"><code>construct.interpolating.polynomial &lt;- function(f, xs) {
  k &lt;- length(xs)
  fxs &lt;- f(xs)
  p &lt;- function(x) {
    value &lt;- 0
    for (i in 1:k) {
      fi &lt;- fxs[i]
      zs &lt;- xs[setdiff(1:k,i)]
      li &lt;- prod((x-zs)/(xs[i]-zs))
      value &lt;- value + fi*li
    }
    return(value)
  }
  return(p)
}

plot.polynomial.approximation &lt;- function(f, xs, a, b) {
  p &lt;- construct.interpolating.polynomial(f, xs)
  vs &lt;- seq(a, b, length.out=500)
  plot(vs, f(vs), type=&#39;l&#39;, xlab=&quot;x&quot;, ylab=&quot;black: f(x), red: p(x)&quot;)
  points(xs, f(xs), pch=20)
  lines(vs, vapply(vs, p, 0), col=&quot;red&quot;)
}

a &lt;- -4
b &lt;- 4

f &lt;- function(x) {
  return(-x^3 + 3*x^2 - 4*x + 1)
}

par(mfrow=c(1,3))
plot.polynomial.approximation(f, c(-2, 2), a, b)
plot.polynomial.approximation(f, c(-2,0,2), a, b)
plot.polynomial.approximation(f, c(-2, 0, 2, 4), a, b)</code></pre>
<p><img src="https://jakespiteri.co.uk/posts/Report-10_files/figure-html/unnamed-chunk-1-1.png" width="1344" style="display: block; margin: auto;" /></p>
</div>
<div id="interpolation-error-and-convergence" class="section level4">
<h4>Interpolation Error and Convergence</h4>
<p>Clearly we can approximate a polynomial using interpolating polynomials for a <span class="math inline">\(k\)</span> which is large enough. However, we are often interested in approximating functions which are not polynomials.</p>
<p><strong>Interpolation Error Theorem.</strong> Let <span class="math inline">\(f \in C^k[a,b]\)</span>, and <span class="math inline">\(p_{k-1}\)</span> be the polynomial interpolating <span class="math inline">\(f\)</span> at the <span class="math inline">\(k\)</span> points <span class="math inline">\(x_1, \dots, x_k\)</span>. Then for any <span class="math inline">\(x \in [a,b]\)</span> there exists <span class="math inline">\(\xi \in (a,b)\)</span> such that
<span class="math display">\[f(x) - p_{k-1}(x) = \frac{1}{k!}f^{(k)}(\xi) \prod_{i=1}^k (x - x_i).\]</span>
The above theorem looks promising, but it does not tell us how to choose our interpolating polynomials such that they converge uniformly (or even pointwise) to <span class="math inline">\(f\)</span>. The theorem tells us that one way of minimizing the error is to choose the interpolation points such that the product <span class="math inline">\(|\prod_{i=1}^k (x - x_k)|\)</span> is as small as possible. The Chebyshev interpolation points detailed below do this.</p>
<p>We know that there exists a sequence of interpolation points that provide us with uniform convergence.</p>
<p><strong>Theorem.</strong> Let <span class="math inline">\(f \in C^0[a,b]\)</span>. There exists a sequence of sets of interpolation points <span class="math inline">\(X_1, X_2, \dots\)</span> such that the corresponding sequence of interpolating polynomials converges uniformly to <span class="math inline">\(f\)</span> on <span class="math inline">\([a,b]\)</span>.</p>
<p>The above tells us that given a function <span class="math inline">\(f\)</span>, it is indeed possible to find a sequence of sets of interpolating points which produce a sequence of interpolating polynomials that converge uniformly to <span class="math inline">\(f\)</span>. We may then wonder if there is a universal sequence of sets of interpolating points that provide us with uniform convergence to <em>any</em> function <span class="math inline">\(f\)</span>. This is not possible, as for any fixed sequence of sets we can find a function <span class="math inline">\(f\)</span> for which our sequence of interpolating polynomials diverges.</p>
<p><strong>Theorem.</strong> For any fixed sequence of sets of interpolation points there exists a continuous function <span class="math inline">\(f\in C^0[a,b]\)</span> for which the sequence of interpolating polynomials diverges on <span class="math inline">\([a,b]\)</span>.</p>
<p><strong>Example:</strong> Consider the sequence of interpolating points <span class="math inline">\(X_k\)</span> to be the set of <span class="math inline">\(k\)</span> uniformly spaced points including <em>a</em>, and <em>b</em> if <span class="math inline">\(k&gt;1\)</span>.</p>
<p>This is a particularly bad way to produce sets of interpolation points as uniform convergence is not guaranteed — even for infinitely differentiable functions. There are of course functions which these interpolation points work well for, as seen below.</p>
<pre class="r"><code>construct.uniform.point.set &lt;- function(a, b, k) {
  if (k==1) return(a)
  return(seq(a, b, length.out=k))
}

a &lt;- 1
b &lt;- 2
plot.polynomial.approximation(log, construct.uniform.point.set(a, b, 10), a, b)</code></pre>
<p><img src="https://jakespiteri.co.uk/posts/Report-10_files/figure-html/unnamed-chunk-2-1.png" width="384" style="display: block; margin: auto;" /></p>
<p>Below is a plot of the Runge function defined by <span class="math inline">\(f(x) = \frac{1}{1+x^2}\)</span> over the interval <span class="math inline">\([-5,5]\)</span>. We see that the interpolation error <span class="math inline">\(\|f-p_n\|_\infty\)</span> grows without bound as <span class="math inline">\(n \rightarrow \infty\)</span>.</p>
<p>Below we look at the polynomial interpolation of the Runge function <span class="math inline">\(f(x) = \frac{1}{1+x^2}\)</span>, over the interval <span class="math inline">\([-5,5]\)</span>.</p>
<pre class="r"><code>a &lt;- -5
b &lt;- 5
f &lt;- function(x) return(1/(1+x^2))
plot.polynomial.approximation(f, construct.uniform.point.set(a,b,50), a, b)</code></pre>
<p><img src="https://jakespiteri.co.uk/posts/Report-10_files/figure-html/unnamed-chunk-3-1.png" width="384" style="display: block; margin: auto;" /></p>
<p>Note that this does not contradict the Interpolation Error Theorem as the maximum of the <span class="math inline">\(k\)</span>’th derivative of the Runge function grows quickly with <span class="math inline">\(k\)</span>. As this outweighs the decreasing product term, the interpolation error grows without bound as <span class="math inline">\(k \rightarrow \infty\)</span>.</p>
<p>We can also look at the function <span class="math inline">\(f(x) = |x|\)</span> over <span class="math inline">\([-1,1]\)</span>. We see that the interpolating polynomial oscillates wildly for large <span class="math inline">\(k\)</span>.</p>
<pre class="r"><code>a &lt;- -1
b &lt;- 1
par(mfrow=c(1,3))
plot.polynomial.approximation(abs, construct.uniform.point.set(a,b,50), a, b)
plot.polynomial.approximation(abs, construct.uniform.point.set(a,b,100), a, b)
plot.polynomial.approximation(abs, construct.uniform.point.set(a,b,200), a, b)</code></pre>
<p><img src="https://jakespiteri.co.uk/posts/Report-10_files/figure-html/unnamed-chunk-4-1.png" width="1344" style="display: block; margin: auto;" />
Note once again that this does not contradict the Interpolation Error Theorem as the function <span class="math inline">\(f(x)\)</span> is not differentiable at <span class="math inline">\(0\)</span>.</p>
<p>As mentioned above, we can avoid this problem by minimizing the interpolation error by minimizing the product term. If we minimize the maximum absolute value of the product term, we can derive the Chebyshev interpolation points. For any given <span class="math inline">\(k\)</span>, we have the points
<span class="math display">\[\cos\left(\frac{2i-1}{2k}\pi\right), \quad i \in \{1, \dots, k\},\]</span>
and the absolute value of the product term is then bounded above by <span class="math inline">\(2^{1-k}\)</span></p>
<p>These points clearly do not minimize the overall error.</p>
<pre class="r"><code>construct.chebyshev.point.set &lt;- function(k) {
  return(cos((2*(1:k)-1)/2/k*pi))
}

# visualize the Chebyshev points
chebyshev.visalization &lt;- function(k){
  df &lt;- data.frame(k = numeric(), points = list())
  for(i in 1:k){
    df &lt;- rbind(df, data.frame(k=i, points = construct.chebyshev.point.set(i)))
  }
  ggplot(df, aes(color=k, x = points, y = k)) +
    geom_point(aes(color=k)) +
    labs(y=&quot;number of points k&quot;, xlab=&quot;&quot;)
}

# produce visualization
chebyshev.visalization(50)</code></pre>
<p><img src="https://jakespiteri.co.uk/posts/Report-10_files/figure-html/unnamed-chunk-5-1.png" width="384" style="display: block; margin: auto;" /></p>
<p>Above we clearly see that as our number of points <span class="math inline">\(k\)</span> increases, the points tend to cluster around <span class="math inline">\(-1\)</span>, and <span class="math inline">\(1\)</span>. We can use the Chebyshev interpolation points as seen below.</p>
<pre class="r"><code>plot.polynomial.approximation(f, construct.chebyshev.point.set(50), a, b)</code></pre>
<p><img src="https://jakespiteri.co.uk/posts/Report-10_files/figure-html/unnamed-chunk-6-1.png" width="384" style="display: block; margin: auto;" /></p>
<pre class="r"><code>plot.polynomial.approximation(abs, construct.chebyshev.point.set(50), a, b)</code></pre>
<p><img src="https://jakespiteri.co.uk/posts/Report-10_files/figure-html/unnamed-chunk-6-2.png" width="384" style="display: block; margin: auto;" /></p>
<p>There is a large improvement in the error as the approximation no longer oscillates. As mentioned in the theorem above, there exist functions <span class="math inline">\(f\)</span> for which the interpolating polynomials with the Chebyshev interpolating points diverge.</p>
</div>
</div>
<div id="composite-polynomial-interpolation" class="section level3">
<h3>Composite Polynomial Interpolation</h3>
<p>Another way to approximate a function is to use different polynomials over subintervals of the domain. This results in a piecewise polynomial approximation which is not necessarily continuous, but can be made to be.</p>
</div>
<div id="other-polynomial-interpolation-schemes" class="section level3">
<h3>Other Polynomial Interpolation Schemes</h3>
<p>There are many other schemes that we may use to approximate a function. We may use Hermite interpolation which fits a polynomial using evaluations of <span class="math inline">\(f\)</span> and evaluations of the derivatives of <span class="math inline">\(f\)</span>. If we ensure that the derivatives at the boundaries of the subintervals are equal then the resulting approximation will have a certain number of continuous derivatives. This idea is known as spline interpolation, and is often preferred to polynomial interpolation as it avoids the problem of Runge’s phenomenon demonstrated above. Spline interpolation also produces a small interpolation error even when using low degree polynomials.</p>
<p>Below is an implementation of natural cubic spline interpolation. For a set of interpolation points <span class="math inline">\(\{x_i\}_{i=1}^n\)</span>, a cubic spline fits a polynomial of at most degree <span class="math inline">\(3\)</span> between each pair of interpolating points. Hence for a set of <span class="math inline">\(n\)</span> interpolation points, <span class="math inline">\(n-1\)</span> splines are produced to approximate the function. A set of constraints are placed upon the values these splines can take, which ensure that the spline is continuous over the interval. A natural cubic spline has degree <span class="math inline">\(3\)</span> with continuity <span class="math inline">\(C^2\)</span>, and has the general form
<span class="math display">\[S_j(x) = a_j + b_j(x - x_j) + c_j(x - x_j)^2 + d_j(x - x_j)^3, \quad j = 0, \dots, n-1\]</span>
for which the individual splines must satisfy
<span class="math display">\[\begin{align*}
&amp;S_i(x_i) &amp;&amp;= y_i  \qquad &amp;&amp;i = 0, \dots, n-1\\
&amp;S_{i-1}(x_i) &amp;&amp;= y_i &amp;&amp;i = 1, \dots, n\\
&amp;S&#39;_{i}(x_i) &amp;&amp;= S&#39;_{i-1}(x_i) &amp;&amp;i = 1, \dots, n-1 \\
&amp;S&#39;&#39;_{i}(x_i) &amp;&amp;= S&#39;&#39;_{i-1}(x_i) &amp;&amp;i = 1, \dots, n-1 \\
&amp;S&#39;&#39;_{0}(x_0) &amp;&amp;= S&#39;&#39;_{n-1}(x_{n-1}) = 0 &amp;&amp;i = 1, \dots, n-1 \\
\end{align*}\]</span></p>
<p>An implementation can be seen below.</p>
<pre class="r"><code># compute spline parameters
create.spline.params &lt;- function(f, xs) {
  n &lt;- length(xs) - 1
  fxs &lt;- f(xs)
  b &lt;- d &lt;- rep(0,n)
  h &lt;- diff(xs)
  hlag2 &lt;- diff(xs, lag=2)
  alpha &lt;- 3/h[-1]*diff(fxs)[-1] - 3/h[-n]*diff(fxs)[-n]
  c &lt;- l &lt;- mu &lt;- z &lt;- rep(0,n+1)
  l[1] &lt;- 1; mu[1] &lt;- z[1] &lt;- 0
  for (i in 2:(n)) {
    l[i] &lt;- 2*hlag2[i-1] - h[i-1]*mu[i-1]
    mu[i] &lt;- h[i]/l[i]
    z[i] &lt;- (alpha[i-1]-h[i-1]*z[i-1])/l[i]
  }
  l[n+1] &lt;- 1; z[n+1] &lt;- c[n+1] &lt;- 0
  for (j in n:1) {
    c[j] &lt;- z[j] - mu[j]*c[j+1]
    b[j] &lt;- (fxs[j+1] - fxs[j])/h[j] - h[j]*(c[j+1] + 2*c[j])/3
    d[j] &lt;- (c[j+1]-c[j])/(3*h[j])
  }
  return(list(a=fxs, b = b, c = c, d = d))
}

# given parameters and the interval, return appropriate spline function
# this is needed as for n interpolation points, n-1 splines are produced
construct.spline &lt;- function(a, b, c, d, i, xs) {
  return(function(x) a[i] + b[i]*(x - xs[i]) + c[i]*(x - xs[i])^2 + d[i]*(x - xs[i])^3)
}

# given x and interpolation points, return spline index
get.spline.subinterval &lt;- function(x, xs){
  n &lt;- length(xs-1)
  for(i in 1:n) {
    if (between(x, xs[i], xs[i+1])) return(i)
  }
}

# produce and plot the spline approximation for a function f and interpolation points xs
plot.cubic.splines.approximation &lt;- function(f, xs){
  par &lt;- create.spline.params(f, xs)
  spline &lt;- function(x) construct.spline(par$a, par$b, par$c, par$d, 
                                         get.spline.subinterval(x, xs), xs)(x)
  vs &lt;- seq(min(xs), max(xs), length.out=500)
  plot(vs, f(vs), type=&#39;l&#39;, xlab=&quot;x&quot;, ylab=&quot;black: f(x), red: p(x)&quot;)
  abline(v=xs, col = &quot;grey&quot;)
  lines(vs, f(vs))
  lines(vs, vapply(vs, spline, 0), col=&quot;red&quot;)
  points(xs, f(xs), pch=20, col=&quot;blue&quot;)
}</code></pre>
<pre class="r"><code>par(mfrow=c(1,2))
plot.cubic.splines.approximation(sin, seq(0,4*pi, length.out=4))
plot.cubic.splines.approximation(sin, seq(0,4*pi, length.out=12))</code></pre>
<p><img src="https://jakespiteri.co.uk/posts/Report-10_files/figure-html/unnamed-chunk-8-1.png" width="1344" style="display: block; margin: auto;" /></p>
<p>We see that our approximation is not very accurate when we have a small set of interpolation points, but we can approximate the <span class="math inline">\(\sin\)</span> function impressively well with only <span class="math inline">\(12\)</span> interpolation points and with polynomials of at most degree <span class="math inline">\(3\)</span>!</p>
<pre class="r"><code>plot.cubic.splines.approximation(function(x) abs(x), seq(-1,1, length.out=16))</code></pre>
<p><img src="https://jakespiteri.co.uk/posts/Report-10_files/figure-html/unnamed-chunk-9-1.png" width="384" style="display: block; margin: auto;" />
Splines avoid the oscillating approximation phenomenon which occurs with polynomial interpolation. For the function <span class="math inline">\(f(x)=|x|\)</span>, the spline method seems to work better than the interpolating polynomial approach but it is not possible for the approximation to capture the true behavior of <span class="math inline">\(f(x)\)</span> around <span class="math inline">\(x=0\)</span>. This is because <span class="math inline">\(f\)</span> is not continuous at this point and an explicit requirement on the natural cubic spline is that it <em>is</em> continuous.</p>
<pre class="r"><code>par(mfrow=c(1,2))
plot.cubic.splines.approximation(function(x) 1/(1+25*x^2), seq(-5,5, length.out=6))
plot.cubic.splines.approximation(function(x) 1/(1+25*x^2), seq(-5,5, length.out=16))</code></pre>
<p><img src="https://jakespiteri.co.uk/posts/Report-10_files/figure-html/unnamed-chunk-10-1.png" width="1344" style="display: block; margin: auto;" /></p>
<pre class="r"><code>plot.cubic.splines.approximation(function(x) 1/(1+25*x^2), seq(-5,5, length.out=45))</code></pre>
<p><img src="https://jakespiteri.co.uk/posts/Report-10_files/figure-html/unnamed-chunk-11-1.png" width="384" style="display: block; margin: auto;" />
Above we see that Runge’s phenomenon does not occur for splines, but it is hard to produce a good approximation to Runge’s function for a small number of interpolation points. Of course <em>small</em> has little meaning — in this case it is very easy to evaluate Runge’s function for a large number of points, for other functions this may not be true.</p>
</div>
</div>
<div id="monte-carlo-integration" class="section level2">
<h2>Monte Carlo Integration</h2>
<p>The quadrature rules introduced above give excellent rates of convergence (in terms of computational cost) for low-dimensional problems. Their costs quickly become too expensive for high-dimensional problems. For such problems, a better approach is to use Monte Carlo algorithms.</p>
<p>Let <span class="math inline">\((X, \mathcal{X})\)</span> be a measurable space. We have a target probability measure <span class="math inline">\(\pi:\mathcal{X} \rightarrow [0,1]\)</span> and we would like to approximate the quantity
<span class="math display">\[\pi(f) := \int_X f(x) \pi(dx),\]</span>
where <span class="math inline">\(f \in L_1(X, \pi) = \{f:\pi(|x|)&lt;\infty\}\)</span>. I.e. <span class="math inline">\(\pi(f)\)</span> is the expectation of <span class="math inline">\(f(X)\)</span> when <span class="math inline">\(X \sim \pi\)</span>.</p>
<div id="monte-carlo-with-iid-random-variables" class="section level3">
<h3>Monte Carlo with IID Random Variables</h3>
<p>Classical Monte Carlo is a natural method for integral approximation. We can motivate its use at a high level using the Law of Large Numbers. Recall that our aim is to approximate the quantity <span class="math inline">\(\pi(f)\)</span>; below we reduce this problem to the problem of simulating random variables with distribution <span class="math inline">\(\pi\)</span>.</p>
<div id="fundamental-results" class="section level4">
<h4>Fundamental Results</h4>
<p><strong>Theorem</strong>(SLLN):
Let <span class="math inline">\((X_n)_{n\geq1}\)</span> be a sequence of iid random variables with distribution <span class="math inline">\(\mu\)</span>. Define the quantity
<span class="math display">\[S_n(f) := \sum_{i=1}^n f(X_i)\]</span>
for <span class="math inline">\(f \in L_1(X, \mu)\)</span>. Then
<span class="math display">\[\lim_{n \rightarrow \infty} \frac{1}{n}S_n(f) = \mu(f),\]</span>
almost surely.</p>
<p>The random variable <span class="math inline">\(n^{-1}S_n(f)\)</span> is a Monte Carlo approximation of <span class="math inline">\(\mu(f)\)</span>. The probabilistic convergence result does not tell us about the variance of the approximation for finite <span class="math inline">\(n\)</span>, so we compute the variance of the approximation and appeal to the Central Limit Theorem.</p>
<p><strong>Proposition</strong>(Variance):
Let <span class="math inline">\((X_n)_n\geq1\)</span> and <span class="math inline">\(S_n(f)\)</span> be as defined in the SLLN, where <span class="math inline">\(f\in L_2(X, \mu)\)</span>. Then
<span class="math display">\[Var[n^{-1}S_n(f)] = \frac{\mu(f^2) - \mu(f)^2}{n}.\]</span></p>
<p><strong>Theorem</strong>(CLT):
Let <span class="math inline">\((X_n)_n\geq1\)</span> and <span class="math inline">\(S_n(f)\)</span> be as defined in the SLLN, where <span class="math inline">\(f\in L_2(X, \mu)\)</span>. Then
<span class="math display">\[n^{1/2}\{n^{-1}S_n(f) - \mu(f)\} \xrightarrow[]{L} X \sim N(0, \mu(\bar{f}^2)),\]</span>
where <span class="math inline">\(\bar{f} = f - \mu(f)\)</span>.</p>
</div>
<div id="sampling" class="section level4">
<h4>Sampling</h4>
<p>The SLLN and CLT which justify our approximations above rely on us being able to simulate random variables from our <em>target</em> distribution <span class="math inline">\(\mu=\pi\)</span>. This is the main difficulty in Monte Carlo methods — we often have limited knowledge of <span class="math inline">\(\pi\)</span>.</p>
<p>Classical Monte Carlo methods differ in the way that they generate samples from <span class="math inline">\(\pi\)</span>. Some very well-known Monte Carlo algorithms are:</p>
<ol style="list-style-type: decimal">
<li>Rejection sampling</li>
<li>Importance sampling</li>
<li>Self-normalized importance sampling</li>
</ol>
</div>
</div>
<div id="markov-chain-monte-carlo-mcmc" class="section level3">
<h3>Markov Chain Monte Carlo (MCMC)</h3>
<p>Markov chain Monte Carlo (MCMC) methods are a development of the standard Monte Carlo techniques. These methods allow us to sample from a probability distribution by constructing a Markov chain as opposed to sampling independent random variables.</p>
<div id="fundamental-results-1" class="section level4">
<h4>Fundamental Results</h4>
<p><strong>Theorem</strong>(LLN for Markov chains):
Suppose that <span class="math inline">\(\mathbf{X}=(X_n)_{n\geq0}\)</span> is a time-homogeneous, positive Harris Markov chain with invariant probability measure <span class="math inline">\(\pi\)</span>. Then for any <span class="math inline">\(f \in L_1(X, \pi)\)</span>,
<span class="math display">\[\lim_{n \rightarrow \infty} \frac{1}{n} S_n(f) = \pi(f),\]</span>
almost surely for any initial distribution for <span class="math inline">\(X_0\)</span>.</p>
<p>Similar to classical Monte Carlo, there also exist Central Limit Theorems for Markov chains such as the following</p>
<p><strong>Theorem</strong>(A CLT for geometrically ergodic Markov chains): Assume that <span class="math inline">\(\mathbf{X}\)</span> is time-homogeneous, positive Harris and geometrically ergodic with invariant probability measure <span class="math inline">\(\pi\)</span>, and that <span class="math inline">\(\pi(|f|^{2+\delta})&lt;\infty\)</span> for some <span class="math inline">\(\delta&gt;0\)</span>. Then
<span class="math display">\[n^{1/2} \{ n^{-1} S_{n}(f) - \pi(f) \} \overset{L}{\to} N(0,\sigma^{2}(f))\]</span>
as <span class="math inline">\(n\rightarrow\infty\)</span>, where <span class="math inline">\(\bar{f}=f-\pi(f)\)</span> and
<span class="math display">\[\sigma^{2}(f)=\mathbb{E}_{\pi}\left[\bar{f}(X_{0})^{2}\right]+2\sum_{k=1}^{\infty}\mathbb{E}_{\pi}\left[\bar{f}(X_{0})\bar{f}(X_{k})\right]&lt;\infty.\]</span></p>
<p>The Law of Large Numbers and Central Limit Theorem results for Markov chains tell us that we are justified in using Markov chains to produce samples from a distribution, as long as certain conditions are met (e.g. time-homoegeneity, chain is positive Harris recurrent, etc.). Our aim is therefore to produce transition kernels and thus Markov chains which satisfy these conditions.</p>
</div>
<div id="metropolis-hastings" class="section level4">
<h4>Metropolis-Hastings</h4>
<p>Metropolis-Hastings (MH) transition kernels are perhaps the most used method of constructing Markov chains. The MH algorithm is as follows:</p>
<p>Suppose we want to sample from a target distribution <span class="math inline">\(\pi\)</span>, which has a density with respect to (w.r.t) some measure <span class="math inline">\(\lambda\)</span>. We specify a proposal Markov kernel <span class="math inline">\(Q\)</span> admitting density <span class="math inline">\(q\)</span> w.r.t <span class="math inline">\(\lambda\)</span>, i.e. <span class="math inline">\(Q(x, dz) = q(x, dz) \lambda (dz)\)</span>.</p>
<p>In order to sample from <span class="math inline">\(\pi\)</span>, we simulate according to the transition kernel <span class="math inline">\(P_{MH}(x, \cdot)\)</span>.</p>
<ol style="list-style-type: decimal">
<li><p>Initialize: Set <span class="math inline">\(X_1 = x_1\)</span>.</p></li>
<li><p>For <span class="math inline">\(t = 2, \dots\)</span></p>
<p>2a. Sample a proposal <span class="math inline">\(Z \sim Q(x_t, \cdot)\)</span>.</p>
<p>2b. Accept proposal <span class="math inline">\(Z\)</span> with the acceptance probability <span class="math inline">\(\alpha_{MH}(x_t, Z)\)</span>, where
<span class="math display">\[\alpha_{MH}(x, z) := \min\left(1, \frac{\pi(z) q(z, x)}{\pi(x) q(x, z)}\right),\]</span>
otherwise, output <span class="math inline">\(x_t\)</span>.</p></li>
</ol>
<p>Note that given the acceptance probability specified by the MH algorithm, we only need to know the density <span class="math inline">\(\pi\)</span> up to a normalizing constant. Also, in the case that a symmetric proposal (i.e. <span class="math inline">\(q(x, y) = q(y, x)\)</span> for all <span class="math inline">\(x, y\)</span>) is specified, the MH update becomes the Metropolis update.</p>
<p>Below is an implementation of the Metropolis-Hastings algorithm.</p>
<pre class="r"><code># returns the transition kernel P(x)
make.metropolis.hastings.kernel &lt;- function(pi, Q) {
  q &lt;- Q$density
  P &lt;- function(x) {
      z &lt;- Q$sample(x)
      alpha &lt;- min(1, pi(z) * q(z, x) / pi(x) / q(x, z))
      if(runif(1) &lt; alpha) {
        return(z)
      } else {
        return(x)
      }
  }
  return(P)
}

make.normal.proposal &lt;- function(sigma) {
  Q &lt;- list()
  Q$sample &lt;- function(x) {
    return(rnorm(length(x), x, sigma))
  }
  Q$density &lt;- function(x, z) {
    return(dnorm(z, x, sigma))
  }
  return(Q)
}

make.uniform.proposal &lt;- function(range) {
  Q &lt;- list()
  Q$sample &lt;- function(x) {
    return(runif(length(x), x-range/2, x+range/2))
  }
  Q$density &lt;- function(x, z) {
    return(dunif(z, z-range/2, z+range/2))
  }
  return(Q)
}

simulate.chain &lt;- function(P, x0, n) {
  xs &lt;- matrix(NA, n, length(x0))
  x &lt;- x0
  for(i in 1:n) {
    x &lt;- P(x)
    xs[i,] &lt;- x
  }
  return(xs)
}</code></pre>
<p>Suppose we want to sample from a standard normal distribution. Below we show the density of samples produced by a Metropolis-Hastings algorithm with a <span class="math inline">\(N(0,0.5^2)\)</span> proposal distribution. We plot the true target distribution <span class="math inline">\(N(0,1)\)</span> over the density of samples. We also plot the trace of our Markov chain which shows the evolution of our chain over time — this gives us some insight as to whether the chain fully explores the state space.</p>
<pre class="r"><code># generate markov chain using MH algorithm and N(0,0.5^2) proposal
# target: N(0,1)
n &lt;- 1e4
xs &lt;- simulate.chain(make.metropolis.hastings.kernel(dnorm,
                                                     make.normal.proposal(0.5)), 3, n)

# setup plot space
par(mfrow=c(1,2))

# plot the chain
plot(1:n, xs, type = &#39;l&#39;, xlab=&quot;iteration&quot;, 
     main=&quot;Trace of Markov chain&quot;)

# plot the density of the chain
plot(density(xs), main = &quot;Density of samples&quot;)
seq &lt;- seq(-10,10,length.out=1e5)
lines(seq, dnorm(seq), col=&quot;red&quot;)</code></pre>
<p><img src="https://jakespiteri.co.uk/posts/Report-10_files/figure-html/unnamed-chunk-13-1.png" width="1344" style="display: block; margin: auto;" /></p>
<p>Below we look at the density of samples produced for a varying number of iterations.</p>
<pre class="r"><code>par(mfrow = c(1,3))
for(iter in c(50, 500, 5000)) {
  # generate markov chain using MH algorithm and N(0,0.5^2) proposal
  # target: N(0,1)
  n &lt;- iter
  xs &lt;- simulate.chain(make.metropolis.hastings.kernel(dnorm,
                                                       make.normal.proposal(0.5)), 3, n)
  
  # plot the density of the chain
  plot(density(xs), main = &quot;Density of samples&quot;)
  seq &lt;- seq(-10,10,length.out=1e5)
  lines(seq, dnorm(seq), col=&quot;red&quot;)
}</code></pre>
<p><img src="https://jakespiteri.co.uk/posts/Report-10_files/figure-html/unnamed-chunk-14-1.png" width="1344" style="display: block; margin: auto;" /></p>
<p>We can also use a proposal distribution which is different from the distribution we want to sample from. Below we use a uniform proposal distribution centered at the current state of the chain with range <span class="math inline">\(5\)</span>.</p>
<pre class="r"><code># generate markov chain using MH algorithm and U[-5,5] proposal
# target: N(0,1)
n &lt;- 5e4
xs &lt;- simulate.chain(make.metropolis.hastings.kernel(dnorm,
                                                     make.uniform.proposal(5)), 3, n)

# plot the density of the chain
plot(density(xs), main = &quot;Density of samples&quot;)
seq &lt;- seq(-10,10,length.out=1e5)
lines(seq, dnorm(seq), col=&quot;red&quot;)</code></pre>
<p><img src="https://jakespiteri.co.uk/posts/Report-10_files/figure-html/unnamed-chunk-15-1.png" width="384" style="display: block; margin: auto;" /></p>
<p>Our target distribution need not be one which is given by R. In practice the distributions we need to sample from are <em>much</em> more complex than the toy problems presented here. Below we sample from a mixture distribution: The weighted mixture of two normal distributions <span class="math inline">\(Z \sim 0.6X + 0.4Y\)</span>, where <span class="math inline">\(X \sim N(-2,1)\)</span>, <span class="math inline">\(Y \sim N(5, 2^2)\)</span>. We use a <span class="math inline">\(N(0,2^2)\)</span> proposal distribution.</p>
<pre class="r"><code># generate markov chain using MH algorithm
# target: mixture of two normals
n &lt;- 1e4
target &lt;- function(x) {0.6*dnorm(x, -2, 1) + 0.4*dnorm(x, 5, 2)}
xs &lt;- simulate.chain(make.metropolis.hastings.kernel(target,
                                                     make.normal.proposal(2)), 10, n)

# setup plot space
par(mfrow=c(1,2))

# plot the chain
plot(1:n, xs, type = &#39;l&#39;, xlab=&quot;iteration&quot;, 
     main=&quot;Trace of Markov chain&quot;)

# plot the density of the chain
plot(density(xs), main = &quot;Density of samples&quot;)
seq &lt;- seq(-20,20,length.out=1e5)
lines(seq, target(seq), col=&quot;red&quot;)</code></pre>
<p><img src="https://jakespiteri.co.uk/posts/Report-10_files/figure-html/unnamed-chunk-16-1.png" width="1344" style="display: block; margin: auto;" /></p>
<p>The Metropolis-Hastings algorithm can also be used to draw samples from multi-variate distributions. Below we attempt to draw samples from a mixture of two 2D multivariate normal distributions. The contour plots highlight areas of the state space with high probability, and the path overlaid shows the steps that the MH algorithm has taken.</p>
<pre class="r"><code># higher-dimensional problem --- target: 2D multivariate normal distributions
# construct target
dmvnorm &lt;- function(x, mu, sigma) return(as.vector(
  exp(-t(x - mu) %*% solve(sigma) %*% (x - mu)/2)/sqrt((2*pi)^length(x)*det(sigma))
  ))
mu1 &lt;- c(2,2); mu2 &lt;- c(-1,-1)
sigma1 &lt;- matrix(c(1,0.3,0.3,1),2,2); sigma2 &lt;- matrix(c(1,-0.1,-0.1,1),2,2)
target &lt;- function(x) return(dmvnorm(x, mu1, sigma1) + dmvnorm(x, mu2, sigma2))

# evaluate density of target over grid
seq &lt;- seq(-5,6,length.out=5e2)
mat &lt;- outer(seq, seq, Vectorize(function(x, y) target(c(x,y))))
rownames(mat) &lt;- colnames(mat) &lt;- seq

# make contour plot
longmat &lt;- melt(mat) #reshape data
p1 &lt;- longmat %&gt;%
  ggplot() +
  geom_raster(aes(x = X1, y = X2, fill = value)) +
  geom_contour(aes(x = X1, y = X2, z = value), col = &quot;white&quot;) +
  labs(title=&quot;Contour plot of the target&quot;)

# simulate chain
n &lt;- 300
xs &lt;- simulate.chain(make.metropolis.hastings.kernel(target,
                                                     make.normal.proposal(3)), c(-5,0), n)

# make plot of chain over contour
p2 &lt;- longmat %&gt;%
  ggplot() +
  geom_raster(aes(x = X1, y = X2, fill = value)) +
  geom_contour(aes(x = X1, y = X2, z = value), col = &quot;white&quot;) +
  geom_path(data = as.data.frame(xs), aes(x=V1, y = V2), col=&quot;red&quot;) +
  geom_point(data = as.data.frame(xs), 
             aes(x=V1, y = V2), pch=21, fill=&quot;red&quot;, col=&quot;white&quot;) +
  labs(title=&quot;Contour plot of the target&quot;)

# plot p1, p2 side-by-side
grid.arrange(p1,p2,ncol=2)</code></pre>
<p><img src="https://jakespiteri.co.uk/posts/Report-10_files/figure-html/unnamed-chunk-17-1.png" width="1536" style="display: block; margin: auto;" /></p>
</div>
<div id="mcmc-in-practice" class="section level4">
<h4>MCMC in practice</h4>
<p>Implementing MCMC methods in practice can be quite simple, but there are a few things that we need to be aware of to ensure that our algorithm is working well.</p>
<p>First we introduce the concept of <strong>pseudo-convergence</strong>. This occurs when there are regions of the state space with high probability that are poorly connected by the Markov chain. The chain may spend a long time exploring one high probability region, which falsely leads us to believe that the chain has converged to the stationary distribution. This phenomenon occurs frequently with multimodal distributions as we demonstrate below. If the modes are so far from one another such that the proposal distribution cannot propose values in one high-probability region when in another, the chain will fail to converge.</p>
<p>In the example below we attempt to sample from a univariate bimodal distribution. In the first scenario, we use a uniform proposal distribution with an interval length such that the chain cannot travel between modes. We repeat the simulation but with a larger interval for the proposal distribution.</p>
<pre class="r"><code># target: sum of two uniform densities
# proposal with interval length 2
n &lt;- 1e3
target &lt;- function(x) {0.5*dunif(x, -2,-1) + 0.5*dunif(x, 1, 2)}
xs &lt;- simulate.chain(make.metropolis.hastings.kernel(target,
                                                     make.uniform.proposal(2)), 2, n)

# setup plot space
par(mfrow=c(1,2))

# plot the chain
plot(xs, type = &#39;l&#39;, ylim=c(-2,2), xlab=&quot;iteration&quot;, 
     main=&quot;Trace of Markov chain&quot;)

# plot the density of the chain
plot(density(xs), main = &quot;Density of samples&quot;, xlim=c(-3,3))
seq &lt;- seq(-20,20,length.out=1e5)
lines(seq, target(seq), col=&quot;red&quot;)</code></pre>
<p><img src="https://jakespiteri.co.uk/posts/Report-10_files/figure-html/unnamed-chunk-18-1.png" width="1536" style="display: block; margin: auto;" /></p>
<pre class="r"><code># target: sum of two uniform densities
# proposal with interval length 5
n &lt;- 1e4
target &lt;- function(x) {0.5*dunif(x, -2,-1) + 0.5*dunif(x, 1, 2)}
xs &lt;- simulate.chain(make.metropolis.hastings.kernel(target,
                                                     make.uniform.proposal(5)), 2, n)

# setup plot space
par(mfrow=c(1,2))

# plot the chain
plot(xs, type = &#39;l&#39;, ylim=c(-2,2), xlab=&quot;iteration&quot;, 
     main=&quot;Trace of Markov chain&quot;)

# plot the density of the chain
plot(density(xs), main = &quot;Density of samples&quot;, xlim=c(-3,3))
seq &lt;- seq(-20,20,length.out=1e5)
lines(seq, target(seq), col=&quot;red&quot;)</code></pre>
<p><img src="https://jakespiteri.co.uk/posts/Report-10_files/figure-html/unnamed-chunk-18-2.png" width="1536" style="display: block; margin: auto;" /></p>
<p>One way that we may attempt to overcome the pseudo-convergence phenomenon is to run <em>multiple chains</em>. This simply means that we run the MCMC algorithm multiple times with different initializations. If the chain seems to have converged to different distributions for different initializations, then the algorithm has failed to fully explore the state space. We may need to run the chain for a longer period of time until it converges to the target distribution, or we may need to adjust the proposal distribution to improve the chain’s mixing. This introduces a new problem: given a fixed amount of computation, should we produce one long chain or multiple chains? This `multistart heuristic’ should be used with caution — it only guarantees against pseudo-convergence if we can ensure that the starting points cover every part of the state space which may pseudo-converge.</p>
<p>Another practical aspect of MCMC methods that we should be aware of is <strong>burn-in</strong>. When initializing our algorithm it may take some time before the Markov chain converges to a stationary distribution, and we call this the burn-in period. The samples produced before the chain has converged are not likely to have been produced by the target distribution, and so we discard these samples. The best way to see which samples to discard is to manually inspect the chain in order to see at which iteration it began to converge.</p>
<p>Below we parallelize our computations and run one chain per one physical CPU core. The details of the target distribution in this example do not matter — often we do not know them in practice — the point is that we can observe pseudo-convergence in the trace plots of the chains. This occurs because our proposal distribution has a small variance and thus the chains cannot fully explore the state space.</p>
<pre class="r"><code>simulate.chains &lt;- function(P, x0, n, n.chains, n.cores = 4) {
  require(parallel)
  require(doParallel)
  # parallelize chains
  registerDoParallel(n.cores)
  simulations &lt;- foreach(i=1:n.chains, .combine=&quot;rbind&quot;) %dopar% {
    x &lt;- x0[i]
    foreach(i=1:n, .combine=&quot;c&quot;) %do% {P(x)}
  }
  stopImplicitCluster()
  return(simulations)
}

# generate markov chain using MH algorithm
# target: mixture of two normals
n &lt;- 5e3; set.seed(123)
target &lt;- function(x) {0.6*dnorm(x, -2, 1) + 0.4*dnorm(x, 5, 2)}
xs &lt;- simulate.chains(make.metropolis.hastings.kernel(target,
                                                      make.normal.proposal(1)), 
                      runif(4,-10,10), n, 4)</code></pre>
<pre><code>## Loading required package: parallel</code></pre>
<pre><code>## Loading required package: doParallel</code></pre>
<pre><code>## Loading required package: foreach</code></pre>
<pre><code>## 
## Attaching package: &#39;foreach&#39;</code></pre>
<pre><code>## The following objects are masked from &#39;package:purrr&#39;:
## 
##     accumulate, when</code></pre>
<pre><code>## Loading required package: iterators</code></pre>
<pre class="r"><code># setup plot space
par(mfrow=c(2,2))

# plot the chains
ylimits &lt;- c(min(xs), max(xs))
for(i in 1:4){
  plot(1:n, xs[i,], type = &#39;l&#39;, xlab=&quot;iteration&quot;, ylim=ylimits,
       ylab=&quot;x&quot;, main=paste0(&quot;Trace of Markov chain &quot;, i))
}</code></pre>
<p><img src="https://jakespiteri.co.uk/posts/Report-10_files/figure-html/unnamed-chunk-19-1.png" width="960" style="display: block; margin: auto;" /></p>
</div>
</div>
</div>
</div>

            </div>
        </article>

        <hr />

        <div class="post-info">

            <p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file-text"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>4214 Words</p>

            <p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-calendar"><rect x="3" y="4" width="18" height="18" rx="2" ry="2"></rect><line x1="16" y1="2" x2="16" y2="6"></line><line x1="8" y1="2" x2="8" y2="6"></line><line x1="3" y1="10" x2="21" y2="10"></line></svg>0000-12-31 23:58 -0001</p>
        </div>

        

        
    </main>

            </div>

            
                <footer class="footer">
    <div class="footer__inner">
        <div class="footer__content">
            <span>&copy; 2020</span>
            
                <span><a href="https://jakespiteri.co.uk">Jake Spiteri</a></span>
            
            <span><a href="https://creativecommons.org/licenses/by-nc/4.0/" target="_blank" rel="noopener">CC BY-NC 4.0</a></span>
            <span> <a href="https://jakespiteri.co.uk/posts/index.xml" target="_blank" title="rss"><svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 20 20" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-rss"><path d="M4 11a9 9 0 0 1 9 9"></path><path d="M4 4a16 16 0 0 1 16 16"></path><circle cx="5" cy="19" r="1"></circle></svg></a></span>
        </div>
    </div>
    <div class="footer__inner">
    </div>
</footer>
    



<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js"></script>



<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/languages/r.min.js"></script>
<script>hljs.configure({languages: []}); hljs.initHighlightingOnLoad();</script>


    <script src="https://jakespiteri.co.uk/js/math-code.js"></script>
<script async src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>

  </body>
</html>
            
        </div>

        




<script type="text/javascript" src="https://jakespiteri.co.uk/bundle.min.2d5469329143160ae2456a69c3c76dc2d0a3b212b46afe291a51bd68650ed6f8697e001dab54f1c272c77ce08092a8c55e5bb4314e0ee334aab4b927ec896638.js" integrity="sha512-LVRpMpFDFgriRWppw8dtwtCjshK0av4pGlG9aGUO1vhpfgAdq1TxwnLHfOCAkqjFXlu0MU4O4zSqtLkn7IlmOA=="></script>



    </body>
</html>
