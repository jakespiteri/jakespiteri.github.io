<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Portfolio Report 10: Integration and Markov Chain Monte Carlo | Jake Spiteri</title><meta name=keywords content><meta name=description content="Numerical Integration Quadrature “Quadrature rules” are integral approximations which use a finite number of evaluations of the function. All of the quadrature rules below approximate a function using interpolating polynomials.
We will first look at some key ideas in numerical integration by approximating definite integrals over a finite interval. We will then extend these ideas to semi-infinite and infinite intervals. We will also consider multiple integrals.
In practice we can use Rs integrate function for one-dimensional integrals, and the cubature package for multiple integrals."><meta name=author content="Jake Spiteri"><link rel=canonical href=https://jakespiteri.co.uk/portfolio/computing-1/report-10/><meta name=google-site-verification content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.5cfc680b1eeaeef9efbced92d46c2a9e876b72ee14fba85846afc4cff9e6e6f8.css integrity="sha256-XPxoCx7q7vnvvO2S1Gwqnodrcu4U+6hYRq/Ez/nm5vg=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://jakespiteri.co.uk/favi.png><link rel=icon type=image/png sizes=16x16 href=https://jakespiteri.co.uk/favi.png><link rel=icon type=image/png sizes=32x32 href=https://jakespiteri.co.uk/favi.png><link rel=apple-touch-icon href=https://jakespiteri.co.uk/favi.png><link rel=mask-icon href=https://jakespiteri.co.uk/favi.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-123-45","auto"),ga("send","pageview"))</script><meta property="og:title" content="Portfolio Report 10: Integration and Markov Chain Monte Carlo"><meta property="og:description" content="Numerical Integration Quadrature “Quadrature rules” are integral approximations which use a finite number of evaluations of the function. All of the quadrature rules below approximate a function using interpolating polynomials.
We will first look at some key ideas in numerical integration by approximating definite integrals over a finite interval. We will then extend these ideas to semi-infinite and infinite intervals. We will also consider multiple integrals.
In practice we can use Rs integrate function for one-dimensional integrals, and the cubature package for multiple integrals."><meta property="og:type" content="article"><meta property="og:url" content="https://jakespiteri.co.uk/portfolio/computing-1/report-10/"><meta property="og:image" content="https://jakespiteri.co.uk/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="portfolio"><meta property="og:site_name" content="Jake Spiteri"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://jakespiteri.co.uk/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Portfolio Report 10: Integration and Markov Chain Monte Carlo"><meta name=twitter:description content="Numerical Integration Quadrature “Quadrature rules” are integral approximations which use a finite number of evaluations of the function. All of the quadrature rules below approximate a function using interpolating polynomials.
We will first look at some key ideas in numerical integration by approximating definite integrals over a finite interval. We will then extend these ideas to semi-infinite and infinite intervals. We will also consider multiple integrals.
In practice we can use Rs integrate function for one-dimensional integrals, and the cubature package for multiple integrals."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Portfolio","item":"https://jakespiteri.co.uk/portfolio/"},{"@type":"ListItem","position":2,"name":"Statistical Computing 1","item":"https://jakespiteri.co.uk/portfolio/computing-1/"},{"@type":"ListItem","position":3,"name":"Portfolio Report 10: Integration and Markov Chain Monte Carlo","item":"https://jakespiteri.co.uk/portfolio/computing-1/report-10/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Portfolio Report 10: Integration and Markov Chain Monte Carlo","name":"Portfolio Report 10: Integration and Markov Chain Monte Carlo","description":"Numerical Integration Quadrature “Quadrature rules” are integral approximations which use a finite number of evaluations of the function. All of the quadrature rules below approximate a function using interpolating polynomials.\nWe will first look at some key ideas in numerical integration by approximating definite integrals over a finite interval. We will then extend these ideas to semi-infinite and infinite intervals. We will also consider multiple integrals.\nIn practice we can use Rs integrate function for one-dimensional integrals, and the cubature package for multiple integrals.","keywords":[],"articleBody":" Numerical Integration Quadrature “Quadrature rules” are integral approximations which use a finite number of evaluations of the function. All of the quadrature rules below approximate a function using interpolating polynomials.\nWe will first look at some key ideas in numerical integration by approximating definite integrals over a finite interval. We will then extend these ideas to semi-infinite and infinite intervals. We will also consider multiple integrals.\nIn practice we can use Rs integrate function for one-dimensional integrals, and the cubature package for multiple integrals.\nPolynomial Interpolation We consider approximating a continuous function \\(f\\) on \\([a,b]\\) (this can be written \\(f \\in C^0([a,b])\\)), using a polynomial function \\(p\\). We know how to compute integrals of polynomials exactly, and so if \\(p\\) is a good approximation of our integrand then we can compute an accurate approximation to the integral. The Weierstrass Approximation Theorem provides a high-level motivation.\nWeierstrass Approximation Theorem. Let \\(f \\in C^0([a,b])\\). There exists a sequence of polynomials \\((p_n)\\) that converges uniformly to \\(f\\) on \\([a,b]\\). That is, \\[ \\|f - p_n\\|_\\infty = \\max_{x \\in [a,b]} |f(x) - p_n(x)| \\rightarrow 0.\\]\nThe above theorem suggests that for any given tolerance, we can find a polynomial which approximates a function \\(f \\in C^0([a,b])\\) up to said tolerance. The Weierstrass theorem tells us that such polynomials exist, but it does not tell us how to find them. The real difficulty lies in constructing such polynomials in a computationally-efficient manner without a strong knowledge of the function \\(f\\).\nLagrange polynomials We can approximate a function \\(f\\) by using an interpolating polynomial with \\(k\\) points \\(\\{x_i, f(x_i)\\}_{i=1}^n\\). The interpolating polynomial produced is unique, has at most \\(k-1\\) degree, and can be written as a Lagrange polynomial: \\[p_{k-1}(x) := \\sum_{i=1}^k \\ell_i(x) f(x_i),\\] where the Lagrange basis polynomials are \\[\\ell_i(x) = \\prod_{j=1, j\\neq i}^p \\frac{x - x_j}{x_i - x_j}, \\quad i \\in \\{1, \\dots, k\\}.\\]\nBelow is code implementing interpolating polynomials. For a polynomial with degree \\(3\\) we can produce the polynomial approximations for \\(k \\in \\{2,3,4\\}\\) and for specific choices of \\(x_1, \\dots, x_4\\).\nconstruct.interpolating.polynomial \u003c- function(f, xs) { k \u003c- length(xs) fxs \u003c- f(xs) p \u003c- function(x) { value \u003c- 0 for (i in 1:k) { fi \u003c- fxs[i] zs \u003c- xs[setdiff(1:k,i)] li \u003c- prod((x-zs)/(xs[i]-zs)) value \u003c- value + fi*li } return(value) } return(p) } plot.polynomial.approximation \u003c- function(f, xs, a, b) { p \u003c- construct.interpolating.polynomial(f, xs) vs \u003c- seq(a, b, length.out=500) plot(vs, f(vs), type='l', xlab=\"x\", ylab=\"black: f(x), red: p(x)\") points(xs, f(xs), pch=20) lines(vs, vapply(vs, p, 0), col=\"red\") } a \u003c- -4 b \u003c- 4 f \u003c- function(x) { return(-x^3 + 3*x^2 - 4*x + 1) } par(mfrow=c(1,3)) plot.polynomial.approximation(f, c(-2, 2), a, b) plot.polynomial.approximation(f, c(-2,0,2), a, b) plot.polynomial.approximation(f, c(-2, 0, 2, 4), a, b) Interpolation Error and Convergence Clearly we can approximate a polynomial using interpolating polynomials for a \\(k\\) which is large enough. However, we are often interested in approximating functions which are not polynomials.\nInterpolation Error Theorem. Let \\(f \\in C^k[a,b]\\), and \\(p_{k-1}\\) be the polynomial interpolating \\(f\\) at the \\(k\\) points \\(x_1, \\dots, x_k\\). Then for any \\(x \\in [a,b]\\) there exists \\(\\xi \\in (a,b)\\) such that \\[f(x) - p_{k-1}(x) = \\frac{1}{k!}f^{(k)}(\\xi) \\prod_{i=1}^k (x - x_i).\\] The above theorem looks promising, but it does not tell us how to choose our interpolating polynomials such that they converge uniformly (or even pointwise) to \\(f\\). The theorem tells us that one way of minimizing the error is to choose the interpolation points such that the product \\(|\\prod_{i=1}^k (x - x_k)|\\) is as small as possible. The Chebyshev interpolation points detailed below do this.\nWe know that there exists a sequence of interpolation points that provide us with uniform convergence.\nTheorem. Let \\(f \\in C^0[a,b]\\). There exists a sequence of sets of interpolation points \\(X_1, X_2, \\dots\\) such that the corresponding sequence of interpolating polynomials converges uniformly to \\(f\\) on \\([a,b]\\).\nThe above tells us that given a function \\(f\\), it is indeed possible to find a sequence of sets of interpolating points which produce a sequence of interpolating polynomials that converge uniformly to \\(f\\). We may then wonder if there is a universal sequence of sets of interpolating points that provide us with uniform convergence to any function \\(f\\). This is not possible, as for any fixed sequence of sets we can find a function \\(f\\) for which our sequence of interpolating polynomials diverges.\nTheorem. For any fixed sequence of sets of interpolation points there exists a continuous function \\(f\\in C^0[a,b]\\) for which the sequence of interpolating polynomials diverges on \\([a,b]\\).\nExample: Consider the sequence of interpolating points \\(X_k\\) to be the set of \\(k\\) uniformly spaced points including a, and b if \\(k\u003e1\\).\nThis is a particularly bad way to produce sets of interpolation points as uniform convergence is not guaranteed — even for infinitely differentiable functions. There are of course functions which these interpolation points work well for, as seen below.\nconstruct.uniform.point.set \u003c- function(a, b, k) { if (k==1) return(a) return(seq(a, b, length.out=k)) } a \u003c- 1 b \u003c- 2 plot.polynomial.approximation(log, construct.uniform.point.set(a, b, 10), a, b) Below is a plot of the Runge function defined by \\(f(x) = \\frac{1}{1+x^2}\\) over the interval \\([-5,5]\\). We see that the interpolation error \\(\\|f-p_n\\|_\\infty\\) grows without bound as \\(n \\rightarrow \\infty\\).\nBelow we look at the polynomial interpolation of the Runge function \\(f(x) = \\frac{1}{1+x^2}\\), over the interval \\([-5,5]\\).\na \u003c- -5 b \u003c- 5 f \u003c- function(x) return(1/(1+x^2)) plot.polynomial.approximation(f, construct.uniform.point.set(a,b,50), a, b) Note that this does not contradict the Interpolation Error Theorem as the maximum of the \\(k\\)’th derivative of the Runge function grows quickly with \\(k\\). As this outweighs the decreasing product term, the interpolation error grows without bound as \\(k \\rightarrow \\infty\\).\nWe can also look at the function \\(f(x) = |x|\\) over \\([-1,1]\\). We see that the interpolating polynomial oscillates wildly for large \\(k\\).\na \u003c- -1 b \u003c- 1 par(mfrow=c(1,3)) plot.polynomial.approximation(abs, construct.uniform.point.set(a,b,50), a, b) plot.polynomial.approximation(abs, construct.uniform.point.set(a,b,100), a, b) plot.polynomial.approximation(abs, construct.uniform.point.set(a,b,200), a, b) Note once again that this does not contradict the Interpolation Error Theorem as the function \\(f(x)\\) is not differentiable at \\(0\\).\nAs mentioned above, we can avoid this problem by minimizing the interpolation error by minimizing the product term. If we minimize the maximum absolute value of the product term, we can derive the Chebyshev interpolation points. For any given \\(k\\), we have the points \\[\\cos\\left(\\frac{2i-1}{2k}\\pi\\right), \\quad i \\in \\{1, \\dots, k\\},\\] and the absolute value of the product term is then bounded above by \\(2^{1-k}\\)\nThese points clearly do not minimize the overall error.\nconstruct.chebyshev.point.set \u003c- function(k) { return(cos((2*(1:k)-1)/2/k*pi)) } # visualize the Chebyshev points chebyshev.visalization \u003c- function(k){ df \u003c- data.frame(k = numeric(), points = list()) for(i in 1:k){ df \u003c- rbind(df, data.frame(k=i, points = construct.chebyshev.point.set(i))) } ggplot(df, aes(color=k, x = points, y = k)) + geom_point(aes(color=k)) + labs(y=\"number of points k\", xlab=\"\") } # produce visualization chebyshev.visalization(50) Above we clearly see that as our number of points \\(k\\) increases, the points tend to cluster around \\(-1\\), and \\(1\\). We can use the Chebyshev interpolation points as seen below.\nplot.polynomial.approximation(f, construct.chebyshev.point.set(50), a, b) plot.polynomial.approximation(abs, construct.chebyshev.point.set(50), a, b) There is a large improvement in the error as the approximation no longer oscillates. As mentioned in the theorem above, there exist functions \\(f\\) for which the interpolating polynomials with the Chebyshev interpolating points diverge.\nComposite Polynomial Interpolation Another way to approximate a function is to use different polynomials over subintervals of the domain. This results in a piecewise polynomial approximation which is not necessarily continuous, but can be made to be.\nOther Polynomial Interpolation Schemes There are many other schemes that we may use to approximate a function. We may use Hermite interpolation which fits a polynomial using evaluations of \\(f\\) and evaluations of the derivatives of \\(f\\). If we ensure that the derivatives at the boundaries of the subintervals are equal then the resulting approximation will have a certain number of continuous derivatives. This idea is known as spline interpolation, and is often preferred to polynomial interpolation as it avoids the problem of Runge’s phenomenon demonstrated above. Spline interpolation also produces a small interpolation error even when using low degree polynomials.\nBelow is an implementation of natural cubic spline interpolation. For a set of interpolation points \\(\\{x_i\\}_{i=1}^n\\), a cubic spline fits a polynomial of at most degree \\(3\\) between each pair of interpolating points. Hence for a set of \\(n\\) interpolation points, \\(n-1\\) splines are produced to approximate the function. A set of constraints are placed upon the values these splines can take, which ensure that the spline is continuous over the interval. A natural cubic spline has degree \\(3\\) with continuity \\(C^2\\), and has the general form \\[S_j(x) = a_j + b_j(x - x_j) + c_j(x - x_j)^2 + d_j(x - x_j)^3, \\quad j = 0, \\dots, n-1\\] for which the individual splines must satisfy \\[\\begin{align*} \u0026S_i(x_i) \u0026\u0026= y_i \\qquad \u0026\u0026i = 0, \\dots, n-1\\\\ \u0026S_{i-1}(x_i) \u0026\u0026= y_i \u0026\u0026i = 1, \\dots, n\\\\ \u0026S'_{i}(x_i) \u0026\u0026= S'_{i-1}(x_i) \u0026\u0026i = 1, \\dots, n-1 \\\\ \u0026S''_{i}(x_i) \u0026\u0026= S''_{i-1}(x_i) \u0026\u0026i = 1, \\dots, n-1 \\\\ \u0026S''_{0}(x_0) \u0026\u0026= S''_{n-1}(x_{n-1}) = 0 \u0026\u0026i = 1, \\dots, n-1 \\\\ \\end{align*}\\]\nAn implementation can be seen below.\n# compute spline parameters create.spline.params \u003c- function(f, xs) { n \u003c- length(xs) - 1 fxs \u003c- f(xs) b \u003c- d \u003c- rep(0,n) h \u003c- diff(xs) hlag2 \u003c- diff(xs, lag=2) alpha \u003c- 3/h[-1]*diff(fxs)[-1] - 3/h[-n]*diff(fxs)[-n] c \u003c- l \u003c- mu \u003c- z \u003c- rep(0,n+1) l[1] \u003c- 1; mu[1] \u003c- z[1] \u003c- 0 for (i in 2:(n)) { l[i] \u003c- 2*hlag2[i-1] - h[i-1]*mu[i-1] mu[i] \u003c- h[i]/l[i] z[i] \u003c- (alpha[i-1]-h[i-1]*z[i-1])/l[i] } l[n+1] \u003c- 1; z[n+1] \u003c- c[n+1] \u003c- 0 for (j in n:1) { c[j] \u003c- z[j] - mu[j]*c[j+1] b[j] \u003c- (fxs[j+1] - fxs[j])/h[j] - h[j]*(c[j+1] + 2*c[j])/3 d[j] \u003c- (c[j+1]-c[j])/(3*h[j]) } return(list(a=fxs, b = b, c = c, d = d)) } # given parameters and the interval, return appropriate spline function # this is needed as for n interpolation points, n-1 splines are produced construct.spline \u003c- function(a, b, c, d, i, xs) { return(function(x) a[i] + b[i]*(x - xs[i]) + c[i]*(x - xs[i])^2 + d[i]*(x - xs[i])^3) } # given x and interpolation points, return spline index get.spline.subinterval \u003c- function(x, xs){ n \u003c- length(xs-1) for(i in 1:n) { if (between(x, xs[i], xs[i+1])) return(i) } } # produce and plot the spline approximation for a function f and interpolation points xs plot.cubic.splines.approximation \u003c- function(f, xs){ par \u003c- create.spline.params(f, xs) spline \u003c- function(x) construct.spline(par$a, par$b, par$c, par$d, get.spline.subinterval(x, xs), xs)(x) vs \u003c- seq(min(xs), max(xs), length.out=500) plot(vs, f(vs), type='l', xlab=\"x\", ylab=\"black: f(x), red: p(x)\") abline(v=xs, col = \"grey\") lines(vs, f(vs)) lines(vs, vapply(vs, spline, 0), col=\"red\") points(xs, f(xs), pch=20, col=\"blue\") } par(mfrow=c(1,2)) plot.cubic.splines.approximation(sin, seq(0,4*pi, length.out=4)) plot.cubic.splines.approximation(sin, seq(0,4*pi, length.out=12)) We see that our approximation is not very accurate when we have a small set of interpolation points, but we can approximate the \\(\\sin\\) function impressively well with only \\(12\\) interpolation points and with polynomials of at most degree \\(3\\)!\nplot.cubic.splines.approximation(function(x) abs(x), seq(-1,1, length.out=16)) Splines avoid the oscillating approximation phenomenon which occurs with polynomial interpolation. For the function \\(f(x)=|x|\\), the spline method seems to work better than the interpolating polynomial approach but it is not possible for the approximation to capture the true behavior of \\(f(x)\\) around \\(x=0\\). This is because \\(f\\) is not continuous at this point and an explicit requirement on the natural cubic spline is that it is continuous.\npar(mfrow=c(1,2)) plot.cubic.splines.approximation(function(x) 1/(1+25*x^2), seq(-5,5, length.out=6)) plot.cubic.splines.approximation(function(x) 1/(1+25*x^2), seq(-5,5, length.out=16)) plot.cubic.splines.approximation(function(x) 1/(1+25*x^2), seq(-5,5, length.out=45)) Above we see that Runge’s phenomenon does not occur for splines, but it is hard to produce a good approximation to Runge’s function for a small number of interpolation points. Of course small has little meaning — in this case it is very easy to evaluate Runge’s function for a large number of points, for other functions this may not be true.\nMonte Carlo Integration The quadrature rules introduced above give excellent rates of convergence (in terms of computational cost) for low-dimensional problems. Their costs quickly become too expensive for high-dimensional problems. For such problems, a better approach is to use Monte Carlo algorithms.\nLet \\((X, \\mathcal{X})\\) be a measurable space. We have a target probability measure \\(\\pi:\\mathcal{X} \\rightarrow [0,1]\\) and we would like to approximate the quantity \\[\\pi(f) := \\int_X f(x) \\pi(dx),\\] where \\(f \\in L_1(X, \\pi) = \\{f:\\pi(|x|)\u003c\\infty\\}\\). I.e. \\(\\pi(f)\\) is the expectation of \\(f(X)\\) when \\(X \\sim \\pi\\).\nMonte Carlo with IID Random Variables Classical Monte Carlo is a natural method for integral approximation. We can motivate its use at a high level using the Law of Large Numbers. Recall that our aim is to approximate the quantity \\(\\pi(f)\\); below we reduce this problem to the problem of simulating random variables with distribution \\(\\pi\\).\nFundamental Results Theorem(SLLN): Let \\((X_n)_{n\\geq1}\\) be a sequence of iid random variables with distribution \\(\\mu\\). Define the quantity \\[S_n(f) := \\sum_{i=1}^n f(X_i)\\] for \\(f \\in L_1(X, \\mu)\\). Then \\[\\lim_{n \\rightarrow \\infty} \\frac{1}{n}S_n(f) = \\mu(f),\\] almost surely.\nThe random variable \\(n^{-1}S_n(f)\\) is a Monte Carlo approximation of \\(\\mu(f)\\). The probabilistic convergence result does not tell us about the variance of the approximation for finite \\(n\\), so we compute the variance of the approximation and appeal to the Central Limit Theorem.\nProposition(Variance): Let \\((X_n)_n\\geq1\\) and \\(S_n(f)\\) be as defined in the SLLN, where \\(f\\in L_2(X, \\mu)\\). Then \\[Var[n^{-1}S_n(f)] = \\frac{\\mu(f^2) - \\mu(f)^2}{n}.\\]\nTheorem(CLT): Let \\((X_n)_n\\geq1\\) and \\(S_n(f)\\) be as defined in the SLLN, where \\(f\\in L_2(X, \\mu)\\). Then \\[n^{1/2}\\{n^{-1}S_n(f) - \\mu(f)\\} \\xrightarrow[]{L} X \\sim N(0, \\mu(\\bar{f}^2)),\\] where \\(\\bar{f} = f - \\mu(f)\\).\nSampling The SLLN and CLT which justify our approximations above rely on us being able to simulate random variables from our target distribution \\(\\mu=\\pi\\). This is the main difficulty in Monte Carlo methods — we often have limited knowledge of \\(\\pi\\).\nClassical Monte Carlo methods differ in the way that they generate samples from \\(\\pi\\). Some very well-known Monte Carlo algorithms are:\nRejection sampling Importance sampling Self-normalized importance sampling Markov Chain Monte Carlo (MCMC) Markov chain Monte Carlo (MCMC) methods are a development of the standard Monte Carlo techniques. These methods allow us to sample from a probability distribution by constructing a Markov chain as opposed to sampling independent random variables.\nFundamental Results Theorem(LLN for Markov chains): Suppose that \\(\\mathbf{X}=(X_n)_{n\\geq0}\\) is a time-homogeneous, positive Harris Markov chain with invariant probability measure \\(\\pi\\). Then for any \\(f \\in L_1(X, \\pi)\\), \\[\\lim_{n \\rightarrow \\infty} \\frac{1}{n} S_n(f) = \\pi(f),\\] almost surely for any initial distribution for \\(X_0\\).\nSimilar to classical Monte Carlo, there also exist Central Limit Theorems for Markov chains such as the following\nTheorem(A CLT for geometrically ergodic Markov chains): Assume that \\(\\mathbf{X}\\) is time-homogeneous, positive Harris and geometrically ergodic with invariant probability measure \\(\\pi\\), and that \\(\\pi(|f|^{2+\\delta})\u003c\\infty\\) for some \\(\\delta\u003e0\\). Then \\[n^{1/2} \\{ n^{-1} S_{n}(f) - \\pi(f) \\} \\overset{L}{\\to} N(0,\\sigma^{2}(f))\\] as \\(n\\rightarrow\\infty\\), where \\(\\bar{f}=f-\\pi(f)\\) and \\[\\sigma^{2}(f)=\\mathbb{E}_{\\pi}\\left[\\bar{f}(X_{0})^{2}\\right]+2\\sum_{k=1}^{\\infty}\\mathbb{E}_{\\pi}\\left[\\bar{f}(X_{0})\\bar{f}(X_{k})\\right]\u003c\\infty.\\]\nThe Law of Large Numbers and Central Limit Theorem results for Markov chains tell us that we are justified in using Markov chains to produce samples from a distribution, as long as certain conditions are met (e.g. time-homoegeneity, chain is positive Harris recurrent, etc.). Our aim is therefore to produce transition kernels and thus Markov chains which satisfy these conditions.\nMetropolis-Hastings Metropolis-Hastings (MH) transition kernels are perhaps the most used method of constructing Markov chains. The MH algorithm is as follows:\nSuppose we want to sample from a target distribution \\(\\pi\\), which has a density with respect to (w.r.t) some measure \\(\\lambda\\). We specify a proposal Markov kernel \\(Q\\) admitting density \\(q\\) w.r.t \\(\\lambda\\), i.e. \\(Q(x, dz) = q(x, dz) \\lambda (dz)\\).\nIn order to sample from \\(\\pi\\), we simulate according to the transition kernel \\(P_{MH}(x, \\cdot)\\).\nInitialize: Set \\(X_1 = x_1\\).\nFor \\(t = 2, \\dots\\)\n2a. Sample a proposal \\(Z \\sim Q(x_t, \\cdot)\\).\n2b. Accept proposal \\(Z\\) with the acceptance probability \\(\\alpha_{MH}(x_t, Z)\\), where \\[\\alpha_{MH}(x, z) := \\min\\left(1, \\frac{\\pi(z) q(z, x)}{\\pi(x) q(x, z)}\\right),\\] otherwise, output \\(x_t\\).\nNote that given the acceptance probability specified by the MH algorithm, we only need to know the density \\(\\pi\\) up to a normalizing constant. Also, in the case that a symmetric proposal (i.e. \\(q(x, y) = q(y, x)\\) for all \\(x, y\\)) is specified, the MH update becomes the Metropolis update.\nBelow is an implementation of the Metropolis-Hastings algorithm.\n# returns the transition kernel P(x) make.metropolis.hastings.kernel \u003c- function(pi, Q) { q \u003c- Q$density P \u003c- function(x) { z \u003c- Q$sample(x) alpha \u003c- min(1, pi(z) * q(z, x) / pi(x) / q(x, z)) if(runif(1) \u003c alpha) { return(z) } else { return(x) } } return(P) } make.normal.proposal \u003c- function(sigma) { Q \u003c- list() Q$sample \u003c- function(x) { return(rnorm(length(x), x, sigma)) } Q$density \u003c- function(x, z) { return(dnorm(z, x, sigma)) } return(Q) } make.uniform.proposal \u003c- function(range) { Q \u003c- list() Q$sample \u003c- function(x) { return(runif(length(x), x-range/2, x+range/2)) } Q$density \u003c- function(x, z) { return(dunif(z, z-range/2, z+range/2)) } return(Q) } simulate.chain \u003c- function(P, x0, n) { xs \u003c- matrix(NA, n, length(x0)) x \u003c- x0 for(i in 1:n) { x \u003c- P(x) xs[i,] \u003c- x } return(xs) } Suppose we want to sample from a standard normal distribution. Below we show the density of samples produced by a Metropolis-Hastings algorithm with a \\(N(0,0.5^2)\\) proposal distribution. We plot the true target distribution \\(N(0,1)\\) over the density of samples. We also plot the trace of our Markov chain which shows the evolution of our chain over time — this gives us some insight as to whether the chain fully explores the state space.\n# generate markov chain using MH algorithm and N(0,0.5^2) proposal # target: N(0,1) n \u003c- 1e4 xs \u003c- simulate.chain(make.metropolis.hastings.kernel(dnorm, make.normal.proposal(0.5)), 3, n) # setup plot space par(mfrow=c(1,2)) # plot the chain plot(1:n, xs, type = 'l', xlab=\"iteration\", main=\"Trace of Markov chain\") # plot the density of the chain plot(density(xs), main = \"Density of samples\") seq \u003c- seq(-10,10,length.out=1e5) lines(seq, dnorm(seq), col=\"red\") Below we look at the density of samples produced for a varying number of iterations.\npar(mfrow = c(1,3)) for(iter in c(50, 500, 5000)) { # generate markov chain using MH algorithm and N(0,0.5^2) proposal # target: N(0,1) n \u003c- iter xs \u003c- simulate.chain(make.metropolis.hastings.kernel(dnorm, make.normal.proposal(0.5)), 3, n) # plot the density of the chain plot(density(xs), main = \"Density of samples\") seq \u003c- seq(-10,10,length.out=1e5) lines(seq, dnorm(seq), col=\"red\") } We can also use a proposal distribution which is different from the distribution we want to sample from. Below we use a uniform proposal distribution centered at the current state of the chain with range \\(5\\).\n# generate markov chain using MH algorithm and U[-5,5] proposal # target: N(0,1) n \u003c- 5e4 xs \u003c- simulate.chain(make.metropolis.hastings.kernel(dnorm, make.uniform.proposal(5)), 3, n) # plot the density of the chain plot(density(xs), main = \"Density of samples\") seq \u003c- seq(-10,10,length.out=1e5) lines(seq, dnorm(seq), col=\"red\") Our target distribution need not be one which is given by R. In practice the distributions we need to sample from are much more complex than the toy problems presented here. Below we sample from a mixture distribution: The weighted mixture of two normal distributions \\(Z \\sim 0.6X + 0.4Y\\), where \\(X \\sim N(-2,1)\\), \\(Y \\sim N(5, 2^2)\\). We use a \\(N(0,2^2)\\) proposal distribution.\n# generate markov chain using MH algorithm # target: mixture of two normals n \u003c- 1e4 target \u003c- function(x) {0.6*dnorm(x, -2, 1) + 0.4*dnorm(x, 5, 2)} xs \u003c- simulate.chain(make.metropolis.hastings.kernel(target, make.normal.proposal(2)), 10, n) # setup plot space par(mfrow=c(1,2)) # plot the chain plot(1:n, xs, type = 'l', xlab=\"iteration\", main=\"Trace of Markov chain\") # plot the density of the chain plot(density(xs), main = \"Density of samples\") seq \u003c- seq(-20,20,length.out=1e5) lines(seq, target(seq), col=\"red\") The Metropolis-Hastings algorithm can also be used to draw samples from multi-variate distributions. Below we attempt to draw samples from a mixture of two 2D multivariate normal distributions. The contour plots highlight areas of the state space with high probability, and the path overlaid shows the steps that the MH algorithm has taken.\n# higher-dimensional problem --- target: 2D multivariate normal distributions # construct target dmvnorm \u003c- function(x, mu, sigma) return(as.vector( exp(-t(x - mu) %*% solve(sigma) %*% (x - mu)/2)/sqrt((2*pi)^length(x)*det(sigma)) )) mu1 \u003c- c(2,2); mu2 \u003c- c(-1,-1) sigma1 \u003c- matrix(c(1,0.3,0.3,1),2,2); sigma2 \u003c- matrix(c(1,-0.1,-0.1,1),2,2) target \u003c- function(x) return(dmvnorm(x, mu1, sigma1) + dmvnorm(x, mu2, sigma2)) # evaluate density of target over grid seq \u003c- seq(-5,6,length.out=5e2) mat \u003c- outer(seq, seq, Vectorize(function(x, y) target(c(x,y)))) rownames(mat) \u003c- colnames(mat) \u003c- seq # make contour plot longmat \u003c- melt(mat) #reshape data ## Warning in type.convert.default(X[[i]], ...): 'as.is' should be specified by the ## caller; using TRUE ## Warning in type.convert.default(X[[i]], ...): 'as.is' should be specified by the ## caller; using TRUE p1 \u003c- longmat %\u003e% ggplot() + geom_raster(aes(x = X1, y = X2, fill = value)) + geom_contour(aes(x = X1, y = X2, z = value), col = \"white\") + labs(title=\"Contour plot of the target\") # simulate chain n \u003c- 300 xs \u003c- simulate.chain(make.metropolis.hastings.kernel(target, make.normal.proposal(3)), c(-5,0), n) # make plot of chain over contour p2 \u003c- longmat %\u003e% ggplot() + geom_raster(aes(x = X1, y = X2, fill = value)) + geom_contour(aes(x = X1, y = X2, z = value), col = \"white\") + geom_path(data = as.data.frame(xs), aes(x=V1, y = V2), col=\"red\") + geom_point(data = as.data.frame(xs), aes(x=V1, y = V2), pch=21, fill=\"red\", col=\"white\") + labs(title=\"Contour plot of the target\") # plot p1, p2 side-by-side grid.arrange(p1,p2,ncol=2) MCMC in practice Implementing MCMC methods in practice can be quite simple, but there are a few things that we need to be aware of to ensure that our algorithm is working well.\nFirst we introduce the concept of pseudo-convergence. This occurs when there are regions of the state space with high probability that are poorly connected by the Markov chain. The chain may spend a long time exploring one high probability region, which falsely leads us to believe that the chain has converged to the stationary distribution. This phenomenon occurs frequently with multimodal distributions as we demonstrate below. If the modes are so far from one another such that the proposal distribution cannot propose values in one high-probability region when in another, the chain will fail to converge.\nIn the example below we attempt to sample from a univariate bimodal distribution. In the first scenario, we use a uniform proposal distribution with an interval length such that the chain cannot travel between modes. We repeat the simulation but with a larger interval for the proposal distribution.\n# target: sum of two uniform densities # proposal with interval length 2 n \u003c- 1e3 target \u003c- function(x) {0.5*dunif(x, -2,-1) + 0.5*dunif(x, 1, 2)} xs \u003c- simulate.chain(make.metropolis.hastings.kernel(target, make.uniform.proposal(2)), 2, n) # setup plot space par(mfrow=c(1,2)) # plot the chain plot(xs, type = 'l', ylim=c(-2,2), xlab=\"iteration\", main=\"Trace of Markov chain\") # plot the density of the chain plot(density(xs), main = \"Density of samples\", xlim=c(-3,3)) seq \u003c- seq(-20,20,length.out=1e5) lines(seq, target(seq), col=\"red\") # target: sum of two uniform densities # proposal with interval length 5 n \u003c- 1e4 target \u003c- function(x) {0.5*dunif(x, -2,-1) + 0.5*dunif(x, 1, 2)} xs \u003c- simulate.chain(make.metropolis.hastings.kernel(target, make.uniform.proposal(5)), 2, n) # setup plot space par(mfrow=c(1,2)) # plot the chain plot(xs, type = 'l', ylim=c(-2,2), xlab=\"iteration\", main=\"Trace of Markov chain\") # plot the density of the chain plot(density(xs), main = \"Density of samples\", xlim=c(-3,3)) seq \u003c- seq(-20,20,length.out=1e5) lines(seq, target(seq), col=\"red\") One way that we may attempt to overcome the pseudo-convergence phenomenon is to run multiple chains. This simply means that we run the MCMC algorithm multiple times with different initializations. If the chain seems to have converged to different distributions for different initializations, then the algorithm has failed to fully explore the state space. We may need to run the chain for a longer period of time until it converges to the target distribution, or we may need to adjust the proposal distribution to improve the chain’s mixing. This introduces a new problem: given a fixed amount of computation, should we produce one long chain or multiple chains? This `multistart heuristic’ should be used with caution — it only guarantees against pseudo-convergence if we can ensure that the starting points cover every part of the state space which may pseudo-converge.\nAnother practical aspect of MCMC methods that we should be aware of is burn-in. When initializing our algorithm it may take some time before the Markov chain converges to a stationary distribution, and we call this the burn-in period. The samples produced before the chain has converged are not likely to have been produced by the target distribution, and so we discard these samples. The best way to see which samples to discard is to manually inspect the chain in order to see at which iteration it began to converge.\nBelow we parallelize our computations and run one chain per one physical CPU core. The details of the target distribution in this example do not matter — often we do not know them in practice — the point is that we can observe pseudo-convergence in the trace plots of the chains. This occurs because our proposal distribution has a small variance and thus the chains cannot fully explore the state space.\nsimulate.chains \u003c- function(P, x0, n, n.chains, n.cores = 4) { require(parallel) require(doParallel) # parallelize chains registerDoParallel(n.cores) simulations \u003c- foreach(i=1:n.chains, .combine=\"rbind\") %dopar% { x \u003c- x0[i] foreach(i=1:n, .combine=\"c\") %do% {P(x)} } stopImplicitCluster() return(simulations) } # generate markov chain using MH algorithm # target: mixture of two normals n \u003c- 5e3; set.seed(123) target \u003c- function(x) {0.6*dnorm(x, -2, 1) + 0.4*dnorm(x, 5, 2)} xs \u003c- simulate.chains(make.metropolis.hastings.kernel(target, make.normal.proposal(1)), runif(4,-10,10), n, 4) ## Loading required package: parallel ## Loading required package: doParallel ## Loading required package: foreach ## ## Attaching package: 'foreach' ## The following objects are masked from 'package:purrr': ## ## accumulate, when ## Loading required package: iterators # setup plot space par(mfrow=c(2,2)) # plot the chains ylimits \u003c- c(min(xs), max(xs)) for(i in 1:4){ plot(1:n, xs[i,], type = 'l', xlab=\"iteration\", ylim=ylimits, ylab=\"x\", main=paste0(\"Trace of Markov chain \", i)) } ","wordCount":"4244","inLanguage":"en","datePublished":"0001-01-01T00:00:00Z","dateModified":"0001-01-01T00:00:00Z","author":{"@type":"Person","name":"Jake Spiteri"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://jakespiteri.co.uk/portfolio/computing-1/report-10/"},"publisher":{"@type":"Organization","name":"Jake Spiteri","logo":{"@type":"ImageObject","url":"https://jakespiteri.co.uk/favi.png"}}}</script></head><body id=top><header class=header><nav class=nav><div class=logo><a href=https://jakespiteri.co.uk/ accesskey=h title="Home (Alt + H)"><img src=https://jakespiteri.co.uk/apple-touch-icon.png alt aria-label=logo height=35>Home</a><div class=logo-switches></div></div><ul id=menu><li><a href=https://jakespiteri.co.uk/about/ title=/about><span>/about</span></a></li><li><a href=https://jakespiteri.co.uk/portfolio/ title=/portfolio><span>/portfolio</span></a></li><li><a href=https://jakespiteri.co.uk/blog/ title=/blog><span>/blog</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://jakespiteri.co.uk/>Home</a>&nbsp;»&nbsp;<a href=https://jakespiteri.co.uk/portfolio/>Portfolio</a>&nbsp;»&nbsp;<a href=https://jakespiteri.co.uk/portfolio/computing-1/>Statistical Computing 1</a></div><h1 class=post-title>Portfolio Report 10: Integration and Markov Chain Monte Carlo</h1><div class=post-meta>Jake Spiteri&nbsp;|&nbsp;<a href=https://github.com/jakespiteri/jakespiteri.github.io/tree/master/portfolio/computing-1/Report-10/index.html rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><div class=post-content><div id=numerical-integration class="section level1"><h1>Numerical Integration</h1><div id=quadrature class="section level2"><h2>Quadrature</h2><p>“Quadrature rules” are integral approximations which use a finite number of evaluations of the function. All of the quadrature rules below approximate a function using interpolating polynomials.</p><p>We will first look at some key ideas in numerical integration by approximating definite integrals over a finite interval. We will then extend these ideas to semi-infinite and infinite intervals. We will also consider multiple integrals.</p><p>In practice we can use <code>R</code>s <code>integrate</code> function for one-dimensional integrals, and the <code>cubature</code> package for multiple integrals.</p><div id=polynomial-interpolation class="section level3"><h3>Polynomial Interpolation</h3><p>We consider approximating a continuous function <span class="math inline">\(f\)</span> on <span class="math inline">\([a,b]\)</span> (this can be written <span class="math inline">\(f \in C^0([a,b])\)</span>), using a polynomial function <span class="math inline">\(p\)</span>. We know how to compute integrals of polynomials exactly, and so if <span class="math inline">\(p\)</span> is a good approximation of our integrand then we can compute an accurate approximation to the integral. The Weierstrass Approximation Theorem provides a high-level motivation.</p><p><strong>Weierstrass Approximation Theorem.</strong> Let <span class="math inline">\(f \in C^0([a,b])\)</span>. There exists a sequence of polynomials <span class="math inline">\((p_n)\)</span> that converges uniformly to <span class="math inline">\(f\)</span> on <span class="math inline">\([a,b]\)</span>. That is,
<span class="math display">\[ \|f - p_n\|_\infty = \max_{x \in [a,b]} |f(x) - p_n(x)| \rightarrow 0.\]</span></p><p>The above theorem suggests that for any given tolerance, we can find a polynomial which approximates a function <span class="math inline">\(f \in C^0([a,b])\)</span> up to said tolerance. The Weierstrass theorem tells us that such polynomials exist, but it does not tell us how to find them. The real difficulty lies in constructing such polynomials in a computationally-efficient manner without a strong knowledge of the function <span class="math inline">\(f\)</span>.</p><div id=lagrange-polynomials class="section level4"><h4>Lagrange polynomials</h4><p>We can approximate a function <span class="math inline">\(f\)</span> by using an interpolating polynomial with <span class="math inline">\(k\)</span> points <span class="math inline">\(\{x_i, f(x_i)\}_{i=1}^n\)</span>. The interpolating polynomial produced is unique, has at most <span class="math inline">\(k-1\)</span> degree, and can be written as a Lagrange polynomial:
<span class="math display">\[p_{k-1}(x) := \sum_{i=1}^k \ell_i(x) f(x_i),\]</span>
where the Lagrange basis polynomials are
<span class="math display">\[\ell_i(x) = \prod_{j=1, j\neq i}^p \frac{x - x_j}{x_i - x_j}, \quad i \in \{1, \dots, k\}.\]</span></p><p>Below is code implementing interpolating polynomials. For a polynomial with degree <span class="math inline">\(3\)</span> we can produce the polynomial approximations for <span class="math inline">\(k \in \{2,3,4\}\)</span> and for specific choices of <span class="math inline">\(x_1, \dots, x_4\)</span>.</p><pre class=r><code>construct.interpolating.polynomial &lt;- function(f, xs) {
  k &lt;- length(xs)
  fxs &lt;- f(xs)
  p &lt;- function(x) {
    value &lt;- 0
    for (i in 1:k) {
      fi &lt;- fxs[i]
      zs &lt;- xs[setdiff(1:k,i)]
      li &lt;- prod((x-zs)/(xs[i]-zs))
      value &lt;- value + fi*li
    }
    return(value)
  }
  return(p)
}

plot.polynomial.approximation &lt;- function(f, xs, a, b) {
  p &lt;- construct.interpolating.polynomial(f, xs)
  vs &lt;- seq(a, b, length.out=500)
  plot(vs, f(vs), type=&#39;l&#39;, xlab=&quot;x&quot;, ylab=&quot;black: f(x), red: p(x)&quot;)
  points(xs, f(xs), pch=20)
  lines(vs, vapply(vs, p, 0), col=&quot;red&quot;)
}

a &lt;- -4
b &lt;- 4

f &lt;- function(x) {
  return(-x^3 + 3*x^2 - 4*x + 1)
}

par(mfrow=c(1,3))
plot.polynomial.approximation(f, c(-2, 2), a, b)
plot.polynomial.approximation(f, c(-2,0,2), a, b)
plot.polynomial.approximation(f, c(-2, 0, 2, 4), a, b)</code></pre><p><img src=https://jakespiteri.co.uk/portfolio/computing-1/report-10/index_files/figure-html/unnamed-chunk-1-1.png width=1344 style=display:block;margin:auto></p></div><div id=interpolation-error-and-convergence class="section level4"><h4>Interpolation Error and Convergence</h4><p>Clearly we can approximate a polynomial using interpolating polynomials for a <span class="math inline">\(k\)</span> which is large enough. However, we are often interested in approximating functions which are not polynomials.</p><p><strong>Interpolation Error Theorem.</strong> Let <span class="math inline">\(f \in C^k[a,b]\)</span>, and <span class="math inline">\(p_{k-1}\)</span> be the polynomial interpolating <span class="math inline">\(f\)</span> at the <span class="math inline">\(k\)</span> points <span class="math inline">\(x_1, \dots, x_k\)</span>. Then for any <span class="math inline">\(x \in [a,b]\)</span> there exists <span class="math inline">\(\xi \in (a,b)\)</span> such that
<span class="math display">\[f(x) - p_{k-1}(x) = \frac{1}{k!}f^{(k)}(\xi) \prod_{i=1}^k (x - x_i).\]</span>
The above theorem looks promising, but it does not tell us how to choose our interpolating polynomials such that they converge uniformly (or even pointwise) to <span class="math inline">\(f\)</span>. The theorem tells us that one way of minimizing the error is to choose the interpolation points such that the product <span class="math inline">\(|\prod_{i=1}^k (x - x_k)|\)</span> is as small as possible. The Chebyshev interpolation points detailed below do this.</p><p>We know that there exists a sequence of interpolation points that provide us with uniform convergence.</p><p><strong>Theorem.</strong> Let <span class="math inline">\(f \in C^0[a,b]\)</span>. There exists a sequence of sets of interpolation points <span class="math inline">\(X_1, X_2, \dots\)</span> such that the corresponding sequence of interpolating polynomials converges uniformly to <span class="math inline">\(f\)</span> on <span class="math inline">\([a,b]\)</span>.</p><p>The above tells us that given a function <span class="math inline">\(f\)</span>, it is indeed possible to find a sequence of sets of interpolating points which produce a sequence of interpolating polynomials that converge uniformly to <span class="math inline">\(f\)</span>. We may then wonder if there is a universal sequence of sets of interpolating points that provide us with uniform convergence to <em>any</em> function <span class="math inline">\(f\)</span>. This is not possible, as for any fixed sequence of sets we can find a function <span class="math inline">\(f\)</span> for which our sequence of interpolating polynomials diverges.</p><p><strong>Theorem.</strong> For any fixed sequence of sets of interpolation points there exists a continuous function <span class="math inline">\(f\in C^0[a,b]\)</span> for which the sequence of interpolating polynomials diverges on <span class="math inline">\([a,b]\)</span>.</p><p><strong>Example:</strong> Consider the sequence of interpolating points <span class="math inline">\(X_k\)</span> to be the set of <span class="math inline">\(k\)</span> uniformly spaced points including <em>a</em>, and <em>b</em> if <span class="math inline">\(k>1\)</span>.</p><p>This is a particularly bad way to produce sets of interpolation points as uniform convergence is not guaranteed — even for infinitely differentiable functions. There are of course functions which these interpolation points work well for, as seen below.</p><pre class=r><code>construct.uniform.point.set &lt;- function(a, b, k) {
  if (k==1) return(a)
  return(seq(a, b, length.out=k))
}

a &lt;- 1
b &lt;- 2
plot.polynomial.approximation(log, construct.uniform.point.set(a, b, 10), a, b)</code></pre><p><img src=https://jakespiteri.co.uk/portfolio/computing-1/report-10/index_files/figure-html/unnamed-chunk-2-1.png width=384 style=display:block;margin:auto></p><p>Below is a plot of the Runge function defined by <span class="math inline">\(f(x) = \frac{1}{1+x^2}\)</span> over the interval <span class="math inline">\([-5,5]\)</span>. We see that the interpolation error <span class="math inline">\(\|f-p_n\|_\infty\)</span> grows without bound as <span class="math inline">\(n \rightarrow \infty\)</span>.</p><p>Below we look at the polynomial interpolation of the Runge function <span class="math inline">\(f(x) = \frac{1}{1+x^2}\)</span>, over the interval <span class="math inline">\([-5,5]\)</span>.</p><pre class=r><code>a &lt;- -5
b &lt;- 5
f &lt;- function(x) return(1/(1+x^2))
plot.polynomial.approximation(f, construct.uniform.point.set(a,b,50), a, b)</code></pre><p><img src=https://jakespiteri.co.uk/portfolio/computing-1/report-10/index_files/figure-html/unnamed-chunk-3-1.png width=384 style=display:block;margin:auto></p><p>Note that this does not contradict the Interpolation Error Theorem as the maximum of the <span class="math inline">\(k\)</span>’th derivative of the Runge function grows quickly with <span class="math inline">\(k\)</span>. As this outweighs the decreasing product term, the interpolation error grows without bound as <span class="math inline">\(k \rightarrow \infty\)</span>.</p><p>We can also look at the function <span class="math inline">\(f(x) = |x|\)</span> over <span class="math inline">\([-1,1]\)</span>. We see that the interpolating polynomial oscillates wildly for large <span class="math inline">\(k\)</span>.</p><pre class=r><code>a &lt;- -1
b &lt;- 1
par(mfrow=c(1,3))
plot.polynomial.approximation(abs, construct.uniform.point.set(a,b,50), a, b)
plot.polynomial.approximation(abs, construct.uniform.point.set(a,b,100), a, b)
plot.polynomial.approximation(abs, construct.uniform.point.set(a,b,200), a, b)</code></pre><p><img src=https://jakespiteri.co.uk/portfolio/computing-1/report-10/index_files/figure-html/unnamed-chunk-4-1.png width=1344 style=display:block;margin:auto>
Note once again that this does not contradict the Interpolation Error Theorem as the function <span class="math inline">\(f(x)\)</span> is not differentiable at <span class="math inline">\(0\)</span>.</p><p>As mentioned above, we can avoid this problem by minimizing the interpolation error by minimizing the product term. If we minimize the maximum absolute value of the product term, we can derive the Chebyshev interpolation points. For any given <span class="math inline">\(k\)</span>, we have the points
<span class="math display">\[\cos\left(\frac{2i-1}{2k}\pi\right), \quad i \in \{1, \dots, k\},\]</span>
and the absolute value of the product term is then bounded above by <span class="math inline">\(2^{1-k}\)</span></p><p>These points clearly do not minimize the overall error.</p><pre class=r><code>construct.chebyshev.point.set &lt;- function(k) {
  return(cos((2*(1:k)-1)/2/k*pi))
}

# visualize the Chebyshev points
chebyshev.visalization &lt;- function(k){
  df &lt;- data.frame(k = numeric(), points = list())
  for(i in 1:k){
    df &lt;- rbind(df, data.frame(k=i, points = construct.chebyshev.point.set(i)))
  }
  ggplot(df, aes(color=k, x = points, y = k)) +
    geom_point(aes(color=k)) +
    labs(y=&quot;number of points k&quot;, xlab=&quot;&quot;)
}

# produce visualization
chebyshev.visalization(50)</code></pre><p><img src=https://jakespiteri.co.uk/portfolio/computing-1/report-10/index_files/figure-html/unnamed-chunk-5-1.png width=384 style=display:block;margin:auto></p><p>Above we clearly see that as our number of points <span class="math inline">\(k\)</span> increases, the points tend to cluster around <span class="math inline">\(-1\)</span>, and <span class="math inline">\(1\)</span>. We can use the Chebyshev interpolation points as seen below.</p><pre class=r><code>plot.polynomial.approximation(f, construct.chebyshev.point.set(50), a, b)</code></pre><p><img src=https://jakespiteri.co.uk/portfolio/computing-1/report-10/index_files/figure-html/unnamed-chunk-6-1.png width=384 style=display:block;margin:auto></p><pre class=r><code>plot.polynomial.approximation(abs, construct.chebyshev.point.set(50), a, b)</code></pre><p><img src=https://jakespiteri.co.uk/portfolio/computing-1/report-10/index_files/figure-html/unnamed-chunk-6-2.png width=384 style=display:block;margin:auto></p><p>There is a large improvement in the error as the approximation no longer oscillates. As mentioned in the theorem above, there exist functions <span class="math inline">\(f\)</span> for which the interpolating polynomials with the Chebyshev interpolating points diverge.</p></div></div><div id=composite-polynomial-interpolation class="section level3"><h3>Composite Polynomial Interpolation</h3><p>Another way to approximate a function is to use different polynomials over subintervals of the domain. This results in a piecewise polynomial approximation which is not necessarily continuous, but can be made to be.</p></div><div id=other-polynomial-interpolation-schemes class="section level3"><h3>Other Polynomial Interpolation Schemes</h3><p>There are many other schemes that we may use to approximate a function. We may use Hermite interpolation which fits a polynomial using evaluations of <span class="math inline">\(f\)</span> and evaluations of the derivatives of <span class="math inline">\(f\)</span>. If we ensure that the derivatives at the boundaries of the subintervals are equal then the resulting approximation will have a certain number of continuous derivatives. This idea is known as spline interpolation, and is often preferred to polynomial interpolation as it avoids the problem of Runge’s phenomenon demonstrated above. Spline interpolation also produces a small interpolation error even when using low degree polynomials.</p><p>Below is an implementation of natural cubic spline interpolation. For a set of interpolation points <span class="math inline">\(\{x_i\}_{i=1}^n\)</span>, a cubic spline fits a polynomial of at most degree <span class="math inline">\(3\)</span> between each pair of interpolating points. Hence for a set of <span class="math inline">\(n\)</span> interpolation points, <span class="math inline">\(n-1\)</span> splines are produced to approximate the function. A set of constraints are placed upon the values these splines can take, which ensure that the spline is continuous over the interval. A natural cubic spline has degree <span class="math inline">\(3\)</span> with continuity <span class="math inline">\(C^2\)</span>, and has the general form
<span class="math display">\[S_j(x) = a_j + b_j(x - x_j) + c_j(x - x_j)^2 + d_j(x - x_j)^3, \quad j = 0, \dots, n-1\]</span>
for which the individual splines must satisfy
<span class="math display">\[\begin{align*}
&S_i(x_i) &&= y_i \qquad &&i = 0, \dots, n-1\\
&S_{i-1}(x_i) &&= y_i &&i = 1, \dots, n\\
&S'_{i}(x_i) &&= S'_{i-1}(x_i) &&i = 1, \dots, n-1 \\
&S''_{i}(x_i) &&= S''_{i-1}(x_i) &&i = 1, \dots, n-1 \\
&S''_{0}(x_0) &&= S''_{n-1}(x_{n-1}) = 0 &&i = 1, \dots, n-1 \\
\end{align*}\]</span></p><p>An implementation can be seen below.</p><pre class=r><code># compute spline parameters
create.spline.params &lt;- function(f, xs) {
  n &lt;- length(xs) - 1
  fxs &lt;- f(xs)
  b &lt;- d &lt;- rep(0,n)
  h &lt;- diff(xs)
  hlag2 &lt;- diff(xs, lag=2)
  alpha &lt;- 3/h[-1]*diff(fxs)[-1] - 3/h[-n]*diff(fxs)[-n]
  c &lt;- l &lt;- mu &lt;- z &lt;- rep(0,n+1)
  l[1] &lt;- 1; mu[1] &lt;- z[1] &lt;- 0
  for (i in 2:(n)) {
    l[i] &lt;- 2*hlag2[i-1] - h[i-1]*mu[i-1]
    mu[i] &lt;- h[i]/l[i]
    z[i] &lt;- (alpha[i-1]-h[i-1]*z[i-1])/l[i]
  }
  l[n+1] &lt;- 1; z[n+1] &lt;- c[n+1] &lt;- 0
  for (j in n:1) {
    c[j] &lt;- z[j] - mu[j]*c[j+1]
    b[j] &lt;- (fxs[j+1] - fxs[j])/h[j] - h[j]*(c[j+1] + 2*c[j])/3
    d[j] &lt;- (c[j+1]-c[j])/(3*h[j])
  }
  return(list(a=fxs, b = b, c = c, d = d))
}

# given parameters and the interval, return appropriate spline function
# this is needed as for n interpolation points, n-1 splines are produced
construct.spline &lt;- function(a, b, c, d, i, xs) {
  return(function(x) a[i] + b[i]*(x - xs[i]) + c[i]*(x - xs[i])^2 + d[i]*(x - xs[i])^3)
}

# given x and interpolation points, return spline index
get.spline.subinterval &lt;- function(x, xs){
  n &lt;- length(xs-1)
  for(i in 1:n) {
    if (between(x, xs[i], xs[i+1])) return(i)
  }
}

# produce and plot the spline approximation for a function f and interpolation points xs
plot.cubic.splines.approximation &lt;- function(f, xs){
  par &lt;- create.spline.params(f, xs)
  spline &lt;- function(x) construct.spline(par$a, par$b, par$c, par$d, 
                                         get.spline.subinterval(x, xs), xs)(x)
  vs &lt;- seq(min(xs), max(xs), length.out=500)
  plot(vs, f(vs), type=&#39;l&#39;, xlab=&quot;x&quot;, ylab=&quot;black: f(x), red: p(x)&quot;)
  abline(v=xs, col = &quot;grey&quot;)
  lines(vs, f(vs))
  lines(vs, vapply(vs, spline, 0), col=&quot;red&quot;)
  points(xs, f(xs), pch=20, col=&quot;blue&quot;)
}</code></pre><pre class=r><code>par(mfrow=c(1,2))
plot.cubic.splines.approximation(sin, seq(0,4*pi, length.out=4))
plot.cubic.splines.approximation(sin, seq(0,4*pi, length.out=12))</code></pre><p><img src=https://jakespiteri.co.uk/portfolio/computing-1/report-10/index_files/figure-html/unnamed-chunk-8-1.png width=1344 style=display:block;margin:auto></p><p>We see that our approximation is not very accurate when we have a small set of interpolation points, but we can approximate the <span class="math inline">\(\sin\)</span> function impressively well with only <span class="math inline">\(12\)</span> interpolation points and with polynomials of at most degree <span class="math inline">\(3\)</span>!</p><pre class=r><code>plot.cubic.splines.approximation(function(x) abs(x), seq(-1,1, length.out=16))</code></pre><p><img src=https://jakespiteri.co.uk/portfolio/computing-1/report-10/index_files/figure-html/unnamed-chunk-9-1.png width=384 style=display:block;margin:auto>
Splines avoid the oscillating approximation phenomenon which occurs with polynomial interpolation. For the function <span class="math inline">\(f(x)=|x|\)</span>, the spline method seems to work better than the interpolating polynomial approach but it is not possible for the approximation to capture the true behavior of <span class="math inline">\(f(x)\)</span> around <span class="math inline">\(x=0\)</span>. This is because <span class="math inline">\(f\)</span> is not continuous at this point and an explicit requirement on the natural cubic spline is that it <em>is</em> continuous.</p><pre class=r><code>par(mfrow=c(1,2))
plot.cubic.splines.approximation(function(x) 1/(1+25*x^2), seq(-5,5, length.out=6))
plot.cubic.splines.approximation(function(x) 1/(1+25*x^2), seq(-5,5, length.out=16))</code></pre><p><img src=https://jakespiteri.co.uk/portfolio/computing-1/report-10/index_files/figure-html/unnamed-chunk-10-1.png width=1344 style=display:block;margin:auto></p><pre class=r><code>plot.cubic.splines.approximation(function(x) 1/(1+25*x^2), seq(-5,5, length.out=45))</code></pre><p><img src=https://jakespiteri.co.uk/portfolio/computing-1/report-10/index_files/figure-html/unnamed-chunk-11-1.png width=384 style=display:block;margin:auto>
Above we see that Runge’s phenomenon does not occur for splines, but it is hard to produce a good approximation to Runge’s function for a small number of interpolation points. Of course <em>small</em> has little meaning — in this case it is very easy to evaluate Runge’s function for a large number of points, for other functions this may not be true.</p></div></div><div id=monte-carlo-integration class="section level2"><h2>Monte Carlo Integration</h2><p>The quadrature rules introduced above give excellent rates of convergence (in terms of computational cost) for low-dimensional problems. Their costs quickly become too expensive for high-dimensional problems. For such problems, a better approach is to use Monte Carlo algorithms.</p><p>Let <span class="math inline">\((X, \mathcal{X})\)</span> be a measurable space. We have a target probability measure <span class="math inline">\(\pi:\mathcal{X} \rightarrow [0,1]\)</span> and we would like to approximate the quantity
<span class="math display">\[\pi(f) := \int_X f(x) \pi(dx),\]</span>
where <span class="math inline">\(f \in L_1(X, \pi) = \{f:\pi(|x|)&lt;\infty\}\)</span>. I.e. <span class="math inline">\(\pi(f)\)</span> is the expectation of <span class="math inline">\(f(X)\)</span> when <span class="math inline">\(X \sim \pi\)</span>.</p><div id=monte-carlo-with-iid-random-variables class="section level3"><h3>Monte Carlo with IID Random Variables</h3><p>Classical Monte Carlo is a natural method for integral approximation. We can motivate its use at a high level using the Law of Large Numbers. Recall that our aim is to approximate the quantity <span class="math inline">\(\pi(f)\)</span>; below we reduce this problem to the problem of simulating random variables with distribution <span class="math inline">\(\pi\)</span>.</p><div id=fundamental-results class="section level4"><h4>Fundamental Results</h4><p><strong>Theorem</strong>(SLLN):
Let <span class="math inline">\((X_n)_{n\geq1}\)</span> be a sequence of iid random variables with distribution <span class="math inline">\(\mu\)</span>. Define the quantity
<span class="math display">\[S_n(f) := \sum_{i=1}^n f(X_i)\]</span>
for <span class="math inline">\(f \in L_1(X, \mu)\)</span>. Then
<span class="math display">\[\lim_{n \rightarrow \infty} \frac{1}{n}S_n(f) = \mu(f),\]</span>
almost surely.</p><p>The random variable <span class="math inline">\(n^{-1}S_n(f)\)</span> is a Monte Carlo approximation of <span class="math inline">\(\mu(f)\)</span>. The probabilistic convergence result does not tell us about the variance of the approximation for finite <span class="math inline">\(n\)</span>, so we compute the variance of the approximation and appeal to the Central Limit Theorem.</p><p><strong>Proposition</strong>(Variance):
Let <span class="math inline">\((X_n)_n\geq1\)</span> and <span class="math inline">\(S_n(f)\)</span> be as defined in the SLLN, where <span class="math inline">\(f\in L_2(X, \mu)\)</span>. Then
<span class="math display">\[Var[n^{-1}S_n(f)] = \frac{\mu(f^2) - \mu(f)^2}{n}.\]</span></p><p><strong>Theorem</strong>(CLT):
Let <span class="math inline">\((X_n)_n\geq1\)</span> and <span class="math inline">\(S_n(f)\)</span> be as defined in the SLLN, where <span class="math inline">\(f\in L_2(X, \mu)\)</span>. Then
<span class="math display">\[n^{1/2}\{n^{-1}S_n(f) - \mu(f)\} \xrightarrow[]{L} X \sim N(0, \mu(\bar{f}^2)),\]</span>
where <span class="math inline">\(\bar{f} = f - \mu(f)\)</span>.</p></div><div id=sampling class="section level4"><h4>Sampling</h4><p>The SLLN and CLT which justify our approximations above rely on us being able to simulate random variables from our <em>target</em> distribution <span class="math inline">\(\mu=\pi\)</span>. This is the main difficulty in Monte Carlo methods — we often have limited knowledge of <span class="math inline">\(\pi\)</span>.</p><p>Classical Monte Carlo methods differ in the way that they generate samples from <span class="math inline">\(\pi\)</span>. Some very well-known Monte Carlo algorithms are:</p><ol style=list-style-type:decimal><li>Rejection sampling</li><li>Importance sampling</li><li>Self-normalized importance sampling</li></ol></div></div><div id=markov-chain-monte-carlo-mcmc class="section level3"><h3>Markov Chain Monte Carlo (MCMC)</h3><p>Markov chain Monte Carlo (MCMC) methods are a development of the standard Monte Carlo techniques. These methods allow us to sample from a probability distribution by constructing a Markov chain as opposed to sampling independent random variables.</p><div id=fundamental-results-1 class="section level4"><h4>Fundamental Results</h4><p><strong>Theorem</strong>(LLN for Markov chains):
Suppose that <span class="math inline">\(\mathbf{X}=(X_n)_{n\geq0}\)</span> is a time-homogeneous, positive Harris Markov chain with invariant probability measure <span class="math inline">\(\pi\)</span>. Then for any <span class="math inline">\(f \in L_1(X, \pi)\)</span>,
<span class="math display">\[\lim_{n \rightarrow \infty} \frac{1}{n} S_n(f) = \pi(f),\]</span>
almost surely for any initial distribution for <span class="math inline">\(X_0\)</span>.</p><p>Similar to classical Monte Carlo, there also exist Central Limit Theorems for Markov chains such as the following</p><p><strong>Theorem</strong>(A CLT for geometrically ergodic Markov chains): Assume that <span class="math inline">\(\mathbf{X}\)</span> is time-homogeneous, positive Harris and geometrically ergodic with invariant probability measure <span class="math inline">\(\pi\)</span>, and that <span class="math inline">\(\pi(|f|^{2+\delta})&lt;\infty\)</span> for some <span class="math inline">\(\delta>0\)</span>. Then
<span class="math display">\[n^{1/2} \{ n^{-1} S_{n}(f) - \pi(f) \} \overset{L}{\to} N(0,\sigma^{2}(f))\]</span>
as <span class="math inline">\(n\rightarrow\infty\)</span>, where <span class="math inline">\(\bar{f}=f-\pi(f)\)</span> and
<span class="math display">\[\sigma^{2}(f)=\mathbb{E}_{\pi}\left[\bar{f}(X_{0})^{2}\right]+2\sum_{k=1}^{\infty}\mathbb{E}_{\pi}\left[\bar{f}(X_{0})\bar{f}(X_{k})\right]&lt;\infty.\]</span></p><p>The Law of Large Numbers and Central Limit Theorem results for Markov chains tell us that we are justified in using Markov chains to produce samples from a distribution, as long as certain conditions are met (e.g. time-homoegeneity, chain is positive Harris recurrent, etc.). Our aim is therefore to produce transition kernels and thus Markov chains which satisfy these conditions.</p></div><div id=metropolis-hastings class="section level4"><h4>Metropolis-Hastings</h4><p>Metropolis-Hastings (MH) transition kernels are perhaps the most used method of constructing Markov chains. The MH algorithm is as follows:</p><p>Suppose we want to sample from a target distribution <span class="math inline">\(\pi\)</span>, which has a density with respect to (w.r.t) some measure <span class="math inline">\(\lambda\)</span>. We specify a proposal Markov kernel <span class="math inline">\(Q\)</span> admitting density <span class="math inline">\(q\)</span> w.r.t <span class="math inline">\(\lambda\)</span>, i.e. <span class="math inline">\(Q(x, dz) = q(x, dz) \lambda (dz)\)</span>.</p><p>In order to sample from <span class="math inline">\(\pi\)</span>, we simulate according to the transition kernel <span class="math inline">\(P_{MH}(x, \cdot)\)</span>.</p><ol style=list-style-type:decimal><li><p>Initialize: Set <span class="math inline">\(X_1 = x_1\)</span>.</p></li><li><p>For <span class="math inline">\(t = 2, \dots\)</span></p><p>2a. Sample a proposal <span class="math inline">\(Z \sim Q(x_t, \cdot)\)</span>.</p><p>2b. Accept proposal <span class="math inline">\(Z\)</span> with the acceptance probability <span class="math inline">\(\alpha_{MH}(x_t, Z)\)</span>, where
<span class="math display">\[\alpha_{MH}(x, z) := \min\left(1, \frac{\pi(z) q(z, x)}{\pi(x) q(x, z)}\right),\]</span>
otherwise, output <span class="math inline">\(x_t\)</span>.</p></li></ol><p>Note that given the acceptance probability specified by the MH algorithm, we only need to know the density <span class="math inline">\(\pi\)</span> up to a normalizing constant. Also, in the case that a symmetric proposal (i.e. <span class="math inline">\(q(x, y) = q(y, x)\)</span> for all <span class="math inline">\(x, y\)</span>) is specified, the MH update becomes the Metropolis update.</p><p>Below is an implementation of the Metropolis-Hastings algorithm.</p><pre class=r><code># returns the transition kernel P(x)
make.metropolis.hastings.kernel &lt;- function(pi, Q) {
  q &lt;- Q$density
  P &lt;- function(x) {
      z &lt;- Q$sample(x)
      alpha &lt;- min(1, pi(z) * q(z, x) / pi(x) / q(x, z))
      if(runif(1) &lt; alpha) {
        return(z)
      } else {
        return(x)
      }
  }
  return(P)
}

make.normal.proposal &lt;- function(sigma) {
  Q &lt;- list()
  Q$sample &lt;- function(x) {
    return(rnorm(length(x), x, sigma))
  }
  Q$density &lt;- function(x, z) {
    return(dnorm(z, x, sigma))
  }
  return(Q)
}

make.uniform.proposal &lt;- function(range) {
  Q &lt;- list()
  Q$sample &lt;- function(x) {
    return(runif(length(x), x-range/2, x+range/2))
  }
  Q$density &lt;- function(x, z) {
    return(dunif(z, z-range/2, z+range/2))
  }
  return(Q)
}

simulate.chain &lt;- function(P, x0, n) {
  xs &lt;- matrix(NA, n, length(x0))
  x &lt;- x0
  for(i in 1:n) {
    x &lt;- P(x)
    xs[i,] &lt;- x
  }
  return(xs)
}</code></pre><p>Suppose we want to sample from a standard normal distribution. Below we show the density of samples produced by a Metropolis-Hastings algorithm with a <span class="math inline">\(N(0,0.5^2)\)</span> proposal distribution. We plot the true target distribution <span class="math inline">\(N(0,1)\)</span> over the density of samples. We also plot the trace of our Markov chain which shows the evolution of our chain over time — this gives us some insight as to whether the chain fully explores the state space.</p><pre class=r><code># generate markov chain using MH algorithm and N(0,0.5^2) proposal
# target: N(0,1)
n &lt;- 1e4
xs &lt;- simulate.chain(make.metropolis.hastings.kernel(dnorm,
                                                     make.normal.proposal(0.5)), 3, n)

# setup plot space
par(mfrow=c(1,2))

# plot the chain
plot(1:n, xs, type = &#39;l&#39;, xlab=&quot;iteration&quot;, 
     main=&quot;Trace of Markov chain&quot;)

# plot the density of the chain
plot(density(xs), main = &quot;Density of samples&quot;)
seq &lt;- seq(-10,10,length.out=1e5)
lines(seq, dnorm(seq), col=&quot;red&quot;)</code></pre><p><img src=https://jakespiteri.co.uk/portfolio/computing-1/report-10/index_files/figure-html/unnamed-chunk-13-1.png width=1344 style=display:block;margin:auto></p><p>Below we look at the density of samples produced for a varying number of iterations.</p><pre class=r><code>par(mfrow = c(1,3))
for(iter in c(50, 500, 5000)) {
  # generate markov chain using MH algorithm and N(0,0.5^2) proposal
  # target: N(0,1)
  n &lt;- iter
  xs &lt;- simulate.chain(make.metropolis.hastings.kernel(dnorm,
                                                       make.normal.proposal(0.5)), 3, n)
  
  # plot the density of the chain
  plot(density(xs), main = &quot;Density of samples&quot;)
  seq &lt;- seq(-10,10,length.out=1e5)
  lines(seq, dnorm(seq), col=&quot;red&quot;)
}</code></pre><p><img src=https://jakespiteri.co.uk/portfolio/computing-1/report-10/index_files/figure-html/unnamed-chunk-14-1.png width=1344 style=display:block;margin:auto></p><p>We can also use a proposal distribution which is different from the distribution we want to sample from. Below we use a uniform proposal distribution centered at the current state of the chain with range <span class="math inline">\(5\)</span>.</p><pre class=r><code># generate markov chain using MH algorithm and U[-5,5] proposal
# target: N(0,1)
n &lt;- 5e4
xs &lt;- simulate.chain(make.metropolis.hastings.kernel(dnorm,
                                                     make.uniform.proposal(5)), 3, n)

# plot the density of the chain
plot(density(xs), main = &quot;Density of samples&quot;)
seq &lt;- seq(-10,10,length.out=1e5)
lines(seq, dnorm(seq), col=&quot;red&quot;)</code></pre><p><img src=https://jakespiteri.co.uk/portfolio/computing-1/report-10/index_files/figure-html/unnamed-chunk-15-1.png width=384 style=display:block;margin:auto></p><p>Our target distribution need not be one which is given by R. In practice the distributions we need to sample from are <em>much</em> more complex than the toy problems presented here. Below we sample from a mixture distribution: The weighted mixture of two normal distributions <span class="math inline">\(Z \sim 0.6X + 0.4Y\)</span>, where <span class="math inline">\(X \sim N(-2,1)\)</span>, <span class="math inline">\(Y \sim N(5, 2^2)\)</span>. We use a <span class="math inline">\(N(0,2^2)\)</span> proposal distribution.</p><pre class=r><code># generate markov chain using MH algorithm
# target: mixture of two normals
n &lt;- 1e4
target &lt;- function(x) {0.6*dnorm(x, -2, 1) + 0.4*dnorm(x, 5, 2)}
xs &lt;- simulate.chain(make.metropolis.hastings.kernel(target,
                                                     make.normal.proposal(2)), 10, n)

# setup plot space
par(mfrow=c(1,2))

# plot the chain
plot(1:n, xs, type = &#39;l&#39;, xlab=&quot;iteration&quot;, 
     main=&quot;Trace of Markov chain&quot;)

# plot the density of the chain
plot(density(xs), main = &quot;Density of samples&quot;)
seq &lt;- seq(-20,20,length.out=1e5)
lines(seq, target(seq), col=&quot;red&quot;)</code></pre><p><img src=https://jakespiteri.co.uk/portfolio/computing-1/report-10/index_files/figure-html/unnamed-chunk-16-1.png width=1344 style=display:block;margin:auto></p><p>The Metropolis-Hastings algorithm can also be used to draw samples from multi-variate distributions. Below we attempt to draw samples from a mixture of two 2D multivariate normal distributions. The contour plots highlight areas of the state space with high probability, and the path overlaid shows the steps that the MH algorithm has taken.</p><pre class=r><code># higher-dimensional problem --- target: 2D multivariate normal distributions
# construct target
dmvnorm &lt;- function(x, mu, sigma) return(as.vector(
  exp(-t(x - mu) %*% solve(sigma) %*% (x - mu)/2)/sqrt((2*pi)^length(x)*det(sigma))
  ))
mu1 &lt;- c(2,2); mu2 &lt;- c(-1,-1)
sigma1 &lt;- matrix(c(1,0.3,0.3,1),2,2); sigma2 &lt;- matrix(c(1,-0.1,-0.1,1),2,2)
target &lt;- function(x) return(dmvnorm(x, mu1, sigma1) + dmvnorm(x, mu2, sigma2))

# evaluate density of target over grid
seq &lt;- seq(-5,6,length.out=5e2)
mat &lt;- outer(seq, seq, Vectorize(function(x, y) target(c(x,y))))
rownames(mat) &lt;- colnames(mat) &lt;- seq

# make contour plot
longmat &lt;- melt(mat) #reshape data</code></pre><pre><code>## Warning in type.convert.default(X[[i]], ...): &#39;as.is&#39; should be specified by the
## caller; using TRUE

## Warning in type.convert.default(X[[i]], ...): &#39;as.is&#39; should be specified by the
## caller; using TRUE</code></pre><pre class=r><code>p1 &lt;- longmat %&gt;%
  ggplot() +
  geom_raster(aes(x = X1, y = X2, fill = value)) +
  geom_contour(aes(x = X1, y = X2, z = value), col = &quot;white&quot;) +
  labs(title=&quot;Contour plot of the target&quot;)

# simulate chain
n &lt;- 300
xs &lt;- simulate.chain(make.metropolis.hastings.kernel(target,
                                                     make.normal.proposal(3)), c(-5,0), n)

# make plot of chain over contour
p2 &lt;- longmat %&gt;%
  ggplot() +
  geom_raster(aes(x = X1, y = X2, fill = value)) +
  geom_contour(aes(x = X1, y = X2, z = value), col = &quot;white&quot;) +
  geom_path(data = as.data.frame(xs), aes(x=V1, y = V2), col=&quot;red&quot;) +
  geom_point(data = as.data.frame(xs), 
             aes(x=V1, y = V2), pch=21, fill=&quot;red&quot;, col=&quot;white&quot;) +
  labs(title=&quot;Contour plot of the target&quot;)

# plot p1, p2 side-by-side
grid.arrange(p1,p2,ncol=2)</code></pre><p><img src=https://jakespiteri.co.uk/portfolio/computing-1/report-10/index_files/figure-html/unnamed-chunk-17-1.png width=1536 style=display:block;margin:auto></p></div><div id=mcmc-in-practice class="section level4"><h4>MCMC in practice</h4><p>Implementing MCMC methods in practice can be quite simple, but there are a few things that we need to be aware of to ensure that our algorithm is working well.</p><p>First we introduce the concept of <strong>pseudo-convergence</strong>. This occurs when there are regions of the state space with high probability that are poorly connected by the Markov chain. The chain may spend a long time exploring one high probability region, which falsely leads us to believe that the chain has converged to the stationary distribution. This phenomenon occurs frequently with multimodal distributions as we demonstrate below. If the modes are so far from one another such that the proposal distribution cannot propose values in one high-probability region when in another, the chain will fail to converge.</p><p>In the example below we attempt to sample from a univariate bimodal distribution. In the first scenario, we use a uniform proposal distribution with an interval length such that the chain cannot travel between modes. We repeat the simulation but with a larger interval for the proposal distribution.</p><pre class=r><code># target: sum of two uniform densities
# proposal with interval length 2
n &lt;- 1e3
target &lt;- function(x) {0.5*dunif(x, -2,-1) + 0.5*dunif(x, 1, 2)}
xs &lt;- simulate.chain(make.metropolis.hastings.kernel(target,
                                                     make.uniform.proposal(2)), 2, n)

# setup plot space
par(mfrow=c(1,2))

# plot the chain
plot(xs, type = &#39;l&#39;, ylim=c(-2,2), xlab=&quot;iteration&quot;, 
     main=&quot;Trace of Markov chain&quot;)

# plot the density of the chain
plot(density(xs), main = &quot;Density of samples&quot;, xlim=c(-3,3))
seq &lt;- seq(-20,20,length.out=1e5)
lines(seq, target(seq), col=&quot;red&quot;)</code></pre><p><img src=https://jakespiteri.co.uk/portfolio/computing-1/report-10/index_files/figure-html/unnamed-chunk-18-1.png width=1536 style=display:block;margin:auto></p><pre class=r><code># target: sum of two uniform densities
# proposal with interval length 5
n &lt;- 1e4
target &lt;- function(x) {0.5*dunif(x, -2,-1) + 0.5*dunif(x, 1, 2)}
xs &lt;- simulate.chain(make.metropolis.hastings.kernel(target,
                                                     make.uniform.proposal(5)), 2, n)

# setup plot space
par(mfrow=c(1,2))

# plot the chain
plot(xs, type = &#39;l&#39;, ylim=c(-2,2), xlab=&quot;iteration&quot;, 
     main=&quot;Trace of Markov chain&quot;)

# plot the density of the chain
plot(density(xs), main = &quot;Density of samples&quot;, xlim=c(-3,3))
seq &lt;- seq(-20,20,length.out=1e5)
lines(seq, target(seq), col=&quot;red&quot;)</code></pre><p><img src=https://jakespiteri.co.uk/portfolio/computing-1/report-10/index_files/figure-html/unnamed-chunk-18-2.png width=1536 style=display:block;margin:auto></p><p>One way that we may attempt to overcome the pseudo-convergence phenomenon is to run <em>multiple chains</em>. This simply means that we run the MCMC algorithm multiple times with different initializations. If the chain seems to have converged to different distributions for different initializations, then the algorithm has failed to fully explore the state space. We may need to run the chain for a longer period of time until it converges to the target distribution, or we may need to adjust the proposal distribution to improve the chain’s mixing. This introduces a new problem: given a fixed amount of computation, should we produce one long chain or multiple chains? This `multistart heuristic’ should be used with caution — it only guarantees against pseudo-convergence if we can ensure that the starting points cover every part of the state space which may pseudo-converge.</p><p>Another practical aspect of MCMC methods that we should be aware of is <strong>burn-in</strong>. When initializing our algorithm it may take some time before the Markov chain converges to a stationary distribution, and we call this the burn-in period. The samples produced before the chain has converged are not likely to have been produced by the target distribution, and so we discard these samples. The best way to see which samples to discard is to manually inspect the chain in order to see at which iteration it began to converge.</p><p>Below we parallelize our computations and run one chain per one physical CPU core. The details of the target distribution in this example do not matter — often we do not know them in practice — the point is that we can observe pseudo-convergence in the trace plots of the chains. This occurs because our proposal distribution has a small variance and thus the chains cannot fully explore the state space.</p><pre class=r><code>simulate.chains &lt;- function(P, x0, n, n.chains, n.cores = 4) {
  require(parallel)
  require(doParallel)
  # parallelize chains
  registerDoParallel(n.cores)
  simulations &lt;- foreach(i=1:n.chains, .combine=&quot;rbind&quot;) %dopar% {
    x &lt;- x0[i]
    foreach(i=1:n, .combine=&quot;c&quot;) %do% {P(x)}
  }
  stopImplicitCluster()
  return(simulations)
}

# generate markov chain using MH algorithm
# target: mixture of two normals
n &lt;- 5e3; set.seed(123)
target &lt;- function(x) {0.6*dnorm(x, -2, 1) + 0.4*dnorm(x, 5, 2)}
xs &lt;- simulate.chains(make.metropolis.hastings.kernel(target,
                                                      make.normal.proposal(1)), 
                      runif(4,-10,10), n, 4)</code></pre><pre><code>## Loading required package: parallel</code></pre><pre><code>## Loading required package: doParallel</code></pre><pre><code>## Loading required package: foreach</code></pre><pre><code>## 
## Attaching package: &#39;foreach&#39;</code></pre><pre><code>## The following objects are masked from &#39;package:purrr&#39;:
## 
##     accumulate, when</code></pre><pre><code>## Loading required package: iterators</code></pre><pre class=r><code># setup plot space
par(mfrow=c(2,2))

# plot the chains
ylimits &lt;- c(min(xs), max(xs))
for(i in 1:4){
  plot(1:n, xs[i,], type = &#39;l&#39;, xlab=&quot;iteration&quot;, ylim=ylimits,
       ylab=&quot;x&quot;, main=paste0(&quot;Trace of Markov chain &quot;, i))
}</code></pre><p><img src=https://jakespiteri.co.uk/portfolio/computing-1/report-10/index_files/figure-html/unnamed-chunk-19-1.png width=960 style=display:block;margin:auto></p></div></div></div></div></div><footer class=post-footer><ul class=post-tags></ul><nav class=paginav><a class=prev href=https://jakespiteri.co.uk/portfolio/computing-1/report-9/><span class=title>« Prev</span><br><span>Portfolio Report 9: Numerical Optimization</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2024 <a href=https://jakespiteri.co.uk/>Jake Spiteri</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>