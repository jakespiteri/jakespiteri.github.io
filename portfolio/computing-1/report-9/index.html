<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Portfolio Report 9: Numerical Optimization | Jake Spiteri</title><meta name=keywords content><meta name=description content="Numerical Optimization An optimization problem is the problem of finding the best solution within a set of feasible solutions. There are many characteristics of optimization problems that define them and determine the methods needed to solve them. For example, we may have continuous or discrete problems; convex or non-convex problems; linear or non-linear problems. In this document we will consider continuous problems, and make no explicit assumptions on the convexity or the linearity of the problem."><meta name=author content="Jake Spiteri"><link rel=canonical href=https://jakespiteri.co.uk/portfolio/computing-1/report-9/><meta name=google-site-verification content="XYZabc"><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/katex.min.css integrity=sha384-bYdxxUwYipFNohQlHt0bjN/LCpueqWz13HufFEV1SUatKs1cm4L6fFgCi1jT643X crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/katex.min.js integrity=sha384-Qsn9KnoKISj6dI8g7p1HBlNpVx0I8p1SvlwOldgi3IorMle61nQy4zEahWYtljaz crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js onload=renderMathInElement(document.body)></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}]})})</script><link crossorigin=anonymous href=/assets/css/stylesheet.ccdd7c01ba69b30dd10ab397e0d0f0774c08df41d39f6b53e16f862223c92b98.css integrity="sha256-zN18Abppsw3RCrOX4NDwd0wI30HTn2tT4W+GIiPJK5g=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://jakespiteri.co.uk/favi.png><link rel=icon type=image/png sizes=16x16 href=https://jakespiteri.co.uk/favi.png><link rel=icon type=image/png sizes=32x32 href=https://jakespiteri.co.uk/favi.png><link rel=apple-touch-icon href=https://jakespiteri.co.uk/favi.png><link rel=mask-icon href=https://jakespiteri.co.uk/favi.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-123-45","auto"),ga("send","pageview"))</script><meta property="og:title" content="Portfolio Report 9: Numerical Optimization"><meta property="og:description" content="Numerical Optimization An optimization problem is the problem of finding the best solution within a set of feasible solutions. There are many characteristics of optimization problems that define them and determine the methods needed to solve them. For example, we may have continuous or discrete problems; convex or non-convex problems; linear or non-linear problems. In this document we will consider continuous problems, and make no explicit assumptions on the convexity or the linearity of the problem."><meta property="og:type" content="article"><meta property="og:url" content="https://jakespiteri.co.uk/portfolio/computing-1/report-9/"><meta property="og:image" content="https://jakespiteri.co.uk/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="portfolio"><meta property="og:site_name" content="Jake Spiteri"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://jakespiteri.co.uk/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Portfolio Report 9: Numerical Optimization"><meta name=twitter:description content="Numerical Optimization An optimization problem is the problem of finding the best solution within a set of feasible solutions. There are many characteristics of optimization problems that define them and determine the methods needed to solve them. For example, we may have continuous or discrete problems; convex or non-convex problems; linear or non-linear problems. In this document we will consider continuous problems, and make no explicit assumptions on the convexity or the linearity of the problem."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Portfolio","item":"https://jakespiteri.co.uk/portfolio/"},{"@type":"ListItem","position":2,"name":"Statistical Computing 1","item":"https://jakespiteri.co.uk/portfolio/computing-1/"},{"@type":"ListItem","position":3,"name":"Portfolio Report 9: Numerical Optimization","item":"https://jakespiteri.co.uk/portfolio/computing-1/report-9/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Portfolio Report 9: Numerical Optimization","name":"Portfolio Report 9: Numerical Optimization","description":"Numerical Optimization An optimization problem is the problem of finding the best solution within a set of feasible solutions. There are many characteristics of optimization problems that define them and determine the methods needed to solve them. For example, we may have continuous or discrete problems; convex or non-convex problems; linear or non-linear problems. In this document we will consider continuous problems, and make no explicit assumptions on the convexity or the linearity of the problem.","keywords":[],"articleBody":" Numerical Optimization An optimization problem is the problem of finding the best solution within a set of feasible solutions. There are many characteristics of optimization problems that define them and determine the methods needed to solve them. For example, we may have continuous or discrete problems; convex or non-convex problems; linear or non-linear problems. In this document we will consider continuous problems, and make no explicit assumptions on the convexity or the linearity of the problem. We will also focus on minimizing a function (maximizing \\(f(x)\\) is equivalent to minimizing \\(-f(x)\\)).\nThe general form of an optimization problem is \\[\\begin{alignat*}{3} \u0026 \\text{minimize } \\quad \u0026\u0026 f(x) \\\\ \u0026 \\text{subject to } \\quad \u0026\u0026 g_i(x) \\leq 0, \\quad \u0026\u0026 i = 1, \\dots, m \\\\ \u0026 \u0026\u0026 h_j(x) = 0, \\quad \u0026\u0026 j = 1, \\dots, p, \\end{alignat*}\\] where \\(f: \\mathbb{R}^n \\rightarrow \\mathbb{R}\\) is our objective function and \\(g_i\\) and \\(h_j\\) are our constraint functions.\nThe above formulation is a constrained optimization problem. We will consider unconstrained optimization in this document. Note that the R function optim offers L-BFGS-B for ‘box’ constrained optimization problems, and more options are supplied using constrOptim.\nOne-Dimensional Optimization The optimization problem is rather difficult. Algorithms make it easy to find local optima, but finding global optima for non-convex problems is much harder. This is best illustrated by attempting to optimize a one-dimensional function. We can clearly see the global minimum in this scenario, but we cannot guarantee convergence to a global minimum with optimization algorithms.\nBelow we have plotted a simple curve which is periodic over \\(2\\pi\\).\nf \u003c- function(x) {return(cos(x) + cos(2*x) + sin(2*x))} curve(f, 0, 2*pi) We can clearly see the global minimum is obtained at approximately 2, and a local minimum occurs around 5. We will test the optimize function in base R and see whether it finds the global minimum.\noptimize(f, interval=c(0,2*pi)) ## $minimum ## [1] 2.116949 ## ## $objective ## [1] -1.867534 We see that optimize quickly found the global minimum in this case. However, if our interval is wider, optimize returns the local minimum.\noptimize(f, interval=c(0,4*pi)) ## $minimum ## [1] 4.928807 ## ## $objective ## [1] -1.112494 The function optimize is based on golden section search, which works well for uni-modal functions. However, when a function has multiple extrema the golden section search method simply finds one of them and provides no assurance as to whether this is a global optimum point. It is useful in that it provides a derivative-free optimization method, which is incredibly fast and scales well.\nIn this case our function is one-dimensional meaning we can easily check whether we have indeed found the global minimum. However, when we have much higher-dimensional data it becomes nearly impossible to confirm whether we have found a global optimal point.\nNewton’s method for optimization Newton’s method aims to solve the unconstrained optimization problem:\n\\[\\begin{alignat*}{3} \u0026 \\text{minimize } \\quad \u0026\u0026 f(x) . \\end{alignat*}\\]\nIn order to do this, we iteratively update \\(x_k\\), where \\(x_{k+1}\\) is chosen such that \\[x_{k+1} = \\text{argmin}_{x} f(x_k) + f'(x_k)(x - x_k) + \\frac{1}{2}f''(x_k)(x - x_k)^2.\\] Simply differentiating with respect to \\(x\\) and setting equal to zero, we see that \\(x_{k+1} = x_k - \\frac{f'(x_k)}{f''(x_k)}.\\) Note that the update uses second-order information — the second derivative — and so converges faster. Newton’s method converges quadratically! It can also easily be rewritten for the multivariate case.\nIt’s important to note that at each iterate \\(x_k\\), Newton’s method chooses the next iterate \\(x_{k+1}\\) by minimizing a quadratic approximation of the true curve at \\(x_k\\).\nWe implement this in R and test the speed of convergence.\nf_exp \u003c- expression(cos(x) + cos(2*x) + sin(2*x)) f1_exp \u003c- D(f_exp, 'x') f1 \u003c- function(x) eval(f1_exp) f2_exp \u003c- D(f1_exp, 'x') f2 \u003c- function(x) eval(f2_exp) x \u003c- 1.5 for(i in 1:5) { x \u003c- x - f1(x)/f2(x) cat(x, '\\n') } ## 2.48044 ## 1.987202 ## 2.116103 ## 2.116954 ## 2.116954 We see that our optimization algorithm converges in only three steps! Newton’s method is incredibly fast, and is only held back by the fact that we require the Hessian. It has computational complexity \\(\\mathcal{O}(n^3)\\) as it needs to compute the inverse of the Hessian.\nHere we demonstrate that at a given point, Newton’s method computes the quadratic approximation of the true curve at that point and minimizes it. We start close to the global minimum, with \\(x_k = 1.7\\). The quadratic approximation at \\(x_k\\) is plotted in blue, and we see that Newton’s method chooses \\(x_{k+1}\\approx2.2\\).\ncat(\"x_k = \", 1.7, \"\\n\", sep=\"\") ## x_k = 1.7 cat(\"x_k+1 = \", 1.7 - f1(1.7)/f2(1.7), sep=\"\") ## x_k+1 = 2.181084 curve(f, 0, 2*pi) second.order.approx \u003c- function(x, xk) f(xk) + f1(xk)*(x-xk) + 1/2*f2(xk)*(x-xk)^2 seq \u003c- seq(0,2*pi,length.out=100) lines(seq, second.order.approx(seq, 1.7), col=\"blue\") Newton’s method is also very dependent upon the starting point. We see below that if we were to initialize \\(x\\) at \\(1\\), then Newton’s method converges to the local minimum near \\(x=5\\). Under a different initialization at \\(x=0.5\\), the algorithm converges to the global maximum almost immediately. This occurs when our Hessian matrix becomes negative definite which corresponds to finding the maximum of a curve (recall that a sufficient condition for a strict local maximum (minimum) is that the Hessian is negative (positive) definite).\n# initialized at 1 x \u003c- 1 par(mfrow=c(1,4)) for(i in 1:4) { # compute updates x \u003c- x - f1(x)/f2(x) cat(\"x\", i, \" = \", round(x, 2), \"\\n\", sep=\"\") # plot the approximation of the true curve at x1 curve(f, -2*pi, 2*pi, ylim=c(-2,5), main=\"Quadratic approximation of f(x)\") seq \u003c- seq(-2*pi,2*pi,length.out=100) lines(seq, second.order.approx(seq, x), col=\"blue\") } ## x1 = -0.39 ## x2 = 2.96 ## x3 = 4.77 ## x4 = 4.94 # initialized at 0.5 x \u003c- 0.5 par(mfrow=c(1,1)) # compute updates x \u003c- x - f1(x)/f2(x) for(i in 1:3) { # compute updates x \u003c- x - f1(x)/f2(x) cat(\"x\", i, \" = \", round(x, 2), \"\\n\", sep=\"\") } ## x1 = 0.33 ## x2 = 0.33 ## x3 = 0.33 # plot the approximation of the true curve at x1 curve(f, -2*pi, 2*pi, ylim=c(-2,5), main=\"Quadratic approximation of f(x)\") seq \u003c- seq(-2*pi,2*pi,length.out=100) lines(seq, second.order.approx(seq, x), col=\"blue\") We have seen that one downside of Newton’s method is that it doesn’t differentiate between minima and maxima. It also requires the Hessian which can be difficult to produce for high-dimensional problems. The method presented above is known as pure Newton’s method, and there are variations which avoid these problems. To avoid finding local maxima we can ensure that the Hessian is positive (semi-) definite at each step, if it is not then we can modify it such that it is (e.g. we can add scalar multiple of the identity matrix to the Hessian such that it becomes positive definite, or we can take the eigendecomposition and modify the eigenvalues such that they are positive.). To avoid computing the Hessian we can use quasi-Newton methods which are similar to Newton’s method but they replace the exact Hessian with an approximation of the Hessian.\nSo far we have seen that one-dimensional unconstrained optimization is not as easy as we might expect. The problem becomes even more complex as the number of dimensions increase, and the computational complexity of our optimization algorithm becomes important.\nMulti-dimensional optimization Most of the objective functions we need to optimize are multi-dimensional. There are three common categories of optimization algorithms:\nSimplex methods — only uses the value of the function. First-order methods — uses the value of the function and the value of its first derivative. Gradient descent is a popular method within this category. Second-order methods — uses the value of the function, its first derivative, and its second derivative (the Hessian). Newton methods fall under this category. The most popular method for multi-dimensional optimization in R is the optim function. optim implements a number of optimization algorithms such as Nelder-Mead, BFGS, CG (conjugate gradient), L-BFGS-B, SANN, and Brent. Another approach is to use the nlm (non-linear minimization) function, which uses a Newton-type algorithm.\nSimplex methods The Nelder-Mead algorithm is a well-known simplex method which is easily used in R.\nGradient-type methods Gradient-type methods are much faster at converging than simplex methods, and avoid evaluating the Hessian. The most well-known method is steepest descent (gradient descent in the machine learning literature), in which we iteratively set \\[x_{k+1} = x_k - \\alpha \\nabla f(x_k),\\] where \\(f(x)\\), \\(x \\in \\mathbb{R}^n\\), is a differentiable and multivariate function, and \\(\\alpha\\) is our step size (also known as learning rate). We can perform a line-search to find the optimal value of \\(\\alpha\\) at each step. \\(\\alpha\\) is often chosen to be a small value such as \\(0.03\\) but can be tuned to the specific problem. If our step size is too large the optimization algorithm will step beyond the optimal solution and could diverge.\nSteepest descent converges slowly for high-dimensional problems but it has a very low computational cost. This is why steepest descent is the most widely-used optimization algorithm in the machine learning literature. When optimizing a neural network with hundreds of thousands of parameters, Newton’s method cannot be used due to its high computational cost (\\(\\mathcal{O}(n^3)\\)). The computational cost of quasi-Newton methods is lower, but still cannot scale to such high-dimensional problems.\nGradient descent is well-known for its tendency to zigzag. Given its frequent use in machine learning, many improvements have been proposed in the machine learning literature. We will look at some of these proposals, implement them, and observe their rate of convergence.\nWe will consider the function \\(f(x, y) = x^2 + 10y^2\\). Below is a contour plot of this function.\n# create contour plot f \u003c- function(x, y) {x^2 + 10*y^2} s \u003c- seq(-100,100,length=300) contmat \u003c- outer(s, s, Vectorize(function(x,y) f(x,y))) rownames(contmat) \u003c- colnames(contmat) \u003c- s # keep x and y values and not the index contlong \u003c- melt(contmat) # melt into long format ## Warning in type.convert.default(X[[i]], ...): 'as.is' should be specified by the ## caller; using TRUE ## Warning in type.convert.default(X[[i]], ...): 'as.is' should be specified by the ## caller; using TRUE b \u003c- exp(seq(log(10), log(100000), length = 25)) contplot \u003c- contlong %\u003e% ggplot() + geom_contour(aes(x = X1, y = X2, z = value, colour = stat(level)), breaks = b) + ggtitle(\"Contour plot of f(x,y)\") + theme_minimal() contplot Let’s implement a simple version of steepest descent and view its zigzagging nature. We see that when the state space varies much more in one dimension than another, the algorithm tends to zigzag.\ngraddesc \u003c- function(x0, fexp, iter, lr) { # get first derivative pars \u003c- paste(\"x\", 1:length(x0), sep=\"\") f1 \u003c- deriv(fexp, namevec = pars, function.arg = TRUE) # do gradient descent and keep all steps x_steps \u003c- matrix(NA, iter+1, length(x0)) x_steps[1,] \u003c- x_gd \u003c- x0 for (i in 1:iter) { deriv \u003c- attributes(do.call(f1, as.list(x_gd)))$gradient x_gd \u003c- x_gd - lr * deriv x_steps[i+1,] \u003c- c(x_gd) } return(x_steps) } f \u003c- expression(x1^2 + 10*x2^2) f_gd \u003c- as.data.frame(graddesc(c(-100,-50), f, 20, 0.09)) contplot + geom_line(data = f_gd, mapping = aes(x = V1, y = V2), colour = 'red') + geom_point(data = f_gd, mapping = aes(x = V1, y = V2), shape = 21, colour = 'red') + labs(subtitle = \"using gradient descent\") We see that for a non-optimal learning rate size (one that is too large) the gradient descent steps zigzag.\nGradient descent variants Gradient descent with momentum This method replaces the gradient of the objective function in the update with an exponentially weighted average of past gradients. This has the effect of smoothing out the update and dampens potential zigzagging. The iterative updates are \\[\\begin{alignat*}{2} x_{k+1} \u0026= x_k - \\alpha V_k \\\\ V_k \u0026= \\beta V_{k-1} + (1 - \\beta)\\nabla f(x_k), \\end{alignat*}\\] where \\(\\beta\\) is a hyperparameter called the momentum, which ranges between 0 and 1. The larger the \\(\\beta\\), the greater the smoothing effect. A popular choice for \\(\\beta\\) is 0.9.\nMomentum helps accelerate gradient descent by dampening oscillations. We will implement gradient descent with momentum and see if there is an improvement. Note that when \\(\\beta = 0\\), gradient descent with momentum is simply gradient descent.\ngraddescmomentum \u003c- function(x0, fexp, iter, lr, momentum) { # get first derivative pars \u003c- paste(\"x\", 1:length(x0), sep=\"\") f1 \u003c- deriv(fexp, namevec = pars, function.arg = TRUE) # do gradient descent and keep all steps x_steps \u003c- matrix(NA, iter+1, length(x0)) x_steps[1,] \u003c- x_gd \u003c- x0 V \u003c- rep(0,length(x0)) for (i in 1:iter) { deriv \u003c- attributes(do.call(f1, as.list(x_gd)))$gradient V \u003c- momentum * V + (1 - momentum) * deriv x_gd \u003c- x_gd - lr * V x_steps[i+1,] \u003c- c(x_gd) } return(x_steps) } f \u003c- expression(x1^2 + 10*x2^2) f_gd \u003c- as.data.frame(graddescmomentum(c(-100,-50), f, 20, 0.09, 0.5)) contplot + geom_line(data = f_gd, mapping = aes(x = V1, y = V2), colour = 'red') + geom_point(data = f_gd, mapping = aes(x = V1, y = V2), shape = 21, colour = 'red') + labs(subtitle = \"using gradient descent with momentum\") RMSprop When finding the minimum point of a function, we may want to take large steps towards the minimum when we are far from it, and smaller steps when we are close. In order to achieve this we can employ a line search method, or a learning rate scheduler which decreases the learning rate at specified intervals. There are also methods which employ adaptive gradients, such as RMSprop.\nThe key idea of RMSprop is to store an exponentially weighted average of the squared gradients for each weight. RMSprop scales the gradient updates by dividing the learning rate by the square root of an exponentially decaying average of squared gradients — hence the name root mean-squared propagation.\nRMSprop has the update formula \\[\\begin{alignat*}{2} x_{k+1} \u0026= x_k - \\frac{\\alpha}{\\sqrt{\\nu_k + \\epsilon}}\\nabla f(x_{k}), \\\\ \\nu_k \u0026= \\beta \\nu_{k-1} + (1 - \\beta) \\nabla f(x_{k})^2, \\end{alignat*}\\] where \\(\\beta\\) is the weighting of the moving average. A suggested default value for \\(\\beta\\) is \\(0.9\\).\nrmsprop \u003c- function(x0, fexp, iter, lr, beta, eps = 1e-5) { # get first derivative pars \u003c- paste(\"x\", 1:length(x0), sep=\"\") f1 \u003c- deriv(fexp, namevec = pars, function.arg = TRUE) # do gradient descent and keep all steps x_steps \u003c- matrix(NA, iter+1, length(x0)) x_steps[1,] \u003c- x_gd \u003c- x0 ms \u003c- rep(0,length(x0)) for (i in 1:iter) { deriv \u003c- attributes(do.call(f1, as.list(x_gd)))$gradient ms \u003c- beta * ms + (1 - beta) * deriv^2 x_gd \u003c- x_gd - (lr / sqrt(ms + eps)) * deriv x_steps[i+1,] \u003c- c(x_gd) } return(x_steps) } f \u003c- expression(x1^2 + 10*x2^2) f_gd \u003c- as.data.frame(rmsprop(c(-100,-50), f, 120, 1, 0.9)) contplot + geom_line(data = f_gd, mapping = aes(x = V1, y = V2), colour = 'red') + geom_point(data = f_gd, mapping = aes(x = V1, y = V2), shape = 21, colour = 'red') + labs(subtitle = \"using RMSprop\") Adam Adaptive Moment Estimation (Adam) also implements adaptive learning rates. Similar to RMSprop above, Adam stores an exponentially decaying average of squared gradients, and it also stores an exponentially decaying average of gradients similar to momentum.\nAdams update formula is \\[\\begin{alignat*}{2} x_{k+1} \u0026= x_k - \\frac{\\alpha}{\\sqrt{\\widehat{\\nu}_k} + \\epsilon}\\widehat{m}_k, \\\\ m_k \u0026= \\beta_1 m_{k-1} + (1 - \\beta_1) \\nabla f(x_{k}),\\\\ \\nu_k \u0026= \\beta_2 \\nu_{k-1} + (1 - \\beta_2) \\nabla f(x_{k})^2,\\\\ \\widehat{m}_k \u0026= \\frac{m_k}{1 - \\beta_1^k},\\\\ \\widehat{\\nu}_k \u0026= \\frac{\\nu_k}{1 - \\beta_2^k}. \\end{alignat*}\\]\nIn the above, \\(\\beta_1\\) and \\(\\beta_2\\) are hyperparameters to be tuned. The original creaters of Adam propose default values of \\(0.9\\) for \\(\\beta_1\\), \\(0.999\\) for \\(\\beta_2\\), and \\(10^{-8}\\) for \\(\\epsilon\\).\nWe can think of \\(m_k\\) and \\(\\nu_k\\) as the first moment (the mean) and the second moment (the uncentered variance) respectively, hence the name adaptive moment estimation. As \\(m\\) and \\(\\nu\\) are often initialized as (vectors of) zeros, they tend to be biased towards zero in the first few iterations. To avoid this bias towards zero we use \\(\\widehat{m}\\) and \\(\\widehat{\\nu}\\) as specified above.\nadam \u003c- function(x0, fexp, iter, lr, beta1, beta2, eps = 1e-8) { # get first derivative pars \u003c- paste(\"x\", 1:length(x0), sep=\"\") f1 \u003c- deriv(fexp, namevec = pars, function.arg = TRUE) # do gradient descent and keep all steps x_steps \u003c- matrix(NA, iter+1, length(x0)) x_steps[1,] \u003c- x_gd \u003c- x0 V \u003c- S \u003c- rep(0,length(x0)) for (i in 1:iter) { deriv \u003c- attributes(do.call(f1, as.list(x_gd)))$gradient V \u003c- beta1 * V + (1-beta1) * deriv S \u003c- beta2 * S + (1-beta2) * deriv^2 Vhat \u003c- V/(1-beta1^i) Shat \u003c- S/(1-beta2^i) x_gd \u003c- x_gd - lr / (sqrt(Shat) + eps) * Vhat x_steps[i+1,] \u003c- c(x_gd) } return(x_steps) } f \u003c- expression(x1^2 + 10*x2^2) f_gd \u003c- as.data.frame(adam(c(-100,-50), f, 100, 2, 0.9, 0.999)) contplot + geom_line(data = f_gd, mapping = aes(x = V1, y = V2), colour = 'red') + geom_point(data = f_gd, mapping = aes(x = V1, y = V2), shape = 21, colour = 'red', fill=\"white\") + labs(subtitle = \"using adam\") Second-order methods Second-order methods converge much faster than first-order methods, but they also require more computation. We have already explained Newton’s method which is arguably the most widely-used second-order method, and so we will provide a brief summary here.\nNewton’s method minimizes the quadratic approximation of \\(f(x)\\) around \\(x_k\\): \\[Q(x) = f(x_k) + \\nabla f(x_k)^T (x - x_k) + \\frac{1}{2} (x - x_k)^T \\nabla^2 f(x_k) (x - x_k)\\] I.e. \\(x_{k+1} = \\text{argmin}_x Q(x)\\). To find the minimum point we simply differentiate with respect to x and set equal to zero. This gives the update formula \\[x_{k+1} = x_k - (\\nabla^2 f(x_k))^{-1} \\nabla f(x_k).\\] We often include a step size parameter \\(\\alpha\\) to control how large our steps are.\nNewton’s method converges quadratically and has computational cost \\(\\mathcal{O}(n^3)\\) for \\(x \\in \\mathbb{R}^n\\)\nPure Newton’s method may diverge if our initialization is bad. To avoid this we must ensure that our Hessian is positive (semi-) definite at each step, and ensure our step size is chosen such that the function decreases.\nComputing the Hessian may be very difficult even if we use automatic differentiation. Storing the Hessian also becomes a problem for very high-dimensional problems.\nBelow we optimize \\(f(x, y) = x^2 + 10y^2\\) using Newton’s method. It converges in just \\(1\\) step! This is because the function is quadratic in two variables and thus the quadratic approximation to the true function computed by Newton’s method is exact. Newton’s method directly minimizes the true function in this case.\nnewton \u003c- function(x0, fexp, iter, lr) { # get first and second derivative pars \u003c- paste(\"x\", 1:length(x0), sep=\"\") d \u003c- deriv(fexp, namevec = pars, function.arg = TRUE, hessian = TRUE) # do gradient descent and keep all steps x_steps \u003c- matrix(NA, iter+1, length(x0)) x_steps[1,] \u003c- x_n \u003c- x0 ms \u003c- rep(0,length(x0)) for (i in 1:iter) { deriv \u003c- attributes(do.call(d, as.list(x_n))) deriv_1 \u003c- t(deriv$gradient) deriv_2 \u003c- deriv$hessian[,,1:length(x0)] x_n \u003c- x_n - lr*solve(deriv_2)%*%deriv_1 x_steps[i+1,] \u003c- c(x_n) } return(x_steps) } f \u003c- expression(x1^2 + 10*x2^2) f_n \u003c- as.data.frame(newton(c(-100,-50), f, 5, 1)) contplot + geom_line(data = f_n, mapping = aes(x = V1, y = V2), colour = 'red') + geom_point(data = f_n, mapping = aes(x = V1, y = V2), shape = 21, colour = 'red', fill=\"white\") + labs(subtitle = \"using Newton's method\") Quasi-Newton methods Quasi-Newton methods are great substitutes for second-order methods. They approximate the Hessian at each iteration, rather than evaluate the true Hessian. This avoids the problem of computing the true Hessian for high-dimensional objective functions. Quasi-Newton methods are not as fast as Newton’s methods — they attain superlinear convergence. However, they also have a lower computational cost! They have complexity \\(\\mathcal{O}(n^2)\\) whilst Newton’s method carries complexity \\(\\mathcal{O}(n^3)\\).\nBFGS is arguably the best quasi-Newton method which has been proposed thus far. It has update formula \\(x_{k+1} = x_k - \\alpha_k B_k \\nabla f(x_k)\\), where \\(B_k\\) is an \\(n \\times n\\) symmetric positive definite matrix that is updated at each iteration and \\(\\alpha_k\\) is our step size.\nIf we have a objective function with \\(1000\\) variables, the Hessian and its approximation is a \\(1000 \\times 1000\\) dimension matrix. This is a large matrix to keep in memory and thus a limited-memory version of BFGS is often used (L-BFGS).\nLet’s derive the BFGS update formula and see how it works. BFGS also works by minimizing the quadratic approximation of the true function around \\(x_k\\). Recall the Taylor series approximation of \\(f(x_k + d)\\) around \\(x_k\\). \\[f(x_k + d) = f(x_k) + \\nabla f(x_k)^T d + \\frac{1}{2} d^T \\nabla^2 f(x_k) d + o(\\|d\\|)^2\\] We shall define the approximation of the objective function about \\(x_k\\) as \\[m_k(d) := f(x_k) + \\nabla f(x_k)^Td + \\frac{1}{2}d^T B_k d,\\] where \\(B_k\\) is an \\(n \\times n\\) symmetric positive definite matrix which we will update. It is our approximation of the Hessian.\nThe minimizer of this convex quadratic \\(m_k(d)\\) is \\(d = B_k^{-1} \\nabla f(x_k)\\). Hence we have the BFGS update formula \\(x_{k+1} = x_k - \\alpha_k B_k^{-1} \\nabla f(x_k)\\), where \\(\\alpha_k\\) is our step size. The question now is: How do we compute \\(B_k\\)?\nInstead of recomputing \\(B_k\\) at each iteration, we will update it based on information about the curvature gained in the most recent step. Suppose we observe a new iterate \\(x_{k+1}\\), then we form the quadratic approximation \\(m_{k+1}(d) = f(x_{k+1}) + \\nabla f(x_{k+1})^Td + \\frac{1}{2}d^T B_{k+1} d\\) In order to determine \\(B_{k+1}\\) we place restrictions on our approximation \\(m_{k+1}(d)\\). We want \\(m_{k+1}(d)\\) to be a good approximation to the objective function at \\(x_{k+1}\\). We propose that a good approximation will have the same gradient as the true function \\(f\\) at the two most recent iterates. That is, we require\n\\(\\nabla m_{k+1}(0) = \\nabla f(x_{k+1}),\\)\n\\(\\nabla m_{k+1}(- \\alpha_k d_k) = \\nabla f(x_{k+1}) - \\alpha_k B_{k+1} d_k = \\nabla f(x_k).\\)\nThis first requirement is always true. The second requirement can be rewritten as \\[\\alpha_k B_{k+1} d_k = \\nabla f(x_{k+1}) - \\nabla f(x_{k}).\\] Using the notation \\(y_k = \\nabla f(x_{k+1}) - \\nabla f(x_{k})\\), and \\(s_k = x_{k+1} - x_k\\) \\((= \\alpha_k d)\\), we can rewrite the above equation as the secant equation \\[B_{k+1}s_k = y_k.\\] Note that if we approximate the Hessian we will then need to find its inverse to update \\(x\\). To avoid the expensive inversion, BFGS directly approximates the inverse of the Hessian which we will denote \\(H\\). Thus, we rewrite the secant equation \\[ H_{k+1} y_k = s_k.\\] This means that we require the symmetric positive definite matrix \\(H_{k+1}\\) to map \\(y_k\\) to \\(s_k\\). This is only possible if \\(s_k\\) and \\(y_k\\) satisfy the curvature condition: \\(y_k^T s_k \u003e 0\\). If \\(f\\) is strongly convex then this is always true for any \\(x_k\\) and \\(x_{k+1}\\). If \\(f\\) is non-convex then we must impose this restriction explicitly. We can do this by imposing the (strong) Wolfe conditions on the line search for our step size.\nThe secant equations place \\(n\\) restrictions on \\(H_{k+1}\\), but we know that \\(H_{k+1}\\) is symmetric positive definite and thus has \\(n(n+1)/2\\) free parameters. Hence \\(H_{k+1}\\) has infinitely many solutions. In order to obtain a unique solution we must impose a further restriction. We require that among all symmetric matrices that satisfy the secant equation, \\(H_{k+1}\\) is in some sense closest to \\(H_k\\). We form the optimization problem \\[\\begin{alignat*}{2} \u0026\\text{minimize }_{H} \\quad \u0026\u0026\\| H - H_k \\|, \\\\ \u0026\\text{subject to } \u0026\u0026 H = H^T, \\\\ \u0026 \u0026\u0026 H y_k = s_k, \\end{alignat*}\\] where \\(s_k\\) and \\(y_k\\) satisfy the curvature condition and \\(H_k\\) is symmetric and positive definite.\nDifferent matrix norms result in different quasi-Newton methods. A norm which provides an easy solution to the optimization problem which is also scale-invariant is the weighted Frobenius norm \\[\\|A\\|_W = \\|W^{1/2}AW^{1/2}\\|_F,\\] where \\(\\|\\cdot\\|_F\\) is defined by \\(\\|A\\|_F = \\sum_{i=1}^n\\sum_{j=1}^n a_{ij}^2\\).\nThe matrix W can be chosen to be any matrix satisfying \\(Ws_k = y_k\\). One possible \\(W\\) which we know gives a solution is the inverse of the average Hessian. That is, \\[W = \\bar{G}_k^{-1}, \\quad \\text{where } \\bar{G}_k = \\int_0^1 \\nabla^2 f(x_k + t \\alpha_k d_k) dt.\\]\nThe choices above: the weighted Frobenius norm and \\(W\\) the inverse of the average Hessian provide the following unique solution to the optimization problem\n\\[H_{k+1} = (I - \\rho_k s_k y_k^T) H_k (I - \\rho y_k s_k^T) + \\rho_k s_k s_k^T,\\] where \\(\\rho_k = \\frac{1}{y_k^T s_k}\\).\nGiven this update formula for \\(H_{k+1}\\), the resulting BFGS algorithm is quite simple!\nBelow we continue to optimize the function \\(f(x, y) = x^2 + 10y^2\\). We use BFGS and see that it converges in only 3 steps! Clearly it is far superior to first-order methods when the objective function has a reasonable number of parameters. This is of course not an easy number to quantify, but well-defined objective functions with a large number of parameters can be optimized with BFGS.\nf \u003c- function(x) {(x[1]^2 + 10*x[2]^2)} f_bfgs \u003c- c(-100,-50) for(i in 1:10) { f_bfgs \u003c- rbind(f_bfgs, optim(c(-100,-50), f, method = \"BFGS\", control=list(maxit=i))$par) } f_bfgs \u003c- as.data.frame(f_bfgs, row.names=1:11) contplot + geom_path(data = f_bfgs, mapping = aes(x = V1, y = V2), colour = 'red') + geom_point(data = f_bfgs, mapping = aes(x = V1, y = V2), shape = 21, colour = 'red', fill=\"white\") + labs(subtitle = \"using BFGS\") ","wordCount":"4106","inLanguage":"en","datePublished":"0001-01-01T00:00:00Z","dateModified":"0001-01-01T00:00:00Z","author":{"@type":"Person","name":"Jake Spiteri"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://jakespiteri.co.uk/portfolio/computing-1/report-9/"},"publisher":{"@type":"Organization","name":"Jake Spiteri","logo":{"@type":"ImageObject","url":"https://jakespiteri.co.uk/favi.png"}}}</script></head><body id=top><header class=header><nav class=nav><div class=logo><a href=https://jakespiteri.co.uk/ accesskey=h title="Home (Alt + H)"><div class=custom-logo><span class=logo__mark></span>
<span class=logo__text>>$ cd /home</span>
<span class=logo__cursor style=background-color:#1290ff></span></div></a><div class=logo-switches></div></div><ul id=menu style=font-family:monospace,monospace><li><a href=https://jakespiteri.co.uk/about/ title=/about><span>/about</span></a></li><li><a href=https://jakespiteri.co.uk/portfolio/ title=/portfolio><span>/portfolio</span></a></li><li><a href=https://jakespiteri.co.uk/blog/ title=/blog><span>/blog</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://jakespiteri.co.uk/>Home</a>&nbsp;»&nbsp;<a href=https://jakespiteri.co.uk/portfolio/>Portfolio</a>&nbsp;»&nbsp;<a href=https://jakespiteri.co.uk/portfolio/computing-1/>Statistical Computing 1</a></div><h1 class=post-title>Portfolio Report 9: Numerical Optimization</h1><div class=post-meta>Jake Spiteri&nbsp;|&nbsp;<a href=https://github.com/jakespiteri.github.io/content/portfolio/computing-1/Report-9/index.html rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><div class=post-content><div id=numerical-optimization class="section level1"><h1>Numerical Optimization</h1><p>An optimization problem is the problem of finding the best solution within a set of feasible solutions. There are many characteristics of optimization problems that define them and determine the methods needed to solve them. For example, we may have continuous or discrete problems; convex or non-convex problems; linear or non-linear problems. In this document we will consider continuous problems, and make no explicit assumptions on the convexity or the linearity of the problem. We will also focus on minimizing a function (maximizing <span class="math inline">\(f(x)\)</span> is equivalent to minimizing <span class="math inline">\(-f(x)\)</span>).</p><p>The general form of an optimization problem is
<span class="math display">\[\begin{alignat*}{3}
& \text{minimize } \quad && f(x) \\
& \text{subject to } \quad && g_i(x) \leq 0, \quad && i = 1, \dots, m \\
& && h_j(x) = 0, \quad && j = 1, \dots, p,
\end{alignat*}\]</span>
where <span class="math inline">\(f: \mathbb{R}^n \rightarrow \mathbb{R}\)</span> is our objective function and <span class="math inline">\(g_i\)</span> and <span class="math inline">\(h_j\)</span> are our constraint functions.</p><p>The above formulation is a constrained optimization problem. We will consider unconstrained optimization in this document. Note that the <code>R</code> function <code>optim</code> offers L-BFGS-B for ‘box’ constrained optimization problems, and more options are supplied using <code>constrOptim</code>.</p></div><div id=one-dimensional-optimization class="section level1"><h1>One-Dimensional Optimization</h1><p>The optimization problem is rather difficult. Algorithms make it easy to find local optima, but finding global optima for non-convex problems is much harder. This is best illustrated by attempting to optimize a one-dimensional function. We can clearly see the global minimum in this scenario, but we cannot guarantee convergence to a global minimum with optimization algorithms.</p><p>Below we have plotted a simple curve which is periodic over <span class="math inline">\(2\pi\)</span>.</p><pre class=r><code>f &lt;- function(x) {return(cos(x) + cos(2*x) + sin(2*x))}
curve(f, 0, 2*pi)</code></pre><p><img src=https://jakespiteri.co.uk/portfolio/computing-1/report-9/index_files/figure-html/unnamed-chunk-1-1.png width=672 style=display:block;margin:auto></p><p>We can clearly see the global minimum is obtained at approximately 2, and a local minimum occurs around 5. We will test the <code>optimize</code> function in base <code>R</code> and see whether it finds the global minimum.</p><pre class=r><code>optimize(f, interval=c(0,2*pi))</code></pre><pre><code>## $minimum
## [1] 2.116949
## 
## $objective
## [1] -1.867534</code></pre><p>We see that <code>optimize</code> quickly found the global minimum in this case. However, if our interval is wider, <code>optimize</code> returns the local minimum.</p><pre class=r><code>optimize(f, interval=c(0,4*pi))</code></pre><pre><code>## $minimum
## [1] 4.928807
## 
## $objective
## [1] -1.112494</code></pre><p>The function <code>optimize</code> is based on golden section search, which works well for uni-modal functions. However, when a function has multiple extrema the golden section search method simply finds one of them and provides no assurance as to whether this is a global optimum point. It is useful in that it provides a derivative-free optimization method, which is incredibly fast and scales well.</p><p>In this case our function is one-dimensional meaning we can easily check whether we have indeed found the global minimum. However, when we have much higher-dimensional data it becomes nearly impossible to confirm whether we have found a global optimal point.</p><div style=page-break-after:always></div></div><div id=newtons-method-for-optimization class="section level1"><h1>Newton’s method for optimization</h1><p>Newton’s method aims to solve the unconstrained optimization problem:</p><p><span class="math display">\[\begin{alignat*}{3}
& \text{minimize } \quad && f(x) .
\end{alignat*}\]</span></p><p>In order to do this, we iteratively update <span class="math inline">\(x_k\)</span>, where <span class="math inline">\(x_{k+1}\)</span> is chosen such that
<span class="math display">\[x_{k+1} = \text{argmin}_{x} f(x_k) + f'(x_k)(x - x_k) + \frac{1}{2}f''(x_k)(x - x_k)^2.\]</span>
Simply differentiating with respect to <span class="math inline">\(x\)</span> and setting equal to zero, we see that
<span class="math inline">\(x_{k+1} = x_k - \frac{f'(x_k)}{f''(x_k)}.\)</span> Note that the update uses second-order information — the second derivative — and so converges faster. Newton’s method converges quadratically! It can also easily be rewritten for the multivariate case.</p><p>It’s important to note that at each iterate <span class="math inline">\(x_k\)</span>, Newton’s method chooses the next iterate <span class="math inline">\(x_{k+1}\)</span> by minimizing a <em>quadratic approximation</em> of the true curve at <span class="math inline">\(x_k\)</span>.</p><p>We implement this in <code>R</code> and test the speed of convergence.</p><pre class=r><code>f_exp &lt;- expression(cos(x) + cos(2*x) + sin(2*x))
f1_exp &lt;- D(f_exp, &#39;x&#39;)
f1 &lt;- function(x) eval(f1_exp)
f2_exp &lt;- D(f1_exp, &#39;x&#39;)
f2 &lt;- function(x) eval(f2_exp)
x &lt;- 1.5
for(i in 1:5) {
  x &lt;- x - f1(x)/f2(x)
  cat(x, &#39;\n&#39;)
}</code></pre><pre><code>## 2.48044 
## 1.987202 
## 2.116103 
## 2.116954 
## 2.116954</code></pre><p>We see that our optimization algorithm converges in only three steps! Newton’s method is incredibly fast, and is only held back by the fact that we require the Hessian. It has computational complexity <span class="math inline">\(\mathcal{O}(n^3)\)</span> as it needs to compute the inverse of the Hessian.</p><p>Here we demonstrate that at a given point, Newton’s method computes the quadratic approximation of the true curve at that point and minimizes it. We start close to the global minimum, with <span class="math inline">\(x_k = 1.7\)</span>. The quadratic approximation at <span class="math inline">\(x_k\)</span> is plotted in blue, and we see that Newton’s method chooses <span class="math inline">\(x_{k+1}\approx2.2\)</span>.</p><pre class=r><code>cat(&quot;x_k = &quot;, 1.7, &quot;\n&quot;, sep=&quot;&quot;)</code></pre><pre><code>## x_k = 1.7</code></pre><pre class=r><code>cat(&quot;x_k+1 = &quot;, 1.7 - f1(1.7)/f2(1.7), sep=&quot;&quot;)</code></pre><pre><code>## x_k+1 = 2.181084</code></pre><pre class=r><code>curve(f, 0, 2*pi)
second.order.approx &lt;- function(x, xk) f(xk) + f1(xk)*(x-xk) + 1/2*f2(xk)*(x-xk)^2
seq &lt;- seq(0,2*pi,length.out=100)
lines(seq, second.order.approx(seq, 1.7), col=&quot;blue&quot;)</code></pre><p><img src=https://jakespiteri.co.uk/portfolio/computing-1/report-9/index_files/figure-html/unnamed-chunk-5-1.png width=672 style=display:block;margin:auto></p><p>Newton’s method is also very dependent upon the starting point. We see below that if we were to initialize <span class="math inline">\(x\)</span> at <span class="math inline">\(1\)</span>, then Newton’s method converges to the local minimum near <span class="math inline">\(x=5\)</span>. Under a different initialization at <span class="math inline">\(x=0.5\)</span>, the algorithm converges to the global maximum almost immediately. This occurs when our Hessian matrix becomes negative definite which corresponds to finding the <em>maximum</em> of a curve (recall that a sufficient condition for a strict local maximum (minimum) is that the Hessian is negative (positive) definite).</p><pre class=r><code># initialized at 1
x &lt;- 1
par(mfrow=c(1,4))
for(i in 1:4) {
  # compute updates
  x &lt;- x - f1(x)/f2(x)
  cat(&quot;x&quot;, i, &quot; = &quot;, round(x, 2), &quot;\n&quot;, sep=&quot;&quot;)
  
  # plot the approximation of the true curve at x1
  curve(f, -2*pi, 2*pi, ylim=c(-2,5), main=&quot;Quadratic approximation of f(x)&quot;)
  seq &lt;- seq(-2*pi,2*pi,length.out=100)
  lines(seq, second.order.approx(seq, x), col=&quot;blue&quot;)
}</code></pre><pre><code>## x1 = -0.39</code></pre><pre><code>## x2 = 2.96</code></pre><pre><code>## x3 = 4.77</code></pre><pre><code>## x4 = 4.94</code></pre><p><img src=https://jakespiteri.co.uk/portfolio/computing-1/report-9/index_files/figure-html/unnamed-chunk-6-1.png width=1536 style=display:block;margin:auto></p><pre class=r><code># initialized at 0.5
x &lt;- 0.5
par(mfrow=c(1,1))

# compute updates
x &lt;- x - f1(x)/f2(x)
for(i in 1:3) {
  # compute updates
  x &lt;- x - f1(x)/f2(x)
  cat(&quot;x&quot;, i, &quot; = &quot;, round(x, 2), &quot;\n&quot;, sep=&quot;&quot;)
}</code></pre><pre><code>## x1 = 0.33
## x2 = 0.33
## x3 = 0.33</code></pre><pre class=r><code># plot the approximation of the true curve at x1
curve(f, -2*pi, 2*pi, ylim=c(-2,5), main=&quot;Quadratic approximation of f(x)&quot;)
seq &lt;- seq(-2*pi,2*pi,length.out=100)
lines(seq, second.order.approx(seq, x), col=&quot;blue&quot;)</code></pre><p><img src=https://jakespiteri.co.uk/portfolio/computing-1/report-9/index_files/figure-html/unnamed-chunk-7-1.png width=672 style=display:block;margin:auto></p><p>We have seen that one downside of Newton’s method is that it doesn’t differentiate between minima and maxima. It also requires the Hessian which can be difficult to produce for high-dimensional problems. The method presented above is known as <em>pure</em> Newton’s method, and there are variations which avoid these problems. To avoid finding local maxima we can ensure that the Hessian is positive (semi-) definite at each step, if it is not then we can modify it such that it is (e.g. we can add scalar multiple of the identity matrix to the Hessian such that it becomes positive definite, or we can take the eigendecomposition and modify the eigenvalues such that they are positive.). To avoid computing the Hessian we can use quasi-Newton methods which are similar to Newton’s method but they replace the exact Hessian with an approximation of the Hessian.</p><p>So far we have seen that one-dimensional unconstrained optimization is not as easy as we might expect. The problem becomes even more complex as the number of dimensions increase, and the computational complexity of our optimization algorithm becomes important.</p></div><div id=multi-dimensional-optimization class="section level1"><h1>Multi-dimensional optimization</h1><p>Most of the objective functions we need to optimize are multi-dimensional. There are three common categories of optimization algorithms:</p><ul><li>Simplex methods — only uses the value of the function.</li><li>First-order methods — uses the value of the function and the value of its first derivative. Gradient descent is a popular method within this category.</li><li>Second-order methods — uses the value of the function, its first derivative, and its second derivative (the Hessian). Newton methods fall under this category.</li></ul><p>The most popular method for multi-dimensional optimization in <code>R</code> is the <code>optim</code> function. <code>optim</code> implements a number of optimization algorithms such as Nelder-Mead, BFGS, CG (conjugate gradient), L-BFGS-B, SANN, and Brent. Another approach is to use the <code>nlm</code> (non-linear minimization) function, which uses a Newton-type algorithm.</p></div><div id=simplex-methods class="section level1"><h1>Simplex methods</h1><p>The Nelder-Mead algorithm is a well-known simplex method which is easily used in <code>R</code>.</p></div><div id=gradient-type-methods class="section level1"><h1>Gradient-type methods</h1><p>Gradient-type methods are much faster at converging than simplex methods, and avoid evaluating the Hessian. The most well-known method is steepest descent (gradient descent in the machine learning literature), in which we iteratively set
<span class="math display">\[x_{k+1} = x_k - \alpha \nabla f(x_k),\]</span>
where <span class="math inline">\(f(x)\)</span>, <span class="math inline">\(x \in \mathbb{R}^n\)</span>, is a differentiable and multivariate function, and <span class="math inline">\(\alpha\)</span> is our step size (also known as learning rate). We can perform a line-search to find the optimal value of <span class="math inline">\(\alpha\)</span> at each step. <span class="math inline">\(\alpha\)</span> is often chosen to be a small value such as <span class="math inline">\(0.03\)</span> but can be tuned to the specific problem. If our step size is too large the optimization algorithm will step beyond the optimal solution and could diverge.</p><p>Steepest descent converges slowly for high-dimensional problems but it has a very low computational cost. This is why steepest descent is the most widely-used optimization algorithm in the machine learning literature. When optimizing a neural network with hundreds of thousands of parameters, Newton’s method cannot be used due to its high computational cost (<span class="math inline">\(\mathcal{O}(n^3)\)</span>). The computational cost of quasi-Newton methods is lower, but still cannot scale to <em>such</em> high-dimensional problems.</p><p>Gradient descent is well-known for its tendency to zigzag. Given its frequent use in machine learning, many improvements have been proposed in the machine learning literature. We will look at some of these proposals, implement them, and observe their rate of convergence.</p><p>We will consider the function <span class="math inline">\(f(x, y) = x^2 + 10y^2\)</span>. Below is a contour plot of this function.</p><pre class=r><code># create contour plot
f &lt;- function(x, y) {x^2 + 10*y^2}
s &lt;- seq(-100,100,length=300)
contmat &lt;- outer(s,
                 s,
                 Vectorize(function(x,y) f(x,y)))
rownames(contmat) &lt;- colnames(contmat) &lt;- s # keep x and y values and not the index
contlong &lt;- melt(contmat) # melt into long format</code></pre><pre><code>## Warning in type.convert.default(X[[i]], ...): &#39;as.is&#39; should be specified by the
## caller; using TRUE

## Warning in type.convert.default(X[[i]], ...): &#39;as.is&#39; should be specified by the
## caller; using TRUE</code></pre><pre class=r><code>b &lt;- exp(seq(log(10), log(100000), length = 25))
contplot &lt;- contlong %&gt;%
  ggplot() +
  geom_contour(aes(x = X1, y = X2, z = value, colour = stat(level)), breaks = b) + 
  ggtitle(&quot;Contour plot of f(x,y)&quot;) +
  theme_minimal()
contplot</code></pre><p><img src=https://jakespiteri.co.uk/portfolio/computing-1/report-9/index_files/figure-html/unnamed-chunk-8-1.png width=672 style=display:block;margin:auto></p><p>Let’s implement a simple version of steepest descent and view its zigzagging nature. We see that when the state space varies much more in one dimension than another, the algorithm tends to zigzag.</p><pre class=r><code>graddesc &lt;- function(x0, fexp, iter, lr) {
  
  # get first derivative
  pars &lt;- paste(&quot;x&quot;, 1:length(x0), sep=&quot;&quot;)
  f1 &lt;- deriv(fexp, namevec = pars, function.arg = TRUE)
  
  # do gradient descent and keep all steps
  x_steps &lt;- matrix(NA, iter+1, length(x0))
  x_steps[1,] &lt;- x_gd &lt;- x0
  for (i in 1:iter) {
    deriv &lt;- attributes(do.call(f1, as.list(x_gd)))$gradient
    x_gd &lt;- x_gd - lr * deriv
    x_steps[i+1,] &lt;- c(x_gd)
  }
 
  return(x_steps)
}</code></pre><pre class=r><code>f &lt;- expression(x1^2 + 10*x2^2)
f_gd &lt;- as.data.frame(graddesc(c(-100,-50), f, 20, 0.09))
contplot + 
  geom_line(data = f_gd, mapping = aes(x = V1, y = V2), colour = &#39;red&#39;) +
  geom_point(data = f_gd, mapping = aes(x = V1, y = V2), shape = 21, colour = &#39;red&#39;) +
  labs(subtitle = &quot;using gradient descent&quot;)</code></pre><p><img src=https://jakespiteri.co.uk/portfolio/computing-1/report-9/index_files/figure-html/unnamed-chunk-10-1.png width=672 style=display:block;margin:auto></p><p>We see that for a non-optimal learning rate size (one that is too large) the gradient descent steps zigzag.</p><div id=gradient-descent-variants class="section level2"><h2>Gradient descent variants</h2><div id=gradient-descent-with-momentum class="section level3"><h3>Gradient descent with momentum</h3><p>This method replaces the gradient of the objective function in the update with an exponentially weighted average of past gradients. This has the effect of smoothing out the update and dampens potential zigzagging. The iterative updates are
<span class="math display">\[\begin{alignat*}{2}
x_{k+1} &= x_k - \alpha V_k \\
V_k &= \beta V_{k-1} + (1 - \beta)\nabla f(x_k),
\end{alignat*}\]</span>
where <span class="math inline">\(\beta\)</span> is a hyperparameter called the momentum, which ranges between 0 and 1. The larger the <span class="math inline">\(\beta\)</span>, the greater the smoothing effect. A popular choice for <span class="math inline">\(\beta\)</span> is 0.9.</p><p>Momentum helps accelerate gradient descent by dampening oscillations. We will implement gradient descent with momentum and see if there is an improvement. Note that when <span class="math inline">\(\beta = 0\)</span>, gradient descent with momentum is simply gradient descent.</p><pre class=r><code>graddescmomentum &lt;- function(x0, fexp, iter, lr, momentum) {
  
  # get first derivative
  pars &lt;- paste(&quot;x&quot;, 1:length(x0), sep=&quot;&quot;)
  f1 &lt;- deriv(fexp, namevec = pars, function.arg = TRUE)
  
  # do gradient descent and keep all steps
  x_steps &lt;- matrix(NA, iter+1, length(x0))
  x_steps[1,] &lt;- x_gd &lt;- x0
  V &lt;- rep(0,length(x0))
  for (i in 1:iter) {
    deriv &lt;- attributes(do.call(f1, as.list(x_gd)))$gradient
    V &lt;- momentum * V + (1 - momentum) * deriv
    x_gd &lt;- x_gd - lr * V
    x_steps[i+1,] &lt;- c(x_gd)
  }
 
  return(x_steps)
}

f &lt;- expression(x1^2 + 10*x2^2)
f_gd &lt;- as.data.frame(graddescmomentum(c(-100,-50), f, 20, 0.09, 0.5))
contplot + 
  geom_line(data = f_gd, mapping = aes(x = V1, y = V2), colour = &#39;red&#39;) +
  geom_point(data = f_gd, mapping = aes(x = V1, y = V2), shape = 21, colour = &#39;red&#39;) +
  labs(subtitle = &quot;using gradient descent with momentum&quot;)</code></pre><p><img src=https://jakespiteri.co.uk/portfolio/computing-1/report-9/index_files/figure-html/unnamed-chunk-11-1.png width=672 style=display:block;margin:auto></p></div><div id=rmsprop class="section level3"><h3>RMSprop</h3><p>When finding the minimum point of a function, we may want to take large steps towards the minimum when we are far from it, and smaller steps when we are close. In order to achieve this we can employ a line search method, or a learning rate scheduler which decreases the learning rate at specified intervals. There are also methods which employ <em>adaptive</em> gradients, such as RMSprop.</p><p>The key idea of RMSprop is to store an exponentially weighted average of the squared gradients for each weight. RMSprop scales the gradient updates by dividing the learning rate by the square root of an exponentially decaying average of squared gradients — hence the name root mean-squared propagation.</p><p>RMSprop has the update formula
<span class="math display">\[\begin{alignat*}{2}
x_{k+1} &= x_k - \frac{\alpha}{\sqrt{\nu_k + \epsilon}}\nabla f(x_{k}), \\
\nu_k &= \beta \nu_{k-1} + (1 - \beta) \nabla f(x_{k})^2,
\end{alignat*}\]</span>
where <span class="math inline">\(\beta\)</span> is the weighting of the moving average. A suggested default value for <span class="math inline">\(\beta\)</span> is <span class="math inline">\(0.9\)</span>.</p><pre class=r><code>rmsprop &lt;- function(x0, fexp, iter, lr, beta, eps = 1e-5) {
  
  # get first derivative
  pars &lt;- paste(&quot;x&quot;, 1:length(x0), sep=&quot;&quot;)
  f1 &lt;- deriv(fexp, namevec = pars, function.arg = TRUE)
  
  # do gradient descent and keep all steps
  x_steps &lt;- matrix(NA, iter+1, length(x0))
  x_steps[1,] &lt;- x_gd &lt;- x0
  ms &lt;- rep(0,length(x0))
  for (i in 1:iter) {
    deriv &lt;- attributes(do.call(f1, as.list(x_gd)))$gradient
    ms &lt;- beta * ms + (1 - beta) * deriv^2
    x_gd &lt;- x_gd - (lr / sqrt(ms + eps)) * deriv
    x_steps[i+1,] &lt;- c(x_gd)
  }
 
  return(x_steps)
}

f &lt;- expression(x1^2 + 10*x2^2)
f_gd &lt;- as.data.frame(rmsprop(c(-100,-50), f, 120, 1, 0.9))
contplot + 
  geom_line(data = f_gd, mapping = aes(x = V1, y = V2), colour = &#39;red&#39;) +
  geom_point(data = f_gd, mapping = aes(x = V1, y = V2), shape = 21, colour = &#39;red&#39;) +
  labs(subtitle = &quot;using RMSprop&quot;)</code></pre><p><img src=https://jakespiteri.co.uk/portfolio/computing-1/report-9/index_files/figure-html/unnamed-chunk-12-1.png width=672 style=display:block;margin:auto></p></div><div id=adam class="section level3"><h3>Adam</h3><p>Adaptive Moment Estimation (Adam) also implements adaptive learning rates. Similar to RMSprop above, Adam stores an exponentially decaying average of squared gradients, and it also stores an exponentially decaying average of gradients similar to momentum.</p><p>Adams update formula is
<span class="math display">\[\begin{alignat*}{2}
x_{k+1} &= x_k - \frac{\alpha}{\sqrt{\widehat{\nu}_k} + \epsilon}\widehat{m}_k, \\
m_k &= \beta_1 m_{k-1} + (1 - \beta_1) \nabla f(x_{k}),\\
\nu_k &= \beta_2 \nu_{k-1} + (1 - \beta_2) \nabla f(x_{k})^2,\\
\widehat{m}_k &= \frac{m_k}{1 - \beta_1^k},\\
\widehat{\nu}_k &= \frac{\nu_k}{1 - \beta_2^k}.
\end{alignat*}\]</span></p><p>In the above, <span class="math inline">\(\beta_1\)</span> and <span class="math inline">\(\beta_2\)</span> are hyperparameters to be tuned. The original creaters of Adam propose default values of <span class="math inline">\(0.9\)</span> for <span class="math inline">\(\beta_1\)</span>, <span class="math inline">\(0.999\)</span> for <span class="math inline">\(\beta_2\)</span>, and <span class="math inline">\(10^{-8}\)</span> for <span class="math inline">\(\epsilon\)</span>.</p><p>We can think of <span class="math inline">\(m_k\)</span> and <span class="math inline">\(\nu_k\)</span> as the first moment (the mean) and the second moment (the uncentered variance) respectively, hence the name adaptive moment estimation. As <span class="math inline">\(m\)</span> and <span class="math inline">\(\nu\)</span> are often initialized as (vectors of) zeros, they tend to be biased towards zero in the first few iterations. To avoid this bias towards zero we use <span class="math inline">\(\widehat{m}\)</span> and <span class="math inline">\(\widehat{\nu}\)</span> as specified above.</p><pre class=r><code>adam &lt;- function(x0, fexp, iter, lr, beta1, beta2, eps = 1e-8) {
  
  # get first derivative
  pars &lt;- paste(&quot;x&quot;, 1:length(x0), sep=&quot;&quot;)
  f1 &lt;- deriv(fexp, namevec = pars, function.arg = TRUE)
  
  # do gradient descent and keep all steps
  x_steps &lt;- matrix(NA, iter+1, length(x0))
  x_steps[1,] &lt;- x_gd &lt;- x0
  V &lt;- S &lt;- rep(0,length(x0))
  for (i in 1:iter) {
    deriv &lt;- attributes(do.call(f1, as.list(x_gd)))$gradient
    V &lt;- beta1 * V + (1-beta1) * deriv
    S &lt;- beta2 * S + (1-beta2) * deriv^2
    Vhat &lt;- V/(1-beta1^i)
    Shat &lt;- S/(1-beta2^i)
    x_gd &lt;- x_gd - lr / (sqrt(Shat) + eps) * Vhat
    x_steps[i+1,] &lt;- c(x_gd)
  }
 
  return(x_steps)
}

f &lt;- expression(x1^2 + 10*x2^2)
f_gd &lt;- as.data.frame(adam(c(-100,-50), f, 100, 2, 0.9, 0.999))
contplot + 
  geom_line(data = f_gd, mapping = aes(x = V1, y = V2), colour = &#39;red&#39;) +
  geom_point(data = f_gd, mapping = aes(x = V1, y = V2), shape = 21, 
             colour = &#39;red&#39;, fill=&quot;white&quot;) +
  labs(subtitle = &quot;using adam&quot;)</code></pre><p><img src=https://jakespiteri.co.uk/portfolio/computing-1/report-9/index_files/figure-html/unnamed-chunk-13-1.png width=672 style=display:block;margin:auto></p></div></div></div><div id=second-order-methods class="section level1"><h1>Second-order methods</h1><p>Second-order methods converge much faster than first-order methods, but they also require more computation. We have already explained Newton’s method which is arguably the most widely-used second-order method, and so we will provide a brief summary here.</p><p>Newton’s method minimizes the quadratic approximation of <span class="math inline">\(f(x)\)</span> around <span class="math inline">\(x_k\)</span>:
<span class="math display">\[Q(x) = f(x_k) + \nabla f(x_k)^T (x - x_k) + \frac{1}{2} (x - x_k)^T \nabla^2 f(x_k) (x - x_k)\]</span>
I.e. <span class="math inline">\(x_{k+1} = \text{argmin}_x Q(x)\)</span>. To find the minimum point we simply differentiate with respect to x and set equal to zero. This gives the update formula
<span class="math display">\[x_{k+1} = x_k - (\nabla^2 f(x_k))^{-1} \nabla f(x_k).\]</span>
We often include a step size parameter <span class="math inline">\(\alpha\)</span> to control how large our steps are.</p><ul><li><p>Newton’s method converges quadratically and has computational cost <span class="math inline">\(\mathcal{O}(n^3)\)</span> for <span class="math inline">\(x \in \mathbb{R}^n\)</span></p></li><li><p>Pure Newton’s method may diverge if our initialization is bad. To avoid this we must ensure that our Hessian is positive (semi-) definite at each step, and ensure our step size is chosen such that the function decreases.</p></li><li><p>Computing the Hessian may be very difficult even if we use automatic differentiation. Storing the Hessian also becomes a problem for very high-dimensional problems.</p></li></ul><p>Below we optimize <span class="math inline">\(f(x, y) = x^2 + 10y^2\)</span> using Newton’s method. It converges in just <span class="math inline">\(1\)</span> step! This is because the function is quadratic in two variables and thus the quadratic approximation to the true function computed by Newton’s method is exact. Newton’s method directly minimizes the true function in this case.</p><pre class=r><code>newton &lt;- function(x0, fexp, iter, lr) {
  
  # get first and second derivative
  pars &lt;- paste(&quot;x&quot;, 1:length(x0), sep=&quot;&quot;)
  d &lt;- deriv(fexp, namevec = pars, function.arg = TRUE, hessian = TRUE)
  
  # do gradient descent and keep all steps
  x_steps &lt;- matrix(NA, iter+1, length(x0))
  x_steps[1,] &lt;- x_n &lt;- x0
  ms &lt;- rep(0,length(x0))
  for (i in 1:iter) {
    deriv &lt;- attributes(do.call(d, as.list(x_n)))
    deriv_1 &lt;- t(deriv$gradient)
    deriv_2 &lt;- deriv$hessian[,,1:length(x0)]
    x_n &lt;- x_n - lr*solve(deriv_2)%*%deriv_1
    x_steps[i+1,] &lt;- c(x_n)
  }
 
  return(x_steps)
}

f &lt;- expression(x1^2 + 10*x2^2)
f_n &lt;- as.data.frame(newton(c(-100,-50), f, 5, 1))
contplot + 
  geom_line(data = f_n, mapping = aes(x = V1, y = V2), colour = &#39;red&#39;) +
  geom_point(data = f_n, mapping = aes(x = V1, y = V2), shape = 21, 
             colour = &#39;red&#39;, fill=&quot;white&quot;) +
  labs(subtitle = &quot;using Newton&#39;s method&quot;)</code></pre><p><img src=https://jakespiteri.co.uk/portfolio/computing-1/report-9/index_files/figure-html/unnamed-chunk-14-1.png width=672 style=display:block;margin:auto></p><div id=quasi-newton-methods class="section level2"><h2>Quasi-Newton methods</h2><p>Quasi-Newton methods are great substitutes for second-order methods. They approximate the Hessian at each iteration, rather than evaluate the true Hessian. This avoids the problem of computing the true Hessian for high-dimensional objective functions. Quasi-Newton methods are not as fast as Newton’s methods — they attain <em>superlinear</em> convergence. However, they also have a lower computational cost! They have complexity <span class="math inline">\(\mathcal{O}(n^2)\)</span> whilst Newton’s method carries complexity <span class="math inline">\(\mathcal{O}(n^3)\)</span>.</p><p>BFGS is arguably the best quasi-Newton method which has been proposed thus far. It has update formula <span class="math inline">\(x_{k+1} = x_k - \alpha_k B_k \nabla f(x_k)\)</span>, where <span class="math inline">\(B_k\)</span> is an <span class="math inline">\(n \times n\)</span> symmetric positive definite matrix that is updated at each iteration and <span class="math inline">\(\alpha_k\)</span> is our step size.</p><p>If we have a objective function with <span class="math inline">\(1000\)</span> variables, the Hessian and its approximation is a <span class="math inline">\(1000 \times 1000\)</span> dimension matrix. This is a large matrix to keep in memory and thus a limited-memory version of BFGS is often used (L-BFGS).</p><p>Let’s derive the BFGS update formula and see how it works.
BFGS also works by minimizing the quadratic approximation of the true function around <span class="math inline">\(x_k\)</span>. Recall the Taylor series approximation of <span class="math inline">\(f(x_k + d)\)</span> around <span class="math inline">\(x_k\)</span>.
<span class="math display">\[f(x_k + d) = f(x_k) + \nabla f(x_k)^T d + \frac{1}{2} d^T \nabla^2 f(x_k) d + o(\|d\|)^2\]</span>
We shall define the approximation of the objective function about <span class="math inline">\(x_k\)</span> as
<span class="math display">\[m_k(d) := f(x_k) + \nabla f(x_k)^Td + \frac{1}{2}d^T B_k d,\]</span>
where <span class="math inline">\(B_k\)</span> is an <span class="math inline">\(n \times n\)</span> symmetric positive definite matrix which we will update. It is our approximation of the Hessian.</p><p>The minimizer of this convex quadratic <span class="math inline">\(m_k(d)\)</span> is <span class="math inline">\(d = B_k^{-1} \nabla f(x_k)\)</span>. Hence we have the BFGS update formula <span class="math inline">\(x_{k+1} = x_k - \alpha_k B_k^{-1} \nabla f(x_k)\)</span>, where <span class="math inline">\(\alpha_k\)</span> is our step size. The question now is: How do we compute <span class="math inline">\(B_k\)</span>?</p><p>Instead of recomputing <span class="math inline">\(B_k\)</span> at each iteration, we will update it based on information about the curvature gained in the most recent step. Suppose we observe a new iterate <span class="math inline">\(x_{k+1}\)</span>, then we form the quadratic approximation
<span class="math inline">\(m_{k+1}(d) = f(x_{k+1}) + \nabla f(x_{k+1})^Td + \frac{1}{2}d^T B_{k+1} d\)</span>
In order to determine <span class="math inline">\(B_{k+1}\)</span> we place restrictions on our approximation <span class="math inline">\(m_{k+1}(d)\)</span>. We want <span class="math inline">\(m_{k+1}(d)\)</span> to be a good approximation to the objective function at <span class="math inline">\(x_{k+1}\)</span>. We propose that a good approximation will have the same gradient as the true function <span class="math inline">\(f\)</span> at the two most recent iterates. That is, we require</p><ul><li><p><span class="math inline">\(\nabla m_{k+1}(0) = \nabla f(x_{k+1}),\)</span></p></li><li><p><span class="math inline">\(\nabla m_{k+1}(- \alpha_k d_k) = \nabla f(x_{k+1}) - \alpha_k B_{k+1} d_k = \nabla f(x_k).\)</span></p></li></ul><p>This first requirement is always true. The second requirement can be rewritten as
<span class="math display">\[\alpha_k B_{k+1} d_k = \nabla f(x_{k+1}) - \nabla f(x_{k}).\]</span>
Using the notation <span class="math inline">\(y_k = \nabla f(x_{k+1}) - \nabla f(x_{k})\)</span>, and <span class="math inline">\(s_k = x_{k+1} - x_k\)</span> <span class="math inline">\((= \alpha_k d)\)</span>, we can rewrite the above equation as the <em>secant</em> equation
<span class="math display">\[B_{k+1}s_k = y_k.\]</span>
Note that if we approximate the Hessian we will then need to find its inverse to update <span class="math inline">\(x\)</span>. To avoid the expensive inversion, BFGS directly approximates the inverse of the Hessian which we will denote <span class="math inline">\(H\)</span>. Thus, we rewrite the secant equation
<span class="math display">\[ H_{k+1} y_k = s_k.\]</span>
This means that we require the symmetric positive definite matrix <span class="math inline">\(H_{k+1}\)</span> to map <span class="math inline">\(y_k\)</span> to <span class="math inline">\(s_k\)</span>. This is only possible if <span class="math inline">\(s_k\)</span> and <span class="math inline">\(y_k\)</span> satisfy the curvature condition: <span class="math inline">\(y_k^T s_k > 0\)</span>. If <span class="math inline">\(f\)</span> is strongly convex then this is always true for any <span class="math inline">\(x_k\)</span> and <span class="math inline">\(x_{k+1}\)</span>. If <span class="math inline">\(f\)</span> is non-convex then we must impose this restriction explicitly. We can do this by imposing the (strong) Wolfe conditions on the line search for our step size.</p><p>The secant equations place <span class="math inline">\(n\)</span> restrictions on <span class="math inline">\(H_{k+1}\)</span>, but we know that <span class="math inline">\(H_{k+1}\)</span> is symmetric positive definite and thus has <span class="math inline">\(n(n+1)/2\)</span> free parameters. Hence <span class="math inline">\(H_{k+1}\)</span> has infinitely many solutions. In order to obtain a unique solution we must impose a further restriction. We require that among all symmetric matrices that satisfy the secant equation, <span class="math inline">\(H_{k+1}\)</span> is in some sense closest to <span class="math inline">\(H_k\)</span>. We form the optimization problem
<span class="math display">\[\begin{alignat*}{2}
&\text{minimize }_{H} \quad &&\| H - H_k \|, \\
&\text{subject to } && H = H^T, \\
& && H y_k = s_k,
\end{alignat*}\]</span>
where <span class="math inline">\(s_k\)</span> and <span class="math inline">\(y_k\)</span> satisfy the curvature condition and <span class="math inline">\(H_k\)</span> is symmetric and positive definite.</p><p>Different matrix norms result in different quasi-Newton methods. A norm which provides an easy solution to the optimization problem which is also scale-invariant is the weighted Frobenius norm
<span class="math display">\[\|A\|_W = \|W^{1/2}AW^{1/2}\|_F,\]</span>
where <span class="math inline">\(\|\cdot\|_F\)</span> is defined by <span class="math inline">\(\|A\|_F = \sum_{i=1}^n\sum_{j=1}^n a_{ij}^2\)</span>.</p><p>The matrix W can be chosen to be any matrix satisfying <span class="math inline">\(Ws_k = y_k\)</span>. One possible <span class="math inline">\(W\)</span> which we know gives a solution is the inverse of the average Hessian. That is,
<span class="math display">\[W = \bar{G}_k^{-1}, \quad \text{where } \bar{G}_k = \int_0^1 \nabla^2 f(x_k + t \alpha_k d_k) dt.\]</span></p><p>The choices above: the weighted Frobenius norm and <span class="math inline">\(W\)</span> the inverse of the average Hessian provide the following unique solution to the optimization problem</p><p><span class="math display">\[H_{k+1} = (I - \rho_k s_k y_k^T) H_k (I - \rho y_k s_k^T) + \rho_k s_k s_k^T,\]</span>
where <span class="math inline">\(\rho_k = \frac{1}{y_k^T s_k}\)</span>.</p><p>Given this update formula for <span class="math inline">\(H_{k+1}\)</span>, the resulting BFGS algorithm is quite simple!</p><p>Below we continue to optimize the function <span class="math inline">\(f(x, y) = x^2 + 10y^2\)</span>. We use BFGS and see that it converges in only 3 steps! Clearly it is far superior to first-order methods when the objective function has a <em>reasonable</em> number of parameters. This is of course not an easy number to quantify, but well-defined objective functions with a large number of parameters can be optimized with BFGS.</p><pre class=r><code>f &lt;- function(x) {(x[1]^2 + 10*x[2]^2)}
f_bfgs &lt;- c(-100,-50)
for(i in 1:10) {
  f_bfgs &lt;- rbind(f_bfgs, optim(c(-100,-50), f, 
                                method = &quot;BFGS&quot;, control=list(maxit=i))$par)
}
f_bfgs &lt;- as.data.frame(f_bfgs, row.names=1:11)
contplot + 
  geom_path(data = f_bfgs, mapping = aes(x = V1, y = V2), colour = &#39;red&#39;) +
  geom_point(data = f_bfgs, mapping = aes(x = V1, y = V2), shape = 21, 
             colour = &#39;red&#39;, fill=&quot;white&quot;) +
  labs(subtitle = &quot;using BFGS&quot;)</code></pre><p><img src=https://jakespiteri.co.uk/portfolio/computing-1/report-9/index_files/figure-html/unnamed-chunk-15-1.png width=672 style=display:block;margin:auto></p></div></div></div><footer class=post-footer><ul class=post-tags></ul><nav class=paginav><a class=prev href=https://jakespiteri.co.uk/portfolio/computing-1/report-8/><span class=title>« Prev</span><br><span>Portfolio Report 8: Matrices</span></a>
<a class=next href=https://jakespiteri.co.uk/portfolio/computing-1/report-10/><span class=title>Next »</span><br><span>Portfolio Report 10: Integration and Markov Chain Monte Carlo</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2022 <a href=https://jakespiteri.co.uk/>Jake Spiteri</a></span>
<span></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>