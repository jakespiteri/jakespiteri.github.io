<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Portfolio Report 4: Vectorization and parallel computing | Jake Spiteri</title><meta name=keywords content><meta name=description content="Vectorization Vectorization is the process of replacing the use of loops in code, with a single operation applied elementwise to the vector. In vectorizing code, the multiple instructions running the same operation in a for loop are replaced by a single instruction which applies the operation to multiple elements in a vector.
Vectorization is much faster than using for loops in interpreted languages such as R, Matlab, and Python. To see this in practice, we will look at some examples."><meta name=author content="Jake Spiteri"><link rel=canonical href=https://jakespiteri.co.uk/portfolio/computing-1/report-4/><meta name=google-site-verification content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.5cfc680b1eeaeef9efbced92d46c2a9e876b72ee14fba85846afc4cff9e6e6f8.css integrity="sha256-XPxoCx7q7vnvvO2S1Gwqnodrcu4U+6hYRq/Ez/nm5vg=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://jakespiteri.co.uk/favi.png><link rel=icon type=image/png sizes=16x16 href=https://jakespiteri.co.uk/favi.png><link rel=icon type=image/png sizes=32x32 href=https://jakespiteri.co.uk/favi.png><link rel=apple-touch-icon href=https://jakespiteri.co.uk/favi.png><link rel=mask-icon href=https://jakespiteri.co.uk/favi.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-123-45","auto"),ga("send","pageview"))</script><meta property="og:title" content="Portfolio Report 4: Vectorization and parallel computing"><meta property="og:description" content="Vectorization Vectorization is the process of replacing the use of loops in code, with a single operation applied elementwise to the vector. In vectorizing code, the multiple instructions running the same operation in a for loop are replaced by a single instruction which applies the operation to multiple elements in a vector.
Vectorization is much faster than using for loops in interpreted languages such as R, Matlab, and Python. To see this in practice, we will look at some examples."><meta property="og:type" content="article"><meta property="og:url" content="https://jakespiteri.co.uk/portfolio/computing-1/report-4/"><meta property="og:image" content="https://jakespiteri.co.uk/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="portfolio"><meta property="og:site_name" content="Jake Spiteri"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://jakespiteri.co.uk/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Portfolio Report 4: Vectorization and parallel computing"><meta name=twitter:description content="Vectorization Vectorization is the process of replacing the use of loops in code, with a single operation applied elementwise to the vector. In vectorizing code, the multiple instructions running the same operation in a for loop are replaced by a single instruction which applies the operation to multiple elements in a vector.
Vectorization is much faster than using for loops in interpreted languages such as R, Matlab, and Python. To see this in practice, we will look at some examples."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Portfolio","item":"https://jakespiteri.co.uk/portfolio/"},{"@type":"ListItem","position":2,"name":"Statistical Computing 1","item":"https://jakespiteri.co.uk/portfolio/computing-1/"},{"@type":"ListItem","position":3,"name":"Portfolio Report 4: Vectorization and parallel computing","item":"https://jakespiteri.co.uk/portfolio/computing-1/report-4/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Portfolio Report 4: Vectorization and parallel computing","name":"Portfolio Report 4: Vectorization and parallel computing","description":"Vectorization Vectorization is the process of replacing the use of loops in code, with a single operation applied elementwise to the vector. In vectorizing code, the multiple instructions running the same operation in a for loop are replaced by a single instruction which applies the operation to multiple elements in a vector.\nVectorization is much faster than using for loops in interpreted languages such as R, Matlab, and Python. To see this in practice, we will look at some examples.","keywords":[],"articleBody":" Vectorization Vectorization is the process of replacing the use of loops in code, with a single operation applied elementwise to the vector. In vectorizing code, the multiple instructions running the same operation in a for loop are replaced by a single instruction which applies the operation to multiple elements in a vector.\nVectorization is much faster than using for loops in interpreted languages such as R, Matlab, and Python. To see this in practice, we will look at some examples.\nWe consider summing the cosine of each component of a vector.\n# set the seed for reproducibility set.seed(123) # uses a for loop add_cos1 \u003c- function(x) { sum \u003c- 0 for (i in 1:length(x)) { sum \u003c- sum + cos(x[i]) } return(sum) } # uses vectorization add_cos2 \u003c- function(x) sum(cos(x)) x = 1:1e7 t1 \u003c- system.time(add_cos1(x)) t2 \u003c- system.time(add_cos2(x)) t1 ## user system elapsed ## 1.003 0.000 1.007 t2 ## user system elapsed ## 0.364 0.075 0.440 At the time of running, this provided a speedup of 2.3 times. We also note that the vectorized code is also easier to read.\nHigh-dimensional vectorization The example above shows that for loops are quite inefficient. It is now easy to see that nested for loops are very inefficient, and should be avoided.\nThere are many reasons we may want to use nested for loops. We may want to run a simulation \\(n_{sim}\\) times, where each simulation consists of \\(n\\) operations.\nAs an example, we consider the average value produced by tossing a fair six-sided die \\(n=500\\) times. We expect our data to center around \\(\\frac{1+2+3+4+5+6}{6}=3.5\\). We simulate the experiment \\(n_{sim} = 1000\\) times.\ntoss_die1 \u003c- function(nsim, n) { average \u003c- numeric(nsim) for (i in 1:nsim) { counts \u003c- numeric(n) for (j in 1:n) { counts[j] \u003c- sample(1:6, 1) } average[i] \u003c- mean(counts) } return(average) } toss_die2 \u003c- function(nsim, n) { tosses \u003c- matrix(sample(1:6, n*nsim, replace=TRUE), nrow=nsim, ncol=n) return(rowSums(tosses)/n) } We now test the runtime of the nested for loop version, and the vectorized version.\nnsim \u003c- 1000; n \u003c- 500 t1 \u003c- system.time(toss_die1(nsim, n)); t1 ## user system elapsed ## 5.641 0.025 5.666 t2 \u003c- system.time(toss_die2(nsim, n)); t2 ## user system elapsed ## 0.036 0.007 0.044 We see that nested for loops are very slow and should be avoided when possible. In this case the vectorized code provides a speedup of 129 times! We will use the vectorized function in the following.\nnsim \u003c- 10000; n \u003c- 500 average_value \u003c- toss_die2(nsim,n) hist(average_value, breaks = 30) Let’s look at a demonstration of Law of Large Numbers. We plot the cumulative mean as the number of throws increases.\nnsim \u003c- 3000; n \u003c- 1 plot(1:nsim, cumsum(toss_die2(nsim, n))/1:nsim, type = 'l', main='Average dice roll', xlab = 'Number of rolls', ylab='Cumulative mean') abline(h=3.5, col='red', lty = 2) The apply family The apply family consists of apply(), lapply(), sapply(), and mapply(). These functions apply a specified function to elements in a data structure. These functions can be viewed as alternatives to for loops; they are often slower but may simplify and enhance the readability of code.\napply(X, MARGIN, FUN, ...) — This function applies a function to a vector or matrix. The MARGIN argument determines whether the function is applied over rows or columns, or both.\nlapply(X, FUN, ...) — This function is similar to apply but it applies a function over a vector or list and returns a list.\nsapply(X, FUN, ...) — This is a wrapper function of lapply which simplifies the output by returning a vector, matrix, or an array when specified using simplify = \"array\".\nmapply(FUN, ...) — This is a multivariate version of sapply. mapply applies the function FUN to the first elements of each subsequent ... argument.\nExamples Below are some examples using the apply family of functions.\n## apply mat \u003c- matrix(rep(1, 12), 3, 4) # 3x4 matrix of 1s # apply sum to each row --- i.e. sum over columns apply(mat, 1, sum) ## [1] 4 4 4 # apply sum to each column --- i.e. sum over rows apply(mat, 2, sum) ## [1] 3 3 3 3 # apply sum to each element --- this does not do anything if FUN = sum apply(mat, c(1,2), sum) ## [,1] [,2] [,3] [,4] ## [1,] 1 1 1 1 ## [2,] 1 1 1 1 ## [3,] 1 1 1 1 # apply our own function to each element apply(mat, c(1,2), function(x) 2*x) ## [,1] [,2] [,3] [,4] ## [1,] 2 2 2 2 ## [2,] 2 2 2 2 ## [3,] 2 2 2 2 ## lapply vec \u003c- rep(1,3) lis \u003c- list(seq = 1:4, rep = rep(1,4)) # get cosine of each element in a vector lapply(vec, cos) ## [[1]] ## [1] 0.5403023 ## ## [[2]] ## [1] 0.5403023 ## ## [[3]] ## [1] 0.5403023 # apply cosine to list lapply(lis, cos) ## $seq ## [1] 0.5403023 -0.4161468 -0.9899925 -0.6536436 ## ## $rep ## [1] 0.5403023 0.5403023 0.5403023 0.5403023 ## sapply # get cosine of each element in a vector sapply(vec, cos) ## [1] 0.5403023 0.5403023 0.5403023 # apply cosine to list sapply(lis, cos) ## seq rep ## [1,] 0.5403023 0.5403023 ## [2,] -0.4161468 0.5403023 ## [3,] -0.9899925 0.5403023 ## [4,] -0.6536436 0.5403023 ## mapply # sum two vectors elementwise mapply('+', 1:3, 4:6) ## [1] 5 7 9 # seq(1, 5), seq(2, 5), ... mapply(seq, 1:5, 5) ## [[1]] ## [1] 1 2 3 4 5 ## ## [[2]] ## [1] 2 3 4 5 ## ## [[3]] ## [1] 3 4 5 ## ## [[4]] ## [1] 4 5 ## ## [[5]] ## [1] 5 Map, Reduce, and Filter These functions can use lambda expressions similar to the above apply family. These are short functions which are sometimes referred to as anonymous functions, as they are not assigned to a variable. Lambda expressions allow us to produce quite complicated functionality in one line.\nBelow is a quick summary of the functions introduced in this section:\nMap — This function maps a function to every element in a vector.\nReduce — This function performs a function on pairs of elements in a vector, iteratively, and returns a single number.\nFilter — Tests every element in a vector and only returns those that satisfy the condition.\nExamples Below are some examples demonstrating the above functions.\nvec \u003c- 1:3 # Map cosine to every element in a vector Map(cos, vec) ## [[1]] ## [1] 0.5403023 ## ## [[2]] ## [1] -0.4161468 ## ## [[3]] ## [1] -0.9899925 # Reduce Reduce(function(x, y) x + 2*y, vec) ## [1] 11 # Filter Filter(function(x) x %% 3 == 0, vec) ## [1] 3 Parallel computing Parallelization is the process of using multiple CPU cores at the same time, to solve different parts of a problem. Often when working on large computational problems, we will encounter a bottleneck. We may encounter a\nCPU-bound: Our computations take too much CPU time. Memory-bound: Our computations use too much memory. I/O bound: It takes too long to read/write from disk. Parallel computing helps us overcome CPU bounds by working with more than one core at a time. This means that if our computer has four physical cores, then we can distribute our computation across four cores. In theory, this would speed up our computation by four times but it is not the case in practice due to the other bounds described above. Part of being a good R programmer is being able to write fast and efficient code.\nParallelize using mclapply mclapply is a multi-core version of lapply.\nlibrary(parallel) library(doParallel) ## Loading required package: foreach ## Loading required package: iterators # detect cores detectCores() ## [1] 8 # demonstrate that distributing over 4 cores does not provide speed up of 4 times system.time(lapply(1:8, function(x) Sys.sleep(0.5))) # 4 seconds ## user system elapsed ## 0.003 0.000 4.007 system.time(mclapply(1:8, function(x) Sys.sleep(0.5), mc.cores=4)) # expect 1 second ## user system elapsed ## 0.009 0.033 1.025 Parallelize using foreach foreach seems a little like a for loop. However foreach uses binary operators %do% and %dopar%, and also returns a list as output. It’s useful to use foreach with %do% to evaluate an expression sequentially and store all outputs in a data struture — by default this is a list but the behavior can be changed using the argument .combine.\nlibrary(foreach) # run sequentially using %do% foreach(i=1:3) %do% {i*i} ## [[1]] ## [1] 1 ## ## [[2]] ## [1] 4 ## ## [[3]] ## [1] 9 # change output to vector foreach(i=1:3, .combine=\"c\") %do% {i*i} ## [1] 1 4 9 The above functions use the operator %do% and therefore run sequentially. If we simply replace the operator with %dopar% R will run the computations in parallel. The above computations are so simple that it’s probably not worth implementing them in parallel. Let’s look at a little example using Markov chain Monte Carlo, in which we may want to run multiple chains to test for pseudo-convergence. All of the parallelization is in the simulate.chains function. Note that in order to run the computations in parallel we must register the parallel backend using registerDoParallel() before foreach. When we are finished we should run stopImplicitCluster().\n# returns the transition kernel P(x) make.metropolis.hastings.kernel \u003c- function(pi, Q) { q \u003c- Q$density P \u003c- function(x) { z \u003c- Q$sample(x) alpha \u003c- min(1, pi(z) * q(z, x) / pi(x) / q(x, z)) if(runif(1) \u003c alpha) { return(z) } else { return(x) } } return(P) } make.normal.proposal \u003c- function(sigma) { Q \u003c- list() Q$sample \u003c- function(x) { return(rnorm(length(x), x, sigma)) } Q$density \u003c- function(x, z) { return(dnorm(z, x, sigma)) } return(Q) } simulate.chains \u003c- function(P, x0, n, n.chains, n.cores = 4) { # parallelize chains registerDoParallel(n.cores) simulations \u003c- foreach(i=1:n.chains, .combine=\"rbind\") %dopar% { x \u003c- x0[i] foreach(i=1:n, .combine=\"c\") %do% {P(x)} } stopImplicitCluster() return(simulations) } Below we sample from a mixture distribution: The weighted mixture of two normal distributions \\({Z \\sim 0.6X + 0.4Y}\\), where \\(X \\sim N(-2,1)\\), \\(Y \\sim N(5, 2^2)\\). We use a \\(N(0,1)\\) proposal distribution. We will run \\(4\\) chains in parallel and test for pseudo-convergence by looking at the trace of the chains. Each chain is initialized using a \\(U[-10,10]\\) distribution.\n# generate markov chain using MH algorithm # target: mixture of two normals n \u003c- 5e3; set.seed(123) target \u003c- function(x) {0.6*dnorm(x, -2, 1) + 0.4*dnorm(x, 5, 2)} xs \u003c- simulate.chains(make.metropolis.hastings.kernel(target, make.normal.proposal(1)), runif(4,-10,10), n, 4) # setup plot space par(mfrow=c(2,2)) # plot the chains ylimits \u003c- c(min(xs), max(xs)) for(i in 1:4){ plot(1:n, xs[i,], type = 'l', xlab=\"iteration\", ylim=ylimits, ylab=\"x\", main=paste0(\"Trace of Markov chain \", i)) } We see that the traces do not converge to the same distribution! This tells us that the chains are not fully exploring the state space and thus we must adjust our proposal distribution.\n","wordCount":"1781","inLanguage":"en","datePublished":"0001-01-01T00:00:00Z","dateModified":"0001-01-01T00:00:00Z","author":{"@type":"Person","name":"Jake Spiteri"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://jakespiteri.co.uk/portfolio/computing-1/report-4/"},"publisher":{"@type":"Organization","name":"Jake Spiteri","logo":{"@type":"ImageObject","url":"https://jakespiteri.co.uk/favi.png"}}}</script></head><body id=top><header class=header><nav class=nav><div class=logo><a href=https://jakespiteri.co.uk/ accesskey=h title="Home (Alt + H)"><img src=https://jakespiteri.co.uk/apple-touch-icon.png alt aria-label=logo height=35>Home</a><div class=logo-switches></div></div><ul id=menu><li><a href=https://jakespiteri.co.uk/about/ title=/about><span>/about</span></a></li><li><a href=https://jakespiteri.co.uk/portfolio/ title=/portfolio><span>/portfolio</span></a></li><li><a href=https://jakespiteri.co.uk/blog/ title=/blog><span>/blog</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://jakespiteri.co.uk/>Home</a>&nbsp;»&nbsp;<a href=https://jakespiteri.co.uk/portfolio/>Portfolio</a>&nbsp;»&nbsp;<a href=https://jakespiteri.co.uk/portfolio/computing-1/>Statistical Computing 1</a></div><h1 class=post-title>Portfolio Report 4: Vectorization and parallel computing</h1><div class=post-meta>Jake Spiteri&nbsp;|&nbsp;<a href=https://github.com/jakespiteri/jakespiteri.github.io/tree/master/portfolio/computing-1/Report-4/index.html rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><div class=post-content><div id=vectorization class="section level1"><h1>Vectorization</h1><p>Vectorization is the process of replacing the use of loops in code, with a single operation applied elementwise to the vector. In vectorizing code, the multiple instructions running the same operation in a for loop are replaced by a single instruction which applies the operation to multiple elements in a vector.</p><p>Vectorization is much faster than using for loops in interpreted languages such as R, Matlab, and Python. To see this in practice, we will look at some examples.</p><p>We consider summing the cosine of each component of a vector.</p><pre class=r><code># set the seed for reproducibility
set.seed(123)

# uses a for loop
add_cos1 &lt;- function(x) {
  sum &lt;- 0
  for (i in 1:length(x)) {
    sum &lt;- sum + cos(x[i])
  }
  return(sum)
}

# uses vectorization
add_cos2 &lt;- function(x) sum(cos(x))

x = 1:1e7
t1 &lt;- system.time(add_cos1(x))
t2 &lt;- system.time(add_cos2(x))</code></pre><pre class=r><code>t1 </code></pre><pre><code>##    user  system elapsed 
##   1.003   0.000   1.007</code></pre><pre class=r><code>t2</code></pre><pre><code>##    user  system elapsed 
##   0.364   0.075   0.440</code></pre><p>At the time of running, this provided a speedup of 2.3 times. We also note that the vectorized code is also easier to read.</p><div id=high-dimensional-vectorization class="section level2"><h2>High-dimensional vectorization</h2><p>The example above shows that for loops are quite inefficient. It is now easy to see that nested for loops are very inefficient, and should be avoided.</p><p>There are many reasons we may want to use nested for loops. We may want to run a simulation <span class="math inline">\(n_{sim}\)</span> times, where each simulation consists of <span class="math inline">\(n\)</span> operations.</p><p>As an example, we consider the average value produced by tossing a fair six-sided die <span class="math inline">\(n=500\)</span> times. We expect our data to center around <span class="math inline">\(\frac{1+2+3+4+5+6}{6}=3.5\)</span>. We simulate the experiment <span class="math inline">\(n_{sim} = 1000\)</span> times.</p><pre class=r><code>toss_die1 &lt;- function(nsim, n) {
  average &lt;- numeric(nsim)
  for (i in 1:nsim) {
    counts &lt;- numeric(n)
    for (j in 1:n) {
      counts[j] &lt;- sample(1:6, 1)
    }
    average[i] &lt;- mean(counts)
  }
  return(average)
}

toss_die2 &lt;- function(nsim, n) {
  tosses &lt;- matrix(sample(1:6, n*nsim, replace=TRUE), nrow=nsim, ncol=n)
  return(rowSums(tosses)/n)
}</code></pre><p>We now test the runtime of the nested for loop version, and the vectorized version.</p><pre class=r><code>nsim &lt;- 1000; n &lt;- 500
t1 &lt;- system.time(toss_die1(nsim, n)); t1</code></pre><pre><code>##    user  system elapsed 
##   5.641   0.025   5.666</code></pre><pre class=r><code>t2 &lt;- system.time(toss_die2(nsim, n)); t2</code></pre><pre><code>##    user  system elapsed 
##   0.036   0.007   0.044</code></pre><p>We see that nested for loops are <em>very</em> slow and should be avoided when possible. In this case the vectorized code provides a speedup of 129 times! We will use the vectorized function in the following.</p><pre class=r><code>nsim &lt;- 10000; n &lt;- 500
average_value &lt;- toss_die2(nsim,n)
hist(average_value, breaks = 30)</code></pre><p><img src=https://jakespiteri.co.uk/portfolio/computing-1/report-4/index_files/figure-html/unnamed-chunk-5-1.png width=672></p><p>Let’s look at a demonstration of Law of Large Numbers. We plot the cumulative mean as the number of throws increases.</p><pre class=r><code>nsim &lt;- 3000; n &lt;- 1
plot(1:nsim, cumsum(toss_die2(nsim, n))/1:nsim, type = &#39;l&#39;,
     main=&#39;Average dice roll&#39;,
     xlab = &#39;Number of rolls&#39;, ylab=&#39;Cumulative mean&#39;)
abline(h=3.5, col=&#39;red&#39;, lty = 2)</code></pre><p><img src=https://jakespiteri.co.uk/portfolio/computing-1/report-4/index_files/figure-html/unnamed-chunk-6-1.png width=672></p></div></div><div id=the-apply-family class="section level1"><h1>The <code>apply</code> family</h1><p>The <code>apply</code> family consists of <code>apply()</code>, <code>lapply()</code>, <code>sapply()</code>, and <code>mapply()</code>. These functions apply a specified function to elements in a data structure. These functions can be viewed as alternatives to for loops; they are often slower but may simplify and enhance the readability of code.</p><ol style=list-style-type:decimal><li><p><code>apply(X, MARGIN, FUN, ...)</code> — This function applies a function to a vector or matrix. The MARGIN argument determines whether the function is applied over rows or columns, or both.</p></li><li><p><code>lapply(X, FUN, ...)</code> — This function is similar to <code>apply</code> but it applies a function over a vector or list and returns a list.</p></li><li><p><code>sapply(X, FUN, ...)</code> — This is a wrapper function of <code>lapply</code> which simplifies the output by returning a vector, matrix, or an array when specified using <code>simplify = "array"</code>.</p></li><li><p><code>mapply(FUN, ...)</code> — This is a multivariate version of <code>sapply</code>. <code>mapply</code> applies the function FUN to the first elements of each subsequent <code>...</code> argument.</p></li></ol><div id=examples class="section level2"><h2>Examples</h2><p>Below are some examples using the <code>apply</code> family of functions.</p><pre class=r><code>## apply
mat &lt;- matrix(rep(1, 12), 3, 4) # 3x4 matrix of 1s

# apply sum to each row --- i.e. sum over columns
apply(mat, 1, sum)</code></pre><pre><code>## [1] 4 4 4</code></pre><pre class=r><code># apply sum to each column --- i.e. sum over rows
apply(mat, 2, sum) </code></pre><pre><code>## [1] 3 3 3 3</code></pre><pre class=r><code># apply sum to each element --- this does not do anything if FUN = sum
apply(mat, c(1,2), sum)</code></pre><pre><code>##      [,1] [,2] [,3] [,4]
## [1,]    1    1    1    1
## [2,]    1    1    1    1
## [3,]    1    1    1    1</code></pre><pre class=r><code># apply our own function to each element
apply(mat, c(1,2), function(x) 2*x)</code></pre><pre><code>##      [,1] [,2] [,3] [,4]
## [1,]    2    2    2    2
## [2,]    2    2    2    2
## [3,]    2    2    2    2</code></pre><pre class=r><code>## lapply
vec &lt;- rep(1,3)
lis &lt;- list(seq = 1:4, rep = rep(1,4))

# get cosine of each element in a vector
lapply(vec, cos)</code></pre><pre><code>## [[1]]
## [1] 0.5403023
## 
## [[2]]
## [1] 0.5403023
## 
## [[3]]
## [1] 0.5403023</code></pre><pre class=r><code># apply cosine to list
lapply(lis, cos)</code></pre><pre><code>## $seq
## [1]  0.5403023 -0.4161468 -0.9899925 -0.6536436
## 
## $rep
## [1] 0.5403023 0.5403023 0.5403023 0.5403023</code></pre><pre class=r><code>## sapply
# get cosine of each element in a vector
sapply(vec, cos)</code></pre><pre><code>## [1] 0.5403023 0.5403023 0.5403023</code></pre><pre class=r><code># apply cosine to list
sapply(lis, cos)</code></pre><pre><code>##             seq       rep
## [1,]  0.5403023 0.5403023
## [2,] -0.4161468 0.5403023
## [3,] -0.9899925 0.5403023
## [4,] -0.6536436 0.5403023</code></pre><pre class=r><code>## mapply
# sum two vectors elementwise
mapply(&#39;+&#39;, 1:3, 4:6)</code></pre><pre><code>## [1] 5 7 9</code></pre><pre class=r><code># seq(1, 5), seq(2, 5), ...
mapply(seq, 1:5, 5)</code></pre><pre><code>## [[1]]
## [1] 1 2 3 4 5
## 
## [[2]]
## [1] 2 3 4 5
## 
## [[3]]
## [1] 3 4 5
## 
## [[4]]
## [1] 4 5
## 
## [[5]]
## [1] 5</code></pre></div></div><div id=map-reduce-and-filter class="section level1"><h1><code>Map</code>, <code>Reduce</code>, and <code>Filter</code></h1><p>These functions can use lambda expressions similar to the above <code>apply</code> family. These are short functions which are sometimes referred to as anonymous functions, as they are not assigned to a variable. Lambda expressions allow us to produce quite complicated functionality in one line.</p><p>Below is a quick summary of the functions introduced in this section:</p><ol style=list-style-type:decimal><li><p><code>Map</code> — This function maps a function to every element in a vector.</p></li><li><p><code>Reduce</code> — This function performs a function on pairs of elements in a vector, iteratively, and returns a single number.</p></li><li><p><code>Filter</code> — Tests every element in a vector and only returns those that satisfy the condition.</p></li></ol><div id=examples-1 class="section level2"><h2>Examples</h2><p>Below are some examples demonstrating the above functions.</p><pre class=r><code>vec &lt;- 1:3

# Map cosine to every element in a vector
Map(cos, vec)</code></pre><pre><code>## [[1]]
## [1] 0.5403023
## 
## [[2]]
## [1] -0.4161468
## 
## [[3]]
## [1] -0.9899925</code></pre><pre class=r><code># Reduce
Reduce(function(x, y) x + 2*y, vec)</code></pre><pre><code>## [1] 11</code></pre><pre class=r><code># Filter
Filter(function(x) x %% 3 == 0, vec) </code></pre><pre><code>## [1] 3</code></pre></div></div><div id=parallel-computing class="section level1"><h1>Parallel computing</h1><p>Parallelization is the process of using multiple CPU cores at the same time, to solve different parts of a problem. Often when working on large computational problems, we will encounter a bottleneck. We may encounter a</p><ul><li>CPU-bound: Our computations take too much CPU time.</li><li>Memory-bound: Our computations use too much memory.</li><li>I/O bound: It takes too long to read/write from disk.</li></ul><p>Parallel computing helps us overcome CPU bounds by working with more than one core at a time. This means that if our computer has four physical cores, then we can distribute our computation across four cores. In theory, this would speed up our computation by four times but it is not the case in practice due to the other bounds described above. Part of being a good R programmer is being able to write fast and efficient code.</p><div id=parallelize-using-mclapply class="section level2"><h2>Parallelize using <code>mclapply</code></h2><p><code>mclapply</code> is a multi-core version of <code>lapply</code>.</p><pre class=r><code>library(parallel)
library(doParallel)</code></pre><pre><code>## Loading required package: foreach</code></pre><pre><code>## Loading required package: iterators</code></pre><pre class=r><code># detect cores
detectCores()</code></pre><pre><code>## [1] 8</code></pre><pre class=r><code># demonstrate that distributing over 4 cores does not provide speed up of 4 times
system.time(lapply(1:8, function(x) Sys.sleep(0.5)))                # 4 seconds</code></pre><pre><code>##    user  system elapsed 
##   0.003   0.000   4.007</code></pre><pre class=r><code>system.time(mclapply(1:8, function(x) Sys.sleep(0.5), mc.cores=4))  # expect 1 second</code></pre><pre><code>##    user  system elapsed 
##   0.009   0.033   1.025</code></pre></div><div id=parallelize-using-foreach class="section level2"><h2>Parallelize using <code>foreach</code></h2><p><code>foreach</code> seems a little like a <code>for</code> loop. However <code>foreach</code> uses binary operators <code>%do%</code> and <code>%dopar%</code>, and also returns a list as output. It’s useful to use <code>foreach</code> with <code>%do%</code> to evaluate an expression sequentially and store all outputs in a data struture — by default this is a list but the behavior can be changed using the argument <code>.combine</code>.</p><pre class=r><code>library(foreach)

# run sequentially using %do%
foreach(i=1:3) %do% {i*i}</code></pre><pre><code>## [[1]]
## [1] 1
## 
## [[2]]
## [1] 4
## 
## [[3]]
## [1] 9</code></pre><pre class=r><code># change output to vector
foreach(i=1:3, .combine=&quot;c&quot;) %do% {i*i}</code></pre><pre><code>## [1] 1 4 9</code></pre><p>The above functions use the operator <code>%do%</code> and therefore run sequentially. If we simply replace the operator with <code>%dopar%</code> R will run the computations in parallel. The above computations are so simple that it’s probably not worth implementing them in parallel. Let’s look at a little example using Markov chain Monte Carlo, in which we may want to run multiple chains to test for pseudo-convergence. All of the parallelization is in the <code>simulate.chains</code> function. Note that in order to run the computations in parallel we must register the parallel backend using <code>registerDoParallel(&lt;cores>)</code> before <code>foreach</code>. When we are finished we should run <code>stopImplicitCluster()</code>.</p><pre class=r><code># returns the transition kernel P(x)
make.metropolis.hastings.kernel &lt;- function(pi, Q) {
  q &lt;- Q$density
  P &lt;- function(x) {
      z &lt;- Q$sample(x)
      alpha &lt;- min(1, pi(z) * q(z, x) / pi(x) / q(x, z))
      if(runif(1) &lt; alpha) {
        return(z)
      } else {
        return(x)
      }
  }
  return(P)
}

make.normal.proposal &lt;- function(sigma) {
  Q &lt;- list()
  Q$sample &lt;- function(x) {
    return(rnorm(length(x), x, sigma))
  }
  Q$density &lt;- function(x, z) {
    return(dnorm(z, x, sigma))
  }
  return(Q)
}

simulate.chains &lt;- function(P, x0, n, n.chains, n.cores = 4) {
  # parallelize chains
  registerDoParallel(n.cores)
  simulations &lt;- foreach(i=1:n.chains, .combine=&quot;rbind&quot;) %dopar% {
    x &lt;- x0[i]
    foreach(i=1:n, .combine=&quot;c&quot;) %do% {P(x)}
  }
  stopImplicitCluster()
  return(simulations)
}</code></pre><p>Below we sample from a mixture distribution: The weighted mixture of two normal distributions <span class="math inline">\({Z \sim 0.6X + 0.4Y}\)</span>, where <span class="math inline">\(X \sim N(-2,1)\)</span>, <span class="math inline">\(Y \sim N(5, 2^2)\)</span>. We use a <span class="math inline">\(N(0,1)\)</span> proposal distribution. We will run <span class="math inline">\(4\)</span> chains in parallel and test for pseudo-convergence by looking at the trace of the chains. Each chain is initialized using a <span class="math inline">\(U[-10,10]\)</span> distribution.</p><pre class=r><code># generate markov chain using MH algorithm
# target: mixture of two normals
n &lt;- 5e3; set.seed(123)
target &lt;- function(x) {0.6*dnorm(x, -2, 1) + 0.4*dnorm(x, 5, 2)}
xs &lt;- simulate.chains(make.metropolis.hastings.kernel(target, make.normal.proposal(1)), 
                      runif(4,-10,10), n, 4)

# setup plot space
par(mfrow=c(2,2))

# plot the chains
ylimits &lt;- c(min(xs), max(xs))
for(i in 1:4){
  plot(1:n, xs[i,], type = &#39;l&#39;, xlab=&quot;iteration&quot;, ylim=ylimits,
       ylab=&quot;x&quot;, main=paste0(&quot;Trace of Markov chain &quot;, i))
}</code></pre><p><img src=https://jakespiteri.co.uk/portfolio/computing-1/report-4/index_files/figure-html/unnamed-chunk-12-1.png width=960></p><p>We see that the traces do not converge to the same distribution! This tells us that the chains are not fully exploring the state space and thus we must adjust our proposal distribution.</p></div></div></div><footer class=post-footer><ul class=post-tags></ul><nav class=paginav><a class=prev href=https://jakespiteri.co.uk/portfolio/computing-1/report-3/><span class=title>« Prev</span><br><span>Portfolio Report 3: Projects, version control, and packages</span></a>
<a class=next href=https://jakespiteri.co.uk/portfolio/computing-1/report-5/><span class=title>Next »</span><br><span>Portfolio Report 5: Functional and object-oriented programming</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2024 <a href=https://jakespiteri.co.uk/>Jake Spiteri</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>