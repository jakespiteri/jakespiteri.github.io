<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="ie=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="author" content="Jake Spiteri">
<meta name="description" content="Numerical Optimization An optimization problem is the problem of finding the best solution within a set of feasible solutions. There are many characteristics of optimization problems that define them and determine the methods needed to solve them. For example, we may have continuous or discrete problems; convex or non-convex problems; linear or non-linear problems. In this document we will consider continuous problems, and make no explicit assumptions on the convexity or the linearity of the problem." />
<meta name="keywords" content="statistics, mathematics, data science, machine learning, deep learning" />
<meta name="robots" content="noodp" />
<meta name="theme-color" content="#252627" />
<link rel="canonical" href="https://jakespiteri.co.uk/portfolio/computing/report-9/" />


    <title>
        
            Portfolio Report 9: Numerical Optimization :: Jake Spiteri  — Jake Spiteri
        
    </title>



<link href="https://cdnjs.cloudflare.com/ajax/libs/flag-icon-css/3.2.1/css/flag-icon.min.css" rel="stylesheet"
    type="text/css">



<link rel="stylesheet" href="https://jakespiteri.co.uk/main.min.7bfbbe12786fa0ded4b4c0d792cbb36a5bd0bdb0b856dde57aa7b1f6fe0f2b87.css">


    
        <link rel="stylesheet" type="text/css" href="static/style.css">
    



    <link rel="apple-touch-icon" sizes="180x180" href="https://jakespiteri.co.uk/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="https://jakespiteri.co.uk/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="https://jakespiteri.co.uk/favicon-16x16.png">
    <link rel="manifest" href="https://jakespiteri.co.uk/site.webmanifest">
    <link rel="mask-icon" href="https://jakespiteri.co.uk/safari-pinned-tab.svg" color="#252627">
    <link rel="shortcut icon" href="https://jakespiteri.co.uk/favicon.ico">
    <meta name="msapplication-TileColor" content="#252627">
    <meta name="theme-color" content="#252627">

<meta itemprop="name" content="Portfolio Report 9: Numerical Optimization">
<meta itemprop="description" content="Numerical Optimization An optimization problem is the problem of finding the best solution within a set of feasible solutions. There are many characteristics of optimization problems that define them and determine the methods needed to solve them. For example, we may have continuous or discrete problems; convex or non-convex problems; linear or non-linear problems. In this document we will consider continuous problems, and make no explicit assumptions on the convexity or the linearity of the problem.">

<meta itemprop="wordCount" content="4076">
<meta itemprop="image" content="https://jakespiteri.co.uk"/>



<meta itemprop="keywords" content="" /><meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://jakespiteri.co.uk"/>

<meta name="twitter:title" content="Portfolio Report 9: Numerical Optimization"/>
<meta name="twitter:description" content="Numerical Optimization An optimization problem is the problem of finding the best solution within a set of feasible solutions. There are many characteristics of optimization problems that define them and determine the methods needed to solve them. For example, we may have continuous or discrete problems; convex or non-convex problems; linear or non-linear problems. In this document we will consider continuous problems, and make no explicit assumptions on the convexity or the linearity of the problem."/>















<link href='//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/dracula.min.css' rel='stylesheet' type='text/css' />



    </head>

    <body class="dark-theme">
        <div class="container">
            <header class="header">
    <span class="header__inner">
        <a href="https://jakespiteri.co.uk/" style="text-decoration: none;">
    <div class="logo">
        
            <span class="logo__mark">></span>
            <span class="logo__text">$ cd /home/</span>
            <span class="logo__cursor" style=
                  "
                   
                   ">
            </span>
        
    </div>
</a>


        <span class="header__right">
            
                <nav class="menu">
    <ul class="menu__inner"><li><a href="https://jakespiteri.co.uk/about/">About</a></li><li><a href="https://jakespiteri.co.uk/portfolio/">Portfolio</a></li><li><a href="https://jakespiteri.co.uk/posts/">Posts</a></li>
    </ul>
</nav>

                <span class="menu-trigger">
                    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
                        <path d="M0 0h24v24H0z" fill="none"/>
                        <path d="M3 18h18v-2H3v2zm0-5h18v-2H3v2zm0-7v2h18V6H3z"/>
                    </svg>
                </span>
            

            <span class="theme-toggle unselectable"><svg class="theme-toggler" width="24" height="24" viewBox="0 0 48 48" fill="none" xmlns="http://www.w3.org/2000/svg">
  <path d="M22 41C32.4934 41 41 32.4934 41 22C41 11.5066 32.4934 3 22
  3C11.5066 3 3 11.5066 3 22C3 32.4934 11.5066 41 22 41ZM7 22C7
  13.7157 13.7157 7 22 7V37C13.7157 37 7 30.2843 7 22Z"/>
</svg>
</span>
        </span>
    </span>
</header>


            <div class="content">
                
    <main class="post">

        <div class="post-info">
            
            </p>
        </div>

        <article>
            <h2 class="post-title"><a href="https://jakespiteri.co.uk/portfolio/computing/report-9/">Portfolio Report 9: Numerical Optimization</a></h2>

            

            <div class="post-content">
                



<div id="numerical-optimization" class="section level1">
<h1>Numerical Optimization</h1>
<p>An optimization problem is the problem of finding the best solution within a set of feasible solutions. There are many characteristics of optimization problems that define them and determine the methods needed to solve them. For example, we may have continuous or discrete problems; convex or non-convex problems; linear or non-linear problems. In this document we will consider continuous problems, and make no explicit assumptions on the convexity or the linearity of the problem. We will also focus on minimizing a function (maximizing <span class="math inline">\(f(x)\)</span> is equivalent to minimizing <span class="math inline">\(-f(x)\)</span>).</p>
<p>The general form of an optimization problem is
<span class="math display">\[\begin{alignat*}{3}
&amp; \text{minimize } \quad &amp;&amp; f(x) \\
&amp; \text{subject to } \quad &amp;&amp; g_i(x) \leq 0, \quad &amp;&amp; i = 1, \dots, m \\
&amp; &amp;&amp; h_j(x) = 0, \quad &amp;&amp; j = 1, \dots, p,
\end{alignat*}\]</span>
where <span class="math inline">\(f: \mathbb{R}^n \rightarrow \mathbb{R}\)</span> is our objective function and <span class="math inline">\(g_i\)</span> and <span class="math inline">\(h_j\)</span> are our constraint functions.</p>
<p>The above formulation is a constrained optimization problem. We will consider unconstrained optimization in this document. Note that the <code>R</code> function <code>optim</code> offers L-BFGS-B for ‘box’ constrained optimization problems, and more options are supplied using <code>constrOptim</code>.</p>
</div>
<div id="one-dimensional-optimization" class="section level1">
<h1>One-Dimensional Optimization</h1>
<p>The optimization problem is rather difficult. Algorithms make it easy to find local optima, but finding global optima for non-convex problems is much harder. This is best illustrated by attempting to optimize a one-dimensional function. We can clearly see the global minimum in this scenario, but we cannot guarantee convergence to a global minimum with optimization algorithms.</p>
<p>Below we have plotted a simple curve which is periodic over <span class="math inline">\(2\pi\)</span>.</p>
<pre class="r"><code>f &lt;- function(x) {return(cos(x) + cos(2*x) + sin(2*x))}
curve(f, 0, 2*pi)</code></pre>
<p><img src="https://jakespiteri.co.uk/portfolio/computing/Report-9_files/figure-html/unnamed-chunk-1-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>We can clearly see the global minimum is obtained at approximately 2, and a local minimum occurs around 5. We will test the <code>optimize</code> function in base <code>R</code> and see whether it finds the global minimum.</p>
<pre class="r"><code>optimize(f, interval=c(0,2*pi))</code></pre>
<pre><code>## $minimum
## [1] 2.116949
## 
## $objective
## [1] -1.867534</code></pre>
<p>We see that <code>optimize</code> quickly found the global minimum in this case. However, if our interval is wider, <code>optimize</code> returns the local minimum.</p>
<pre class="r"><code>optimize(f, interval=c(0,4*pi))</code></pre>
<pre><code>## $minimum
## [1] 4.928807
## 
## $objective
## [1] -1.112494</code></pre>
<p>The function <code>optimize</code> is based on golden section search, which works well for uni-modal functions. However, when a function has multiple extrema the golden section search method simply finds one of them and provides no assurance as to whether this is a global optimum point. It is useful in that it provides a derivative-free optimization method, which is incredibly fast and scales well.</p>
<p>In this case our function is one-dimensional meaning we can easily check whether we have indeed found the global minimum. However, when we have much higher-dimensional data it becomes nearly impossible to confirm whether we have found a global optimal point.</p>
<div style="page-break-after: always;"></div>
</div>
<div id="newtons-method-for-optimization" class="section level1">
<h1>Newton’s method for optimization</h1>
<p>Newton’s method aims to solve the unconstrained optimization problem:</p>
<p><span class="math display">\[\begin{alignat*}{3}
&amp; \text{minimize } \quad &amp;&amp; f(x) .
\end{alignat*}\]</span></p>
<p>In order to do this, we iteratively update <span class="math inline">\(x_k\)</span>, where <span class="math inline">\(x_{k+1}\)</span> is chosen such that
<span class="math display">\[x_{k+1} = \text{argmin}_{x} f(x_k) + f&#39;(x_k)(x - x_k) + \frac{1}{2}f&#39;&#39;(x_k)(x - x_k)^2.\]</span>
Simply differentiating with respect to <span class="math inline">\(x\)</span> and setting equal to zero, we see that
<span class="math inline">\(x_{k+1} = x_k - \frac{f&#39;(x_k)}{f&#39;&#39;(x_k)}.\)</span> Note that the update uses second-order information — the second derivative — and so converges faster. Newton’s method converges quadratically! It can also easily be rewritten for the multivariate case.</p>
<p>It’s important to note that at each iterate <span class="math inline">\(x_k\)</span>, Newton’s method chooses the next iterate <span class="math inline">\(x_{k+1}\)</span> by minimizing a <em>quadratic approximation</em> of the true curve at <span class="math inline">\(x_k\)</span>.</p>
<p>We implement this in <code>R</code> and test the speed of convergence.</p>
<pre class="r"><code>f_exp &lt;- expression(cos(x) + cos(2*x) + sin(2*x))
f1_exp &lt;- D(f_exp, &#39;x&#39;)
f1 &lt;- function(x) eval(f1_exp)
f2_exp &lt;- D(f1_exp, &#39;x&#39;)
f2 &lt;- function(x) eval(f2_exp)
x &lt;- 1.5
for(i in 1:5) {
  x &lt;- x - f1(x)/f2(x)
  cat(x, &#39;\n&#39;)
}</code></pre>
<pre><code>## 2.48044 
## 1.987202 
## 2.116103 
## 2.116954 
## 2.116954</code></pre>
<p>We see that our optimization algorithm converges in only three steps! Newton’s method is incredibly fast, and is only held back by the fact that we require the Hessian. It has computational complexity <span class="math inline">\(\mathcal{O}(n^3)\)</span> as it needs to compute the inverse of the Hessian.</p>
<p>Here we demonstrate that at a given point, Newton’s method computes the quadratic approximation of the true curve at that point and minimizes it. We start close to the global minimum, with <span class="math inline">\(x_k = 1.7\)</span>. The quadratic approximation at <span class="math inline">\(x_k\)</span> is plotted in blue, and we see that Newton’s method chooses <span class="math inline">\(x_{k+1}\approx2.2\)</span>.</p>
<pre class="r"><code>cat(&quot;x_k = &quot;, 1.7, &quot;\n&quot;, sep=&quot;&quot;)</code></pre>
<pre><code>## x_k = 1.7</code></pre>
<pre class="r"><code>cat(&quot;x_k+1 = &quot;, 1.7 - f1(1.7)/f2(1.7), sep=&quot;&quot;)</code></pre>
<pre><code>## x_k+1 = 2.181084</code></pre>
<pre class="r"><code>curve(f, 0, 2*pi)
second.order.approx &lt;- function(x, xk) f(xk) + f1(xk)*(x-xk) + 1/2*f2(xk)*(x-xk)^2
seq &lt;- seq(0,2*pi,length.out=100)
lines(seq, second.order.approx(seq, 1.7), col=&quot;blue&quot;)</code></pre>
<p><img src="https://jakespiteri.co.uk/portfolio/computing/Report-9_files/figure-html/unnamed-chunk-5-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Newton’s method is also very dependent upon the starting point. We see below that if we were to initialize <span class="math inline">\(x\)</span> at <span class="math inline">\(1\)</span>, then Newton’s method converges to the local minimum near <span class="math inline">\(x=5\)</span>. Under a different initialization at <span class="math inline">\(x=0.5\)</span>, the algorithm converges to the global maximum almost immediately. This occurs when our Hessian matrix becomes negative definite which corresponds to finding the <em>maximum</em> of a curve (recall that a sufficient condition for a strict local maximum (minimum) is that the Hessian is negative (positive) definite).</p>
<pre class="r"><code># initialized at 1
x &lt;- 1
par(mfrow=c(1,4))
for(i in 1:4) {
  # compute updates
  x &lt;- x - f1(x)/f2(x)
  cat(&quot;x&quot;, i, &quot; = &quot;, round(x, 2), &quot;\n&quot;, sep=&quot;&quot;)
  
  # plot the approximation of the true curve at x1
  curve(f, -2*pi, 2*pi, ylim=c(-2,5), main=&quot;Quadratic approximation of f(x)&quot;)
  seq &lt;- seq(-2*pi,2*pi,length.out=100)
  lines(seq, second.order.approx(seq, x), col=&quot;blue&quot;)
}</code></pre>
<pre><code>## x1 = -0.39</code></pre>
<pre><code>## x2 = 2.96</code></pre>
<pre><code>## x3 = 4.77</code></pre>
<pre><code>## x4 = 4.94</code></pre>
<p><img src="https://jakespiteri.co.uk/portfolio/computing/Report-9_files/figure-html/unnamed-chunk-6-1.png" width="1536" style="display: block; margin: auto;" /></p>
<pre class="r"><code># initialized at 0.5
x &lt;- 0.5
par(mfrow=c(1,1))

# compute updates
x &lt;- x - f1(x)/f2(x)
for(i in 1:3) {
  # compute updates
  x &lt;- x - f1(x)/f2(x)
  cat(&quot;x&quot;, i, &quot; = &quot;, round(x, 2), &quot;\n&quot;, sep=&quot;&quot;)
}</code></pre>
<pre><code>## x1 = 0.33
## x2 = 0.33
## x3 = 0.33</code></pre>
<pre class="r"><code># plot the approximation of the true curve at x1
curve(f, -2*pi, 2*pi, ylim=c(-2,5), main=&quot;Quadratic approximation of f(x)&quot;)
seq &lt;- seq(-2*pi,2*pi,length.out=100)
lines(seq, second.order.approx(seq, x), col=&quot;blue&quot;)</code></pre>
<p><img src="https://jakespiteri.co.uk/portfolio/computing/Report-9_files/figure-html/unnamed-chunk-7-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>We have seen that one downside of Newton’s method is that it doesn’t differentiate between minima and maxima. It also requires the Hessian which can be difficult to produce for high-dimensional problems. The method presented above is known as <em>pure</em> Newton’s method, and there are variations which avoid these problems. To avoid finding local maxima we can ensure that the Hessian is positive (semi-) definite at each step, if it is not then we can modify it such that it is (e.g. we can add scalar multiple of the identity matrix to the Hessian such that it becomes positive definite, or we can take the eigendecomposition and modify the eigenvalues such that they are positive.). To avoid computing the Hessian we can use quasi-Newton methods which are similar to Newton’s method but they replace the exact Hessian with an approximation of the Hessian.</p>
<p>So far we have seen that one-dimensional unconstrained optimization is not as easy as we might expect. The problem becomes even more complex as the number of dimensions increase, and the computational complexity of our optimization algorithm becomes important.</p>
</div>
<div id="multi-dimensional-optimization" class="section level1">
<h1>Multi-dimensional optimization</h1>
<p>Most of the objective functions we need to optimize are multi-dimensional. There are three common categories of optimization algorithms:</p>
<ul>
<li>Simplex methods — only uses the value of the function.</li>
<li>First-order methods — uses the value of the function and the value of its first derivative. Gradient descent is a popular method within this category.</li>
<li>Second-order methods — uses the value of the function, its first derivative, and its second derivative (the Hessian). Newton methods fall under this category.</li>
</ul>
<p>The most popular method for multi-dimensional optimization in <code>R</code> is the <code>optim</code> function. <code>optim</code> implements a number of optimization algorithms such as Nelder-Mead, BFGS, CG (conjugate gradient), L-BFGS-B, SANN, and Brent. Another approach is to use the <code>nlm</code> (non-linear minimization) function, which uses a Newton-type algorithm.</p>
</div>
<div id="simplex-methods" class="section level1">
<h1>Simplex methods</h1>
<p>The Nelder-Mead algorithm is a well-known simplex method which is easily used in <code>R</code>.</p>
</div>
<div id="gradient-type-methods" class="section level1">
<h1>Gradient-type methods</h1>
<p>Gradient-type methods are much faster at converging than simplex methods, and avoid evaluating the Hessian. The most well-known method is steepest descent (gradient descent in the machine learning literature), in which we iteratively set
<span class="math display">\[x_{k+1} = x_k - \alpha \nabla f(x_k),\]</span>
where <span class="math inline">\(f(x)\)</span>, <span class="math inline">\(x \in \mathbb{R}^n\)</span>, is a differentiable and multivariate function, and <span class="math inline">\(\alpha\)</span> is our step size (also known as learning rate). We can perform a line-search to find the optimal value of <span class="math inline">\(\alpha\)</span> at each step. <span class="math inline">\(\alpha\)</span> is often chosen to be a small value such as <span class="math inline">\(0.03\)</span> but can be tuned to the specific problem. If our step size is too large the optimization algorithm will step beyond the optimal solution and could diverge.</p>
<p>Steepest descent converges slowly for high-dimensional problems but it has a very low computational cost. This is why steepest descent is the most widely-used optimization algorithm in the machine learning literature. When optimizing a neural network with hundreds of thousands of parameters, Newton’s method cannot be used due to its high computational cost (<span class="math inline">\(\mathcal{O}(n^3)\)</span>). The computational cost of quasi-Newton methods is lower, but still cannot scale to <em>such</em> high-dimensional problems.</p>
<p>Gradient descent is well-known for its tendency to zigzag. Given its frequent use in machine learning, many improvements have been proposed in the machine learning literature. We will look at some of these proposals, implement them, and observe their rate of convergence.</p>
<p>We will consider the function <span class="math inline">\(f(x, y) = x^2 + 10y^2\)</span>. Below is a contour plot of this function.</p>
<pre class="r"><code># create contour plot
f &lt;- function(x, y) {x^2 + 10*y^2}
s &lt;- seq(-100,100,length=300)
contmat &lt;- outer(s,
                 s,
                 Vectorize(function(x,y) f(x,y)))
rownames(contmat) &lt;- colnames(contmat) &lt;- s # keep x and y values and not the index
contlong &lt;- melt(contmat) # melt into long format
b &lt;- exp(seq(log(10), log(100000), length = 25))
contplot &lt;- contlong %&gt;%
  ggplot() +
  geom_contour(aes(x = X1, y = X2, z = value, colour = stat(level)), breaks = b) + 
  ggtitle(&quot;Contour plot of f(x,y)&quot;) +
  theme_minimal()
contplot</code></pre>
<p><img src="https://jakespiteri.co.uk/portfolio/computing/Report-9_files/figure-html/unnamed-chunk-8-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Let’s implement a simple version of steepest descent and view its zigzagging nature. We see that when the state space varies much more in one dimension than another, the algorithm tends to zigzag.</p>
<pre class="r"><code>graddesc &lt;- function(x0, fexp, iter, lr) {
  
  # get first derivative
  pars &lt;- paste(&quot;x&quot;, 1:length(x0), sep=&quot;&quot;)
  f1 &lt;- deriv(fexp, namevec = pars, function.arg = TRUE)
  
  # do gradient descent and keep all steps
  x_steps &lt;- matrix(NA, iter+1, length(x0))
  x_steps[1,] &lt;- x_gd &lt;- x0
  for (i in 1:iter) {
    deriv &lt;- attributes(do.call(f1, as.list(x_gd)))$gradient
    x_gd &lt;- x_gd - lr * deriv
    x_steps[i+1,] &lt;- c(x_gd)
  }
 
  return(x_steps)
}</code></pre>
<pre class="r"><code>f &lt;- expression(x1^2 + 10*x2^2)
f_gd &lt;- as.data.frame(graddesc(c(-100,-50), f, 20, 0.09))
contplot + 
  geom_line(data = f_gd, mapping = aes(x = V1, y = V2), colour = &#39;red&#39;) +
  geom_point(data = f_gd, mapping = aes(x = V1, y = V2), shape = 21, colour = &#39;red&#39;) +
  labs(subtitle = &quot;using gradient descent&quot;)</code></pre>
<p><img src="https://jakespiteri.co.uk/portfolio/computing/Report-9_files/figure-html/unnamed-chunk-10-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>We see that for a non-optimal learning rate size (one that is too large) the gradient descent steps zigzag.</p>
<div id="gradient-descent-variants" class="section level2">
<h2>Gradient descent variants</h2>
<div id="gradient-descent-with-momentum" class="section level3">
<h3>Gradient descent with momentum</h3>
<p>This method replaces the gradient of the objective function in the update with an exponentially weighted average of past gradients. This has the effect of smoothing out the update and dampens potential zigzagging. The iterative updates are
<span class="math display">\[\begin{alignat*}{2}
x_{k+1} &amp;= x_k - \alpha V_k \\
V_k &amp;= \beta V_{k-1} + (1 - \beta)\nabla f(x_k),
\end{alignat*}\]</span>
where <span class="math inline">\(\beta\)</span> is a hyperparameter called the momentum, which ranges between 0 and 1. The larger the <span class="math inline">\(\beta\)</span>, the greater the smoothing effect. A popular choice for <span class="math inline">\(\beta\)</span> is 0.9.</p>
<p>Momentum helps accelerate gradient descent by dampening oscillations. We will implement gradient descent with momentum and see if there is an improvement. Note that when <span class="math inline">\(\beta = 0\)</span>, gradient descent with momentum is simply gradient descent.</p>
<pre class="r"><code>graddescmomentum &lt;- function(x0, fexp, iter, lr, momentum) {
  
  # get first derivative
  pars &lt;- paste(&quot;x&quot;, 1:length(x0), sep=&quot;&quot;)
  f1 &lt;- deriv(fexp, namevec = pars, function.arg = TRUE)
  
  # do gradient descent and keep all steps
  x_steps &lt;- matrix(NA, iter+1, length(x0))
  x_steps[1,] &lt;- x_gd &lt;- x0
  V &lt;- rep(0,length(x0))
  for (i in 1:iter) {
    deriv &lt;- attributes(do.call(f1, as.list(x_gd)))$gradient
    V &lt;- momentum * V + (1 - momentum) * deriv
    x_gd &lt;- x_gd - lr * V
    x_steps[i+1,] &lt;- c(x_gd)
  }
 
  return(x_steps)
}

f &lt;- expression(x1^2 + 10*x2^2)
f_gd &lt;- as.data.frame(graddescmomentum(c(-100,-50), f, 20, 0.09, 0.5))
contplot + 
  geom_line(data = f_gd, mapping = aes(x = V1, y = V2), colour = &#39;red&#39;) +
  geom_point(data = f_gd, mapping = aes(x = V1, y = V2), shape = 21, colour = &#39;red&#39;) +
  labs(subtitle = &quot;using gradient descent with momentum&quot;)</code></pre>
<p><img src="https://jakespiteri.co.uk/portfolio/computing/Report-9_files/figure-html/unnamed-chunk-11-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
<div id="rmsprop" class="section level3">
<h3>RMSprop</h3>
<p>When finding the minimum point of a function, we may want to take large steps towards the minimum when we are far from it, and smaller steps when we are close. In order to achieve this we can employ a line search method, or a learning rate scheduler which decreases the learning rate at specified intervals. There are also methods which employ <em>adaptive</em> gradients, such as RMSprop.</p>
<p>The key idea of RMSprop is to store an exponentially weighted average of the squared gradients for each weight. RMSprop scales the gradient updates by dividing the learning rate by the square root of an exponentially decaying average of squared gradients — hence the name root mean-squared propagation.</p>
<p>RMSprop has the update formula
<span class="math display">\[\begin{alignat*}{2}
x_{k+1} &amp;= x_k - \frac{\alpha}{\sqrt{\nu_k + \epsilon}}\nabla f(x_{k}), \\
\nu_k &amp;= \beta \nu_{k-1} + (1 - \beta) \nabla f(x_{k})^2,
\end{alignat*}\]</span>
where <span class="math inline">\(\beta\)</span> is the weighting of the moving average. A suggested default value for <span class="math inline">\(\beta\)</span> is <span class="math inline">\(0.9\)</span>.</p>
<pre class="r"><code>rmsprop &lt;- function(x0, fexp, iter, lr, beta, eps = 1e-5) {
  
  # get first derivative
  pars &lt;- paste(&quot;x&quot;, 1:length(x0), sep=&quot;&quot;)
  f1 &lt;- deriv(fexp, namevec = pars, function.arg = TRUE)
  
  # do gradient descent and keep all steps
  x_steps &lt;- matrix(NA, iter+1, length(x0))
  x_steps[1,] &lt;- x_gd &lt;- x0
  ms &lt;- rep(0,length(x0))
  for (i in 1:iter) {
    deriv &lt;- attributes(do.call(f1, as.list(x_gd)))$gradient
    ms &lt;- beta * ms + (1 - beta) * deriv^2
    x_gd &lt;- x_gd - (lr / sqrt(ms + eps)) * deriv
    x_steps[i+1,] &lt;- c(x_gd)
  }
 
  return(x_steps)
}

f &lt;- expression(x1^2 + 10*x2^2)
f_gd &lt;- as.data.frame(rmsprop(c(-100,-50), f, 120, 1, 0.9))
contplot + 
  geom_line(data = f_gd, mapping = aes(x = V1, y = V2), colour = &#39;red&#39;) +
  geom_point(data = f_gd, mapping = aes(x = V1, y = V2), shape = 21, colour = &#39;red&#39;) +
  labs(subtitle = &quot;using RMSprop&quot;)</code></pre>
<p><img src="https://jakespiteri.co.uk/portfolio/computing/Report-9_files/figure-html/unnamed-chunk-12-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
<div id="adam" class="section level3">
<h3>Adam</h3>
<p>Adaptive Moment Estimation (Adam) also implements adaptive learning rates. Similar to RMSprop above, Adam stores an exponentially decaying average of squared gradients, and it also stores an exponentially decaying average of gradients similar to momentum.</p>
<p>Adams update formula is
<span class="math display">\[\begin{alignat*}{2}
x_{k+1} &amp;= x_k - \frac{\alpha}{\sqrt{\widehat{\nu}_k} + \epsilon}\widehat{m}_k, \\
m_k &amp;= \beta_1 m_{k-1} + (1 - \beta_1) \nabla f(x_{k}),\\
\nu_k &amp;= \beta_2 \nu_{k-1} + (1 - \beta_2) \nabla f(x_{k})^2,\\
\widehat{m}_k &amp;= \frac{m_k}{1 - \beta_1^k},\\
\widehat{\nu}_k &amp;= \frac{\nu_k}{1 - \beta_2^k}.
\end{alignat*}\]</span></p>
<p>In the above, <span class="math inline">\(\beta_1\)</span> and <span class="math inline">\(\beta_2\)</span> are hyperparameters to be tuned. The original creaters of Adam propose default values of <span class="math inline">\(0.9\)</span> for <span class="math inline">\(\beta_1\)</span>, <span class="math inline">\(0.999\)</span> for <span class="math inline">\(\beta_2\)</span>, and <span class="math inline">\(10^{-8}\)</span> for <span class="math inline">\(\epsilon\)</span>.</p>
<p>We can think of <span class="math inline">\(m_k\)</span> and <span class="math inline">\(\nu_k\)</span> as the first moment (the mean) and the second moment (the uncentered variance) respectively, hence the name adaptive moment estimation. As <span class="math inline">\(m\)</span> and <span class="math inline">\(\nu\)</span> are often initialized as (vectors of) zeros, they tend to be biased towards zero in the first few iterations. To avoid this bias towards zero we use <span class="math inline">\(\widehat{m}\)</span> and <span class="math inline">\(\widehat{\nu}\)</span> as specified above.</p>
<pre class="r"><code>adam &lt;- function(x0, fexp, iter, lr, beta1, beta2, eps = 1e-8) {
  
  # get first derivative
  pars &lt;- paste(&quot;x&quot;, 1:length(x0), sep=&quot;&quot;)
  f1 &lt;- deriv(fexp, namevec = pars, function.arg = TRUE)
  
  # do gradient descent and keep all steps
  x_steps &lt;- matrix(NA, iter+1, length(x0))
  x_steps[1,] &lt;- x_gd &lt;- x0
  V &lt;- S &lt;- rep(0,length(x0))
  for (i in 1:iter) {
    deriv &lt;- attributes(do.call(f1, as.list(x_gd)))$gradient
    V &lt;- beta1 * V + (1-beta1) * deriv
    S &lt;- beta2 * S + (1-beta2) * deriv^2
    Vhat &lt;- V/(1-beta1^i)
    Shat &lt;- S/(1-beta2^i)
    x_gd &lt;- x_gd - lr / (sqrt(Shat) + eps) * Vhat
    x_steps[i+1,] &lt;- c(x_gd)
  }
 
  return(x_steps)
}

f &lt;- expression(x1^2 + 10*x2^2)
f_gd &lt;- as.data.frame(adam(c(-100,-50), f, 100, 2, 0.9, 0.999))
contplot + 
  geom_line(data = f_gd, mapping = aes(x = V1, y = V2), colour = &#39;red&#39;) +
  geom_point(data = f_gd, mapping = aes(x = V1, y = V2), shape = 21, 
             colour = &#39;red&#39;, fill=&quot;white&quot;) +
  labs(subtitle = &quot;using adam&quot;)</code></pre>
<p><img src="https://jakespiteri.co.uk/portfolio/computing/Report-9_files/figure-html/unnamed-chunk-13-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
</div>
</div>
<div id="second-order-methods" class="section level1">
<h1>Second-order methods</h1>
<p>Second-order methods converge much faster than first-order methods, but they also require more computation. We have already explained Newton’s method which is arguably the most widely-used second-order method, and so we will provide a brief summary here.</p>
<p>Newton’s method minimizes the quadratic approximation of <span class="math inline">\(f(x)\)</span> around <span class="math inline">\(x_k\)</span>:
<span class="math display">\[Q(x) = f(x_k) + \nabla f(x_k)^T (x - x_k) + \frac{1}{2} (x - x_k)^T \nabla^2 f(x_k) (x - x_k)\]</span>
I.e. <span class="math inline">\(x_{k+1} = \text{argmin}_x Q(x)\)</span>. To find the minimum point we simply differentiate with respect to x and set equal to zero. This gives the update formula
<span class="math display">\[x_{k+1} = x_k - (\nabla^2 f(x_k))^{-1} \nabla f(x_k).\]</span>
We often include a step size parameter <span class="math inline">\(\alpha\)</span> to control how large our steps are.</p>
<ul>
<li><p>Newton’s method converges quadratically and has computational cost <span class="math inline">\(\mathcal{O}(n^3)\)</span> for <span class="math inline">\(x \in \mathbb{R}^n\)</span></p></li>
<li><p>Pure Newton’s method may diverge if our initialization is bad. To avoid this we must ensure that our Hessian is positive (semi-) definite at each step, and ensure our step size is chosen such that the function decreases.</p></li>
<li><p>Computing the Hessian may be very difficult even if we use automatic differentiation. Storing the Hessian also becomes a problem for very high-dimensional problems.</p></li>
</ul>
<p>Below we optimize <span class="math inline">\(f(x, y) = x^2 + 10y^2\)</span> using Newton’s method. It converges in just <span class="math inline">\(1\)</span> step! This is because the function is quadratic in two variables and thus the quadratic approximation to the true function computed by Newton’s method is exact. Newton’s method directly minimizes the true function in this case.</p>
<pre class="r"><code>newton &lt;- function(x0, fexp, iter, lr) {
  
  # get first and second derivative
  pars &lt;- paste(&quot;x&quot;, 1:length(x0), sep=&quot;&quot;)
  d &lt;- deriv(fexp, namevec = pars, function.arg = TRUE, hessian = TRUE)
  
  # do gradient descent and keep all steps
  x_steps &lt;- matrix(NA, iter+1, length(x0))
  x_steps[1,] &lt;- x_n &lt;- x0
  ms &lt;- rep(0,length(x0))
  for (i in 1:iter) {
    deriv &lt;- attributes(do.call(d, as.list(x_n)))
    deriv_1 &lt;- t(deriv$gradient)
    deriv_2 &lt;- deriv$hessian[,,1:length(x0)]
    x_n &lt;- x_n - lr*solve(deriv_2)%*%deriv_1
    x_steps[i+1,] &lt;- c(x_n)
  }
 
  return(x_steps)
}

f &lt;- expression(x1^2 + 10*x2^2)
f_n &lt;- as.data.frame(newton(c(-100,-50), f, 5, 1))
contplot + 
  geom_line(data = f_n, mapping = aes(x = V1, y = V2), colour = &#39;red&#39;) +
  geom_point(data = f_n, mapping = aes(x = V1, y = V2), shape = 21, 
             colour = &#39;red&#39;, fill=&quot;white&quot;) +
  labs(subtitle = &quot;using Newton&#39;s method&quot;)</code></pre>
<p><img src="https://jakespiteri.co.uk/portfolio/computing/Report-9_files/figure-html/unnamed-chunk-14-1.png" width="672" style="display: block; margin: auto;" /></p>
<div id="quasi-newton-methods" class="section level2">
<h2>Quasi-Newton methods</h2>
<p>Quasi-Newton methods are great substitutes for second-order methods. They approximate the Hessian at each iteration, rather than evaluate the true Hessian. This avoids the problem of computing the true Hessian for high-dimensional objective functions. Quasi-Newton methods are not as fast as Newton’s methods — they attain <em>superlinear</em> convergence. However, they also have a lower computational cost! They have complexity <span class="math inline">\(\mathcal{O}(n^2)\)</span> whilst Newton’s method carries complexity <span class="math inline">\(\mathcal{O}(n^3)\)</span>.</p>
<p>BFGS is arguably the best quasi-Newton method which has been proposed thus far. It has update formula <span class="math inline">\(x_{k+1} = x_k - \alpha_k B_k \nabla f(x_k)\)</span>, where <span class="math inline">\(B_k\)</span> is an <span class="math inline">\(n \times n\)</span> symmetric positive definite matrix that is updated at each iteration and <span class="math inline">\(\alpha_k\)</span> is our step size.</p>
<p>If we have a objective function with <span class="math inline">\(1000\)</span> variables, the Hessian and its approximation is a <span class="math inline">\(1000 \times 1000\)</span> dimension matrix. This is a large matrix to keep in memory and thus a limited-memory version of BFGS is often used (L-BFGS).</p>
<p>Let’s derive the BFGS update formula and see how it works.
BFGS also works by minimizing the quadratic approximation of the true function around <span class="math inline">\(x_k\)</span>. Recall the Taylor series approximation of <span class="math inline">\(f(x_k + d)\)</span> around <span class="math inline">\(x_k\)</span>.
<span class="math display">\[f(x_k + d) = f(x_k) + \nabla f(x_k)^T d + \frac{1}{2} d^T \nabla^2 f(x_k) d + o(\|d\|)^2\]</span>
We shall define the approximation of the objective function about <span class="math inline">\(x_k\)</span> as
<span class="math display">\[m_k(d) := f(x_k) + \nabla f(x_k)^Td + \frac{1}{2}d^T B_k d,\]</span>
where <span class="math inline">\(B_k\)</span> is an <span class="math inline">\(n \times n\)</span> symmetric positive definite matrix which we will update. It is our approximation of the Hessian.</p>
<p>The minimizer of this convex quadratic <span class="math inline">\(m_k(d)\)</span> is <span class="math inline">\(d = B_k^{-1} \nabla f(x_k)\)</span>. Hence we have the BFGS update formula <span class="math inline">\(x_{k+1} = x_k - \alpha_k B_k^{-1} \nabla f(x_k)\)</span>, where <span class="math inline">\(\alpha_k\)</span> is our step size. The question now is: How do we compute <span class="math inline">\(B_k\)</span>?</p>
<p>Instead of recomputing <span class="math inline">\(B_k\)</span> at each iteration, we will update it based on information about the curvature gained in the most recent step. Suppose we observe a new iterate <span class="math inline">\(x_{k+1}\)</span>, then we form the quadratic approximation
<span class="math inline">\(m_{k+1}(d) = f(x_{k+1}) + \nabla f(x_{k+1})^Td + \frac{1}{2}d^T B_{k+1} d\)</span>
In order to determine <span class="math inline">\(B_{k+1}\)</span> we place restrictions on our approximation <span class="math inline">\(m_{k+1}(d)\)</span>. We want <span class="math inline">\(m_{k+1}(d)\)</span> to be a good approximation to the objective function at <span class="math inline">\(x_{k+1}\)</span>. We propose that a good approximation will have the same gradient as the true function <span class="math inline">\(f\)</span> at the two most recent iterates. That is, we require</p>
<ul>
<li><p><span class="math inline">\(\nabla m_{k+1}(0) = \nabla f(x_{k+1}),\)</span></p></li>
<li><p><span class="math inline">\(\nabla m_{k+1}(- \alpha_k d_k) = \nabla f(x_{k+1}) - \alpha_k B_{k+1} d_k = \nabla f(x_k).\)</span></p></li>
</ul>
<p>This first requirement is always true. The second requirement can be rewritten as
<span class="math display">\[\alpha_k B_{k+1} d_k = \nabla f(x_{k+1}) - \nabla f(x_{k}).\]</span>
Using the notation <span class="math inline">\(y_k = \nabla f(x_{k+1}) - \nabla f(x_{k})\)</span>, and <span class="math inline">\(s_k = x_{k+1} - x_k\)</span> <span class="math inline">\((= \alpha_k d)\)</span>, we can rewrite the above equation as the <em>secant</em> equation
<span class="math display">\[B_{k+1}s_k = y_k.\]</span>
Note that if we approximate the Hessian we will then need to find its inverse to update <span class="math inline">\(x\)</span>. To avoid the expensive inversion, BFGS directly approximates the inverse of the Hessian which we will denote <span class="math inline">\(H\)</span>. Thus, we rewrite the secant equation
<span class="math display">\[ H_{k+1} y_k = s_k.\]</span>
This means that we require the symmetric positive definite matrix <span class="math inline">\(H_{k+1}\)</span> to map <span class="math inline">\(y_k\)</span> to <span class="math inline">\(s_k\)</span>. This is only possible if <span class="math inline">\(s_k\)</span> and <span class="math inline">\(y_k\)</span> satisfy the curvature condition: <span class="math inline">\(y_k^T s_k &gt; 0\)</span>. If <span class="math inline">\(f\)</span> is strongly convex then this is always true for any <span class="math inline">\(x_k\)</span> and <span class="math inline">\(x_{k+1}\)</span>. If <span class="math inline">\(f\)</span> is non-convex then we must impose this restriction explicitly. We can do this by imposing the (strong) Wolfe conditions on the line search for our step size.</p>
<p>The secant equations place <span class="math inline">\(n\)</span> restrictions on <span class="math inline">\(H_{k+1}\)</span>, but we know that <span class="math inline">\(H_{k+1}\)</span> is symmetric positive definite and thus has <span class="math inline">\(n(n+1)/2\)</span> free parameters. Hence <span class="math inline">\(H_{k+1}\)</span> has infinitely many solutions. In order to obtain a unique solution we must impose a further restriction. We require that among all symmetric matrices that satisfy the secant equation, <span class="math inline">\(H_{k+1}\)</span> is in some sense closest to <span class="math inline">\(H_k\)</span>. We form the optimization problem
<span class="math display">\[\begin{alignat*}{2}
&amp;\text{minimize }_{H} \quad &amp;&amp;\| H - H_k \|, \\
&amp;\text{subject to } &amp;&amp; H = H^T, \\
&amp; &amp;&amp; H y_k = s_k,
\end{alignat*}\]</span>
where <span class="math inline">\(s_k\)</span> and <span class="math inline">\(y_k\)</span> satisfy the curvature condition and <span class="math inline">\(H_k\)</span> is symmetric and positive definite.</p>
<p>Different matrix norms result in different quasi-Newton methods. A norm which provides an easy solution to the optimization problem which is also scale-invariant is the weighted Frobenius norm
<span class="math display">\[\|A\|_W = \|W^{1/2}AW^{1/2}\|_F,\]</span>
where <span class="math inline">\(\|\cdot\|_F\)</span> is defined by <span class="math inline">\(\|A\|_F = \sum_{i=1}^n\sum_{j=1}^n a_{ij}^2\)</span>.</p>
<p>The matrix W can be chosen to be any matrix satisfying <span class="math inline">\(Ws_k = y_k\)</span>. One possible <span class="math inline">\(W\)</span> which we know gives a solution is the inverse of the average Hessian. That is,
<span class="math display">\[W = \bar{G}_k^{-1}, \quad \text{where } \bar{G}_k = \int_0^1 \nabla^2 f(x_k + t \alpha_k d_k) dt.\]</span></p>
<p>The choices above: the weighted Frobenius norm and <span class="math inline">\(W\)</span> the inverse of the average Hessian provide the following unique solution to the optimization problem</p>
<p><span class="math display">\[H_{k+1} = (I - \rho_k s_k y_k^T) H_k (I - \rho y_k s_k^T) + \rho_k s_k s_k^T,\]</span>
where <span class="math inline">\(\rho_k = \frac{1}{y_k^T s_k}\)</span>.</p>
<p>Given this update formula for <span class="math inline">\(H_{k+1}\)</span>, the resulting BFGS algorithm is quite simple!</p>

<p>Below we continue to optimize the function <span class="math inline">\(f(x, y) = x^2 + 10y^2\)</span>. We use BFGS and see that it converges in only 3 steps! Clearly it is far superior to first-order methods when the objective function has a <em>reasonable</em> number of parameters. This is of course not an easy number to quantify, but well-defined objective functions with a large number of parameters can be optimized with BFGS.</p>
<pre class="r"><code>f &lt;- function(x) {(x[1]^2 + 10*x[2]^2)}
f_bfgs &lt;- c(-100,-50)
for(i in 1:10) {
  f_bfgs &lt;- rbind(f_bfgs, optim(c(-100,-50), f, 
                                method = &quot;BFGS&quot;, control=list(maxit=i))$par)
}
f_bfgs &lt;- as.data.frame(f_bfgs, row.names=1:11)
contplot + 
  geom_path(data = f_bfgs, mapping = aes(x = V1, y = V2), colour = &#39;red&#39;) +
  geom_point(data = f_bfgs, mapping = aes(x = V1, y = V2), shape = 21, 
             colour = &#39;red&#39;, fill=&quot;white&quot;) +
  labs(subtitle = &quot;using BFGS&quot;)</code></pre>
<p><img src="https://jakespiteri.co.uk/portfolio/computing/Report-9_files/figure-html/unnamed-chunk-15-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
</div>

            </div>
        </article>

        <hr />

        <div class="post-info">
  			</div>

        
    </main>

            </div>

            
                <footer class="footer">
    <div class="footer__inner">
        <div class="footer__content">
            <span>&copy; 2020</span>
            
                <span><a href="https://jakespiteri.co.uk">Jake Spiteri</a></span>
            
            <span><a href="https://creativecommons.org/licenses/by-nc/4.0/" target="_blank" rel="noopener">CC BY-NC 4.0</a></span>
            <span> <a href="https://jakespiteri.co.uk/posts/index.xml" target="_blank" title="rss"><svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 20 20" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-rss"><path d="M4 11a9 9 0 0 1 9 9"></path><path d="M4 4a16 16 0 0 1 16 16"></path><circle cx="5" cy="19" r="1"></circle></svg></a></span>
        </div>
    </div>
    <div class="footer__inner">
    </div>
</footer>
    



<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js"></script>



<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/languages/r.min.js"></script>
<script>hljs.configure({languages: []}); hljs.initHighlightingOnLoad();</script>


    <script src="https://jakespiteri.co.uk/js/math-code.js"></script>
<script async src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>

  </body>
</html>
            
        </div>

        




<script type="text/javascript" src="https://jakespiteri.co.uk/bundle.min.2d5469329143160ae2456a69c3c76dc2d0a3b212b46afe291a51bd68650ed6f8697e001dab54f1c272c77ce08092a8c55e5bb4314e0ee334aab4b927ec896638.js" integrity="sha512-LVRpMpFDFgriRWppw8dtwtCjshK0av4pGlG9aGUO1vhpfgAdq1TxwnLHfOCAkqjFXlu0MU4O4zSqtLkn7IlmOA=="></script>



    </body>
</html>
